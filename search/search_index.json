{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"delfi \u00b6 delfi : A Python toolbox to perform simulation-based inference using density-estimation approaches. The focus of delfi is Sequential Neural Posterior Estimation (SNPE). In SNPE, a neural network is trained to perform Bayesian inference on simulated data. To see illustrations of SNPE on canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To get started quickly, refer to the installation instructions and tutorials . More in-depth tutorials will be added soon. To learn more about the general motivation behind simulation-based inference, and algorithms included in delfi , keep on reading. Motivation and approach \u00b6 Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. Sequential Neural Posterior Estimation (SNPE) is a powerful machine-learning technique to address this problem. Goal: Algorithmically identify mechanistic models which are consistent with data. SNPE takes three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations. Publications \u00b6 Algorithms included in delfi were published in the following papers, which provide additional information: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. As an alternative to directly estimating the posterior on parameters given data, it is also possible to estimate the likelihood of data given parameters, and then subsequently draw posterior samples using MCMC ( Papamakarios, Sterratt & Murray, 2019 1 , Lueckmann, Karaletsos, Bassetto, Macke, 2019 ). Depending on the problem, approximating the likelihood can be more or less effective than SNPE techniques. See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience. Code for SNL is available from the original repository or as a python 3 package . \u21a9","title":"Home"},{"location":"#delfi","text":"delfi : A Python toolbox to perform simulation-based inference using density-estimation approaches. The focus of delfi is Sequential Neural Posterior Estimation (SNPE). In SNPE, a neural network is trained to perform Bayesian inference on simulated data. To see illustrations of SNPE on canonical problems in neuroscience, read our preprint: Training deep neural density estimators to identify mechanistic models of neural dynamics . To get started quickly, refer to the installation instructions and tutorials . More in-depth tutorials will be added soon. To learn more about the general motivation behind simulation-based inference, and algorithms included in delfi , keep on reading.","title":"delfi"},{"location":"#motivation-and-approach","text":"Many areas of science and engineering make extensive use of complex, stochastic, numerical simulations to describe the structure and dynamics of the processes being investigated. A key challenge in simulation-based science is linking simulation models to empirical data: Bayesian inference provides a general and powerful framework for identifying the set of parameters which are consistent both with empirical data and prior knowledge. One of the key quantities required for statistical inference, the likelihood of observed data given parameters, \\(\\mathcal{L}(\\theta) = p(x_o|\\theta)\\) , is typically intractable for simulation-based models, rendering conventional statistical approaches inapplicable. Sequential Neural Posterior Estimation (SNPE) is a powerful machine-learning technique to address this problem. Goal: Algorithmically identify mechanistic models which are consistent with data. SNPE takes three inputs: A candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by: sampling parameters from the prior and simulating synthetic datasets from these parameters, and using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, i.e. to learn statistical inference from simulated data. This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, i.e. the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. If needed, an initial estimate of the posterior can be used to adaptively generate additional informative simulations.","title":"Motivation and approach"},{"location":"#publications","text":"Algorithms included in delfi were published in the following papers, which provide additional information: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. As an alternative to directly estimating the posterior on parameters given data, it is also possible to estimate the likelihood of data given parameters, and then subsequently draw posterior samples using MCMC ( Papamakarios, Sterratt & Murray, 2019 1 , Lueckmann, Karaletsos, Bassetto, Macke, 2019 ). Depending on the problem, approximating the likelihood can be more or less effective than SNPE techniques. See Cranmer, Brehmer, Louppe (2019) for a recent review on simulation-based inference and our recent preprint Training deep neural density estimators to identify mechanistic models of neural dynamics (Goncalves et al., 2019) for applications to canonical problems in neuroscience. Code for SNL is available from the original repository or as a python 3 package . \u21a9","title":"Publications"},{"location":"credits/","text":"Credits \u00b6 Algorithms \u00b6 delfi implements algorithms from the following papers: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively. Code \u00b6 delfi.distribution and delfi.neuralnet build on code written by George Papamakarios . The separation of classes in delfi is partially inspired by the design of ELFI: Engine for Likelihood-Free Inference . Modified work Copyright (c) 2019, David Greenberg, Marcel Nonnemacher, Jan-Matthis Lueckmann, Pedro J. Goncalves, Kaan \u00d6cal, Jakob H. Macke Modified work Copyright (c) 2017, Jan-Matthis Lueckmann, Pedro J. Goncalves, Jakob H. Macke Original work Copyright (c) 2016, George Papamakarios All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The views and conclusions contained in the software and documentation are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of anybody else.","title":"Credits"},{"location":"credits/#credits","text":"","title":"Credits"},{"location":"credits/#algorithms","text":"delfi implements algorithms from the following papers: Fast \u03b5-free Inference of Simulation Models with Bayesian Conditional Density Estimation by Papamakarios & Murray (NeurIPS 2016) [PDF] [BibTeX] Flexible statistical inference for mechanistic models of neural dynamics by Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) [PDF] [BibTeX] Automatic posterior transformation for likelihood-free inference by Greenberg, Nonnenmacher & Macke (ICML 2019) [PDF] [BibTeX] We refer to these algorithms as SNPE-A, SNPE-B, and SNPE-C/APT, respectively.","title":"Algorithms"},{"location":"credits/#code","text":"delfi.distribution and delfi.neuralnet build on code written by George Papamakarios . The separation of classes in delfi is partially inspired by the design of ELFI: Engine for Likelihood-Free Inference . Modified work Copyright (c) 2019, David Greenberg, Marcel Nonnemacher, Jan-Matthis Lueckmann, Pedro J. Goncalves, Kaan \u00d6cal, Jakob H. Macke Modified work Copyright (c) 2017, Jan-Matthis Lueckmann, Pedro J. Goncalves, Jakob H. Macke Original work Copyright (c) 2016, George Papamakarios All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The views and conclusions contained in the software and documentation are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of anybody else.","title":"Code"},{"location":"install/","text":"Installation \u00b6 You can install delfi by cloning from the GitHub repository and using pip: git clone https://github.com/mackelab/delfi.git cd delfi pip install -r requirements.txt pip install -e . Installing delfi as described above will automatically take care of the requirements. In total, this installation will typically take less than a minute. Small print \u00b6 delfi is written for Python 3 and not compatible with older versions. Core dependencies are the packages theano and lasagne . For lasagne , delfi relies on the development version of lasagne (0.2dev) rather than the stable version (0.1) that is available through pip. To use the APT inference algorithm with Gaussian or Mixture-of-Gaussians proposals, you will likely need to make openblas available for theano . If openblas is missing on your system and you happen to use Debian/Ubuntu you can install it with sudo apt install libopenblas-dev .","title":"Installation"},{"location":"install/#installation","text":"You can install delfi by cloning from the GitHub repository and using pip: git clone https://github.com/mackelab/delfi.git cd delfi pip install -r requirements.txt pip install -e . Installing delfi as described above will automatically take care of the requirements. In total, this installation will typically take less than a minute.","title":"Installation"},{"location":"install/#small-print","text":"delfi is written for Python 3 and not compatible with older versions. Core dependencies are the packages theano and lasagne . For lasagne , delfi relies on the development version of lasagne (0.2dev) rather than the stable version (0.1) that is available through pip. To use the APT inference algorithm with Gaussian or Mixture-of-Gaussians proposals, you will likely need to make openblas available for theano . If openblas is missing on your system and you happen to use Debian/Ubuntu you can install it with sudo apt install libopenblas-dev .","title":"Small print"},{"location":"reference/distributions/","text":"Distributions \u00b6 Discrete class \u00b6 Discrete . __init__ ( self , p , seed = None ) Discrete distribution Parameters ---------- p : list or np.array, 1d Probabilities of elements, must sum to 1 seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 def __init__ ( self , p , seed = None ): super () . __init__ ( ndim = 1 , seed = seed ) p = np . asarray ( p ) assert p . ndim == 1 , 'p must be a 1-d array' assert np . isclose ( np . sum ( p ), 1 ), 'p must sum to 1' self . p = p Discrete . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): raise NotImplementedError ( \"To be implemented\" ) Discrete . gen ( self , n_samples = 1 , seed = None ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 @copy_ancestor_docstring def gen ( self , n_samples = 1 , seed = None ): # See BaseDistribution.py for docstring c = np . cumsum ( self . p [: - 1 ])[ np . newaxis , :] # cdf r = self . rng . rand ( n_samples , 1 ) return np . sum (( r > c ) . astype ( int ), axis = 1 ) . reshape ( - 1 , 1 ) Discrete . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Discrete . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed Gamma class \u00b6 Gamma . __init__ ( self , alpha = 1.0 , beta = 1.0 , offset = 0.0 , seed = None ) Univariate (!) Gamma distribution Parameters ---------- alpha : list, or np.array, 1d Shape parameters beta : list, or np.array, 1d inverse scale paramters seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 def __init__ ( self , alpha = 1. , beta = 1. , offset = 0. , seed = None ): super () . __init__ ( ndim = 1 , seed = seed ) alpha , beta = np . atleast_1d ( alpha ), np . atleast_1d ( beta ) assert alpha . ndim == 1 , 'alpha must be a 1-d array' assert alpha . size == beta . size , 'alpha and beta must match in size' assert np . all ( alpha > 0. ), 'Should be greater than zero.' assert np . all ( beta > 0. ), 'Should be greater than zero.' self . alpha = alpha self . beta = beta self . offset = offset self . _gamma = gamma ( a = alpha , scale = 1. / beta ) Gamma . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # univariate distribution only, i.e. ii=[0] in any case # x should have a second dim with length 1, not more x = np . atleast_2d ( x ) assert x . shape [ 1 ] == 1 , 'x needs second dim' assert not x . ndim > 2 , 'no more than 2 dims in x' res = self . _gamma . logpdf ( x - self . offset ) if log else self . _gamma . pdf ( x - self . offset ) # reshape to (nbatch, ) return res . reshape ( - 1 ) Gamma . gen ( self , n_samples = 1 , seed = None ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 7 8 @copy_ancestor_docstring def gen ( self , n_samples = 1 , seed = None ): # See BaseDistribution.py for docstring x = self . rng . gamma ( shape = self . alpha , scale = 1. / self . beta , size = ( n_samples , self . ndim )) + self . offset return x Gamma . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Gamma . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed Gaussian class \u00b6 Gaussian . __imul__ ( self , other ) Incrementally multiply with another Gaussian Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __imul__ ( self , other ): assert isinstance ( other , Gaussian ) res = self * other self . m = res . m self . P = res . P self . C = res . C self . S = res . S self . Pm = res . Pm self . logdetP = res . logdetP return res Gaussian . __init__ ( self , m = None , P = None , U = None , S = None , Pm = None , seed = None ) Gaussian distribution Initialize a gaussian pdf given a valid combination of its parameters. Valid combinations are: m-P, m-U, m-S, Pm-P, Pm-U, Pm-S Focus is on efficient multiplication, division and sampling. Parameters ---------- m : list or np.array, 1d Mean P : list or np.array, 2d Precision U : list or np.array, 2d Upper triangular precision factor such that U'U = P S : list or np.array, 2d Covariance C : list or np.array, 2d Upper or lower triangular covariance factor, in any case S = C'C Pm : list or np.array, 1d Precision times mean such that P*m = Pm seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , m = None , P = None , U = None , S = None , Pm = None , seed = None ): assert m is None or np . asarray ( m ) . ndim == 1 assert P is None or np . asarray ( P ) . ndim == 2 assert U is None or np . asarray ( U ) . ndim == 2 assert S is None or np . asarray ( S ) . ndim == 2 assert Pm is None or np . asarray ( Pm ) . ndim == 1 self . __div__ = self . __truediv__ self . __idiv__ = self . __itruediv__ if m is not None : m = np . asarray ( m ) self . m = m ndim = self . m . size if P is not None : P = np . asarray ( P ) L = np . linalg . cholesky ( P ) # P=LL' (lower triag) self . P = P self . C = np . linalg . inv ( L ) # C is lower triangular here # S = C'C = L^{-1}^T L^{-1} = (LL^T)^{-1} self . S = np . dot ( self . C . T , self . C ) self . Pm = np . dot ( P , m ) self . logdetP = 2.0 * np . sum ( np . log ( np . diagonal ( L ))) elif U is not None : U = np . asarray ( U ) self . P = np . dot ( U . T , U ) self . C = np . linalg . inv ( U . T ) # C is lower triangular here self . S = np . dot ( self . C . T , self . C ) self . Pm = np . dot ( self . P , m ) self . logdetP = 2.0 * np . sum ( np . log ( np . diagonal ( U ))) elif S is not None : S = np . asarray ( S ) self . P = np . linalg . inv ( S ) self . C = np . linalg . cholesky ( S ) . T # C is upper triangular here self . S = S self . Pm = np . dot ( self . P , m ) self . logdetP = - 2.0 * np . sum ( np . log ( np . diagonal ( self . C ))) else : raise ValueError ( 'Precision information missing' ) elif Pm is not None : Pm = np . asarray ( Pm ) self . Pm = Pm ndim = self . Pm . size if P is not None : P = np . asarray ( P ) L = np . linalg . cholesky ( P ) # L = np.linalg.cholesky(P + 0.001*np.identity(P.shape[0])) self . P = P self . C = np . linalg . inv ( L ) self . S = np . dot ( self . C . T , self . C ) self . m = np . linalg . solve ( P , Pm ) self . logdetP = 2.0 * np . sum ( np . log ( np . diagonal ( L ))) elif U is not None : U = np . asarray ( U ) self . P = np . dot ( U . T , U ) self . C = np . linalg . inv ( U . T ) self . S = np . dot ( self . C . T , self . C ) self . m = np . linalg . solve ( self . P , Pm ) self . logdetP = 2.0 * np . sum ( np . log ( np . diagonal ( U ))) elif S is not None : S = np . asarray ( S ) self . P = np . linalg . inv ( S ) self . C = np . linalg . cholesky ( S ) . T self . S = S self . m = np . dot ( S , Pm ) self . logdetP = - 2.0 * np . sum ( np . log ( np . diagonal ( self . C ))) else : raise ValueError ( 'Precision information missing' ) else : raise ValueError ( 'Mean information missing' ) super () . __init__ ( ndim , seed = seed ) Gaussian . __ipow__ ( self , power ) Incrementally raise gaussian to a power Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def __ipow__ ( self , power ): res = self ** power self . m = res . m self . P = res . P self . C = res . C self . S = res . S self . Pm = res . Pm self . logdetP = res . logdetP return res Gaussian . __itruediv__ ( self , other ) Incrementally divide by another Gaussian Note that the resulting Gaussian might be improper. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __itruediv__ ( self , other ): assert isinstance ( other , Gaussian ) res = self / other self . m = res . m self . P = res . P self . C = res . C self . S = res . S self . Pm = res . Pm self . logdetP = res . logdetP return res Gaussian . __mul__ ( self , other ) Multiply with another Gaussian Source Code 1 2 3 4 5 6 7 8 def __mul__ ( self , other ): assert isinstance ( other , Gaussian ) P = self . P + other . P Pm = self . Pm + other . Pm return Gaussian ( P = P , Pm = Pm , seed = self . seed ) Gaussian . __pow__ ( self , power , modulo = None ) Raise Gaussian to a power and get another Gaussian Source Code 1 2 3 4 5 6 def __pow__ ( self , power , modulo = None ): P = power * self . P Pm = power * self . Pm return Gaussian ( P = P , Pm = Pm , seed = self . seed ) Gaussian . __truediv__ ( self , other ) Divide by another Gaussian Note that the resulting Gaussian might be improper. Source Code 1 2 3 4 5 6 7 8 def __truediv__ ( self , other ): assert isinstance ( other , Gaussian ) P = self . P - other . P Pm = self . Pm - other . Pm return Gaussian ( P = P , Pm = Pm , seed = self . seed ) Gaussian . convert_to_T ( self , dof ) Converts Gaussian to Student T Parameters ---------- dof : int Degrees of freedom Source Code 1 2 3 def convert_to_T ( self , dof ): return StudentsT ( self . m , self . S , dof , seed = self . seed ) Gaussian . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # See BaseDistribution.py for docstring x = np . atleast_2d ( x ) if ii is None : assert x . shape [ 1 ] == self . ndim , \"incorrect data dimension\" xm = x - self . m lp = - np . sum ( np . dot ( xm , self . P ) * xm , axis = 1 ) lp += self . logdetP - self . ndim * np . log ( 2.0 * np . pi ) lp *= 0.5 else : ii = np . atleast_1d ( ii ) m = self . m [ ii ] S = self . S [ ii ][:, ii ] if m . size == 1 : # single marginal x = x . reshape ( - 1 , m . size ) assert x . shape [ 1 ] == m . size if np . linalg . matrix_rank ( S ) == len ( S [:, 0 ]): lp = scipy . stats . multivariate_normal . logpdf ( x , m , S , allow_singular = True ) lp = np . array ([ lp ]) if x . shape [ 0 ] == 1 else lp else : raise ValueError ( 'Rank deficiency in covariance matrix' ) res = lp if log else np . exp ( lp ) return res Gaussian . gen ( self , n_samples = 1 ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 @copy_ancestor_docstring def gen ( self , n_samples = 1 ): # See BaseDistribution.py for docstring z = self . rng . randn ( n_samples , self . ndim ) samples = np . dot ( z , self . C ) + self . m return samples Gaussian . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Gaussian . kl ( self , other ) Calculates the KL divergence from this to another Gaussian Direction of KL is KL(this | other) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def kl ( self , other ): assert isinstance ( other , Gaussian ) assert self . ndim == other . ndim t1 = np . sum ( other . P * self . S ) m = other . m - self . m t2 = np . dot ( m , np . dot ( other . P , m )) t3 = self . logdetP - other . logdetP t = 0.5 * ( t1 + t2 + t3 - self . ndim ) return t Gaussian . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed Gaussian . ztrans ( self , mean , std ) Z-transform Parameters ---------- mean : array Mean vector std : array Std vector Returns ------- Gaussian distribution Source Code 1 2 3 4 5 def ztrans ( self , mean , std ): m = ( self . m - mean ) / std S = self . S / np . outer ( std , std ) return Gaussian ( m = m , S = S , seed = self . seed ) Gaussian . ztrans_inv ( self , mean , std ) Z-transform inverse Parameters ---------- mean : array Mean vector std : array Std vector Returns ------- Gaussian distribution Source Code 1 2 3 4 5 def ztrans_inv ( self , mean , std ): m = std * self . m + mean S = np . outer ( std , std ) * self . S return Gaussian ( m = m , S = S , seed = self . seed ) Logistic class \u00b6 Logistic . __init__ ( self , mu = 0.0 , s = 1.0 , seed = None ) Distribution with independent dimensions and logistic marginals Parameters --------- mu : list, or np.array, 1d Means s : list, or np.array, 1d Scale factors seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def __init__ ( self , mu = 0.0 , s = 1.0 , seed = None ): mu , s = np . atleast_1d ( mu ), np . atleast_1d ( s ) if s . size == 1 : s = np . full ( mu . size , s [ 0 ]) assert ( s > 0 ) . all () and np . isfinite ( s ) . all () and np . isfinite ( mu ) . all () and np . isreal ( s ) . all () and \\ np . isreal ( mu ) . all (), \"bad params\" assert s . ndim == 1 and mu . ndim == 1 and mu . size == s . size , \"bad sizes\" self . mu , self . s = mu , s super () . __init__ ( ndim = mu . size , seed = seed ) Logistic . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # See BaseDistribution.py for docstring x = np . atleast_2d ( x ) assert x . shape [ 1 ] == self . ndim , \"incorrect data dimension\" if ii is None : ii = np . arange ( self . ndim ) z = ( x - self . mu ) / self . s logp_eachdim = - z - np . log ( self . s ) - 2.0 * np . log ( 1.0 + np . exp ( - z )) logp = logp_eachdim [:, ii ] . sum ( axis = 1 ) return logp if log else np . exp ( logp ) Logistic . gen ( self , n_samples = 1 ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 @copy_ancestor_docstring def gen ( self , n_samples = 1 ): # See BaseDistribution.py for docstring u = np . random . uniform ( size = ( n_samples , self . ndim )) return self . mu + self . s * ( np . log ( u ) - np . log ( 1 - u )) Logistic . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Logistic . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed Poisson class \u00b6 Poisson . __init__ ( self , mu = 0.0 , offset = 0.0 , seed = None ) Univariate (!) Poisson distribution Parameters ---------- mu: shape parameter of the Poisson (Poisson rate) offset: shift in the mean parameter, see scipy.stats.Poisson documentation. seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 def __init__ ( self , mu = 0. , offset = 0. , seed = None ): super () . __init__ ( ndim = 1 , seed = seed ) mu = np . atleast_1d ( mu ) assert mu . ndim == 1 , 'mu must be a 1-d array' assert offset >= 0 , 'offset must not be negative' self . mu = mu self . offset = offset self . _poisson = poisson ( mu = mu , loc = offset ) Poisson . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # univariate distribution only, i.e. ii=[0] in any case assert ii is None , 'this is a univariate Poisson, ii must be None.' # x should have a second dim with length 1, not more x = np . atleast_2d ( x ) assert x . shape [ 1 ] == 1 , 'x needs second dim' assert not x . ndim > 2 , 'no more than 2 dims in x' res = self . _poisson . logpmf ( x ) if log else self . _poisson . pmf ( x ) # reshape to (nbatch, ) return res . reshape ( - 1 ) Poisson . gen ( self , n_samples = 1 , seed = None ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 @copy_ancestor_docstring def gen ( self , n_samples = 1 , seed = None ): # See BaseDistribution.py for docstring x = self . _poisson . rvs ( random_state = self . rng , size = ( n_samples , self . ndim )) return x Poisson . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Poisson . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed StudentsT class \u00b6 StudentsT . __init__ ( self , m , S , dof , seed = None ) Student's T distribution Parameters ---------- m : list or np.array, 1d Mean S : list or np.array, 1d Covariance dof : int Degrees of freedom seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def __init__ ( self , m , S , dof , seed = None ): m = np . asarray ( m ) self . m = m self . dof = dof assert ( dof > 0 ) S = np . asarray ( S ) self . P = np . linalg . inv ( S ) self . C = np . linalg . cholesky ( S ) . T # C is upper triangular here self . S = S self . Pm = np . dot ( self . P , m ) self . logdetP = - 2.0 * np . sum ( np . log ( np . diagonal ( self . C ))) super () . __init__ ( ndim = m . size , seed = seed ) StudentsT . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # See BaseDistribution.py for docstring if ii is not None : raise NotImplementedError xm = x - self . m lp = np . log ( 1 + np . sum ( np . dot ( xm , self . P ) * xm , axis = 1 ) / self . dof ) lp *= - ( self . dof + self . ndim ) / 2.0 lp += np . log ( scipy . special . gamma (( self . dof + self . ndim ) / 2 )) lp -= scipy . special . gammaln ( self . dof / 2 ) + \\ self . ndim / 2 * np . log ( self . dof * np . pi ) - 0.5 * self . logdetP res = lp if log else np . exp ( lp ) return res StudentsT . gen ( self , n_samples = 1 ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 7 @copy_ancestor_docstring def gen ( self , n_samples = 1 ): # See BaseDistribution.py for docstring u = self . rng . chisquare ( self . dof , n_samples ) / self . dof y = self . rng . multivariate_normal ( np . zeros ( self . ndim ), self . S , ( n_samples ,)) return self . m + y / np . sqrt ( u )[:, None ] StudentsT . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) StudentsT . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed Uniform class \u00b6 Uniform . __init__ ( self , lower = 0.0 , upper = 1.0 , seed = None ) Uniform distribution Parameters ---------- lower : list, or np.array, 1d Lower bound(s) upper : list, or np.array, 1d Upper bound(s) seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 def __init__ ( self , lower = 0. , upper = 1. , seed = None ): self . lower = np . atleast_1d ( lower ) self . upper = np . atleast_1d ( upper ) assert self . lower . ndim == self . upper . ndim assert self . lower . ndim == 1 super () . __init__ ( ndim = len ( self . lower ), seed = seed ) Uniform . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # See BaseDistribution.py for docstring if ii is None : ii = np . arange ( self . ndim ) else : ii = np . atleast_1d ( ii ) if x . ndim == 1 and ii . size == 1 : x = x . reshape ( - 1 , 1 ) else : x = np . atleast_2d ( x ) assert x . ndim == 2 and ii . ndim == 1 assert x . shape [ 1 ] == ii . size N = x . shape [ 0 ] p = 1.0 / np . prod ( self . upper [ ii ] - self . lower [ ii ]) p = p * np . ones (( N ,)) # broadcasting # truncation of density ind = ( x > self . lower [ ii ]) & ( x < self . upper [ ii ]) p [ np . prod ( ind , axis = 1 ) == 0 ] = 0 if log : if ind . any () == False : raise ValueError ( 'log probability not defined outside of truncation' ) else : return np . log ( p ) else : return p Uniform . gen ( self , n_samples = 1 ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 @copy_ancestor_docstring def gen ( self , n_samples = 1 ): # See BaseDistribution.py for docstring ms = self . rng . rand ( n_samples , self . ndim ) * ( self . upper - self . lower ) + self . lower return ms Uniform . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Uniform . reseed ( self , seed ) Reseeds the distribution's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed MoG class \u00b6 Abstract base class for mixture distributions Distributions must at least implement abstract methods of this class. Component distributions should be added to self.xs, which is a list containing the distributions of individual components. Parameters ---------- a : list or np.array, 1d Mixing coefficients ncomp : int Number of components ndim : int Number of ndimensions of the component distributions seed : int or None If provided, random number generator will be seeded MoG . __imul__ ( self , other ) Incrementally multiply with a single gaussian Source Code 1 2 3 4 5 6 7 8 9 10 def __imul__ ( self , other ): assert isinstance ( other , Gaussian ) res = self * other self . a = res . a self . xs = res . xs return res MoG . __init__ ( self , a , ms = None , Ps = None , Us = None , Ss = None , xs = None , seed = None ) Mixture of Gaussians Creates a MoG with a valid combination of parameters or an already given list of Gaussian variables. Parameters ---------- a : list or np.array, 1d Mixing coefficients ms : list, length n_components Means Ps : list, length n_components Precisions Us : list, length n_components Precision factors such that U'U = P Ss : list, length n_components Covariances xs : list, length n_components List of gaussian variables seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , a , ms = None , Ps = None , Us = None , Ss = None , xs = None , seed = None ): self . __div__ = self . __truediv__ self . __idiv__ = self . __itruediv__ if ms is not None : super () . __init__ ( a = np . asarray ( a ), ncomp = len ( ms ), ndim = np . asarray ( ms [ 0 ]) . ndim , seed = seed ) if Ps is not None : self . xs = [ Gaussian ( m = m , P = P , seed = self . gen_newseed ()) for m , P in zip ( ms , Ps )] elif Us is not None : self . xs = [ Gaussian ( m = m , U = U , seed = self . gen_newseed ()) for m , U in zip ( ms , Us )] elif Ss is not None : self . xs = [ Gaussian ( m = m , S = S , seed = self . gen_newseed ()) for m , S in zip ( ms , Ss )] else : raise ValueError ( 'Precision information missing' ) elif xs is not None : super () . __init__ ( a = np . asarray ( a ), ncomp = len ( xs ), ndim = xs [ 0 ] . ndim , seed = seed ) self . xs = xs else : raise ValueError ( 'Mean information missing' ) MoG . __itruediv__ ( self , other ) Incrementally divide by a single gaussian Source Code 1 2 3 4 5 6 7 8 9 10 def __itruediv__ ( self , other ): assert isinstance ( other , Gaussian ) res = self / other self . a = res . a self . xs = res . xs return res MoG . __mul__ ( self , other ) Multiply with a single gaussian Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __mul__ ( self , other ): assert isinstance ( other , Gaussian ) ys = [ x * other for x in self . xs ] lcs = np . empty_like ( self . a ) for i , ( x , y ) in enumerate ( zip ( self . xs , ys )): lcs [ i ] = x . logdetP + other . logdetP - y . logdetP lcs [ i ] -= np . dot ( x . m , np . dot ( x . P , x . m )) + \\ np . dot ( other . m , np . dot ( other . P , other . m )) - \\ np . dot ( y . m , np . dot ( y . P , y . m )) lcs [ i ] *= 0.5 la = np . log ( self . a ) + lcs la -= scipy . special . logsumexp ( la ) a = np . exp ( la ) return MoG ( a = a , xs = ys , seed = self . seed ) MoG . __truediv__ ( self , other ) Divide by a single gaussian Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __truediv__ ( self , other ): assert isinstance ( other , Gaussian ) ys = [ x / other for x in self . xs ] lcs = np . empty_like ( self . a ) for i , ( x , y ) in enumerate ( zip ( self . xs , ys )): lcs [ i ] = x . logdetP - other . logdetP - y . logdetP lcs [ i ] -= np . dot ( x . m , np . dot ( x . P , x . m )) - \\ np . dot ( other . m , np . dot ( other . P , other . m )) - \\ np . dot ( y . m , np . dot ( y . P , y . m )) lcs [ i ] *= 0.5 la = np . log ( self . a ) + lcs la -= scipy . special . logsumexp ( la ) a = np . exp ( la ) return MoG ( a = a , xs = ys , seed = self . seed ) MoG . calc_mean_and_cov ( self ) Calculate the mean vector and the covariance matrix of the MoG Source Code 1 2 3 4 5 6 7 8 9 10 def calc_mean_and_cov ( self ): ms = [ x . m for x in self . xs ] m = np . dot ( self . a , np . array ( ms )) msqs = [ x . S + np . outer ( mi , mi ) for x , mi in zip ( self . xs , ms )] S = np . sum ( np . array ([ a * msq for a , msq in zip ( self . a , msqs )]), axis = 0 ) - np . outer ( m , m ) return m , S MoG . convert_to_E ( self , beta = 0.99 ) Convert to Mixture of ellipsoidal distributions Source Code 1 2 3 def convert_to_E ( self , beta = 0.99 ): return MoE ( self . a , xs = self . xs , seed = self . seed , beta = beta ) MoG . convert_to_T ( self , dofs ) Convert to Mixture of Student's T distributions Parameters ---------- dofs : int or list of ints Degrees of freedom of component distributions Source Code 1 2 3 4 5 6 def convert_to_T ( self , dofs ): if type ( dofs ) == int : dofs = [ dofs for i in range ( len ( self . xs ))] ys = [ x . convert_to_T ( dof ) for x , dof in zip ( self . xs , dofs )] return MoT ( self . a , xs = ys , seed = self . seed ) MoG . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # See BaseMixture.py for docstring ps = np . array ([ c . eval ( x , ii , log ) for c in self . xs ]) . T res = scipy . special . logsumexp ( ps + np . log ( self . a ), axis = 1 ) if log else np . dot ( ps , self . a ) return res MoG . gen ( self , n_samples = 1 ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 7 8 9 10 11 @copy_ancestor_docstring def gen ( self , n_samples = 1 ): # See BaseMixture.py for docstring ii = self . gen_comp ( n_samples ) # n_samples, ns = [ np . sum (( ii == i ) . astype ( int )) for i in range ( self . n_components )] samples = [ x . gen ( n ) for x , n in zip ( self . xs , ns )] samples = np . concatenate ( samples , axis = 0 ) self . rng . shuffle ( samples ) return samples MoG . gen_comp ( self , n_samples ) Generate component index according to self.a Source Code 1 2 3 def gen_comp ( self , n_samples ): return self . discrete_sample . gen ( n_samples ) . reshape ( - 1 ) # n_samples, MoG . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) MoG . kl ( self , other , n_samples = 10000 ) Estimates the KL from this to another PDF KL(this | other), using Monte Carlo Source Code 1 2 3 4 5 6 7 8 9 10 11 def kl ( self , other , n_samples = 10000 ): x = self . gen ( n_samples ) lp = self . eval ( x , log = True ) lq = other . eval ( x , log = True ) t = lp - lq res = np . mean ( t ) err = np . std ( t , ddof = 1 ) / np . sqrt ( n_samples ) return res , err MoG . project_to_gaussian ( self ) Returns a gaussian with the same mean and precision as the mog Source Code 1 2 3 4 def project_to_gaussian ( self ): m , S = self . calc_mean_and_cov () return Gaussian ( m = m , S = S , seed = self . seed ) MoG . prune_negligible_components ( self , threshold ) Prune components Removes all the components whose mixing coefficient is less than a threshold. Source Code 1 2 3 4 5 6 7 8 9 10 def prune_negligible_components ( self , threshold ): ii = np . nonzero (( self . a < threshold ) . astype ( int ))[ 0 ] total_del_a = np . sum ( self . a [ ii ]) del_count = ii . size self . ncomp -= del_count self . a = np . delete ( self . a , ii ) self . a += total_del_a / self . n_components self . xs = [ x for i , x in enumerate ( self . xs ) if i not in ii ] MoG . reseed ( self , seed ) Reseeds the following RNGs in the following order: 1) Master RNG for the mixture object, using the input seed 2) RNG for the discrete distribution used to sample components. The seed is generated using the master RNG. 3) RNG for each mixture component, in order. Each seed is generated by the master RNG. Source Code 1 2 3 4 5 6 7 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . discrete_sample . reseed ( seed = self . gen_newseed ()) for x in self . xs : x . reseed ( seed = self . gen_newseed ()) MoG . ztrans ( self , mean , std ) Z-transform Source Code 1 2 3 4 def ztrans ( self , mean , std ): xs = [ x . ztrans ( mean , std ) for x in self . xs ] return MoG ( self . a , xs = xs , seed = self . seed ) MoG . ztrans_inv ( self , mean , std ) Z-transform inverse Source Code 1 2 3 4 def ztrans_inv ( self , mean , std ): xs = [ x . ztrans_inv ( mean , std ) for x in self . xs ] return MoG ( self . a , xs = xs , seed = self . seed ) MoT class \u00b6 Abstract base class for mixture distributions Distributions must at least implement abstract methods of this class. Component distributions should be added to self.xs, which is a list containing the distributions of individual components. Parameters ---------- a : list or np.array, 1d Mixing coefficients ncomp : int Number of components ndim : int Number of ndimensions of the component distributions seed : int or None If provided, random number generator will be seeded MoT . __init__ ( self , a , ms = None , Ss = None , dofs = None , xs = None , seed = None ) Mixture of Student's T distributions Creates a MoT with a valid combination of parameters or an already given list of gaussian variables. Parameters ---------- a : list or 1d array Mixing coefficients ms : list of length n_components Means Ss : list of length n_components Covariances dofs: list of length n_components Degrees of freedom xs : list of length n_components List of Student's T distributions seed : int or None If provided, random number generator will be seeded Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , a , ms = None , Ss = None , dofs = None , xs = None , seed = None ): if ms is not None : super () . __init__ ( a = np . asarray ( a ), ncomp = len ( ms ), ndim = np . asarray ( ms [ 0 ]) . ndim , seed = seed ) self . xs = [ StudentsT ( m = m , S = S , dof = dof , seed = self . gen_newseed ()) for m , S , dof in zip ( ms , Ss , dofs )] elif xs is not None : super () . __init__ ( a = np . asarray ( a ), ncomp = len ( xs ), ndim = xs [ 0 ] . ndim , seed = seed ) self . xs = xs else : raise ValueError ( 'Mean information missing' ) MoT . eval ( self , x , ii = None , log = True ) Method to evaluate pdf Parameters ---------- x : int or list or np.array Rows are inputs to evaluate at ii : list A list of indices specifying which marginal to evaluate. If None, the joint pdf is evaluated log : bool, defaulting to True If True, the log pdf is evaluated Returns ------- scalar Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @copy_ancestor_docstring def eval ( self , x , ii = None , log = True ): # See BaseMixture.py for docstring if ii is not None : raise NotImplementedError ps = np . array ([ c . eval ( x , ii = None , log = log ) for c in self . xs ]) . T res = scipy . special . logsumexp ( ps + np . log ( self . a ), axis = 1 ) if log else np . dot ( ps , self . a ) return res MoT . gen ( self , n_samples = 1 ) Method to generate samples Parameters ---------- n_samples : int Number of samples to generate Returns ------- n_samples x self.ndim Source Code 1 2 3 4 5 6 7 8 9 10 11 @copy_ancestor_docstring def gen ( self , n_samples = 1 ): # See BaseMixture.py for docstring ii = self . gen_comp ( n_samples ) # n_samples, ns = [ np . sum (( ii == i ) . astype ( int )) for i in range ( self . n_components )] samples = [ x . gen ( n ) for x , n in zip ( self . xs , ns )] samples = np . concatenate ( samples , axis = 0 ) self . rng . shuffle ( samples ) return samples MoT . gen_comp ( self , n_samples ) Generate component index according to self.a Source Code 1 2 3 def gen_comp ( self , n_samples ): return self . discrete_sample . gen ( n_samples ) . reshape ( - 1 ) # n_samples, MoT . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) MoT . kl ( self , other , n_samples = 10000 ) Estimates the KL from this to another PDF KL(this | other), using Monte Carlo Source Code 1 2 3 4 5 6 7 8 9 10 11 def kl ( self , other , n_samples = 10000 ): x = self . gen ( n_samples ) lp = self . eval ( x , log = True ) lq = other . eval ( x , log = True ) t = lp - lq res = np . mean ( t ) err = np . std ( t , ddof = 1 ) / np . sqrt ( n_samples ) return res , err MoT . prune_negligible_components ( self , threshold ) Prune components Removes all the components whose mixing coefficient is less than a threshold. Source Code 1 2 3 4 5 6 7 8 9 10 def prune_negligible_components ( self , threshold ): ii = np . nonzero (( self . a < threshold ) . astype ( int ))[ 0 ] total_del_a = np . sum ( self . a [ ii ]) del_count = ii . size self . ncomp -= del_count self . a = np . delete ( self . a , ii ) self . a += total_del_a / self . n_components self . xs = [ x for i , x in enumerate ( self . xs ) if i not in ii ] MoT . reseed ( self , seed ) Reseeds the following RNGs in the following order: 1) Master RNG for the mixture object, using the input seed 2) RNG for the discrete distribution used to sample components. The seed is generated using the master RNG. 3) RNG for each mixture component, in order. Each seed is generated by the master RNG. Source Code 1 2 3 4 5 6 7 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . discrete_sample . reseed ( seed = self . gen_newseed ()) for x in self . xs : x . reseed ( seed = self . gen_newseed ())","title":"Distributions"},{"location":"reference/distributions/#distributions","text":"","title":"Distributions"},{"location":"reference/distributions/#Discrete","text":"","title":"Discrete"},{"location":"reference/distributions/#Gamma","text":"","title":"Gamma"},{"location":"reference/distributions/#Gaussian","text":"","title":"Gaussian"},{"location":"reference/distributions/#Logistic","text":"","title":"Logistic"},{"location":"reference/distributions/#Poisson","text":"","title":"Poisson"},{"location":"reference/distributions/#StudentsT","text":"","title":"StudentsT"},{"location":"reference/distributions/#Uniform","text":"","title":"Uniform"},{"location":"reference/distributions/#MoG","text":"Abstract base class for mixture distributions Distributions must at least implement abstract methods of this class. Component distributions should be added to self.xs, which is a list containing the distributions of individual components. Parameters ---------- a : list or np.array, 1d Mixing coefficients ncomp : int Number of components ndim : int Number of ndimensions of the component distributions seed : int or None If provided, random number generator will be seeded","title":"MoG"},{"location":"reference/distributions/#MoT","text":"Abstract base class for mixture distributions Distributions must at least implement abstract methods of this class. Component distributions should be added to self.xs, which is a list containing the distributions of individual components. Parameters ---------- a : list or np.array, 1d Mixing coefficients ncomp : int Number of components ndim : int Number of ndimensions of the component distributions seed : int or None If provided, random number generator will be seeded","title":"MoT"},{"location":"reference/generator/","text":"Generator \u00b6 Default class \u00b6 Default . __init__ ( self , model , prior , summary , seed = None ) Generator Parameters ---------- model : Simulator instance Forward model prior : Distribution or Mixture instance Prior over parameters summary : SummaryStats instance Summary statistics Attributes ---------- proposal : None or Distribution or Mixture instance Proposal prior over parameters. If specified, will generate samples given parameters drawn from proposal distribution rather than samples drawn from prior when `gen` is called. Source Code 1 2 3 4 5 6 7 8 def __init__ ( self , model , prior , summary , seed = None ): self . model = model self . prior = prior self . summary = summary self . proposal = None self . seed = seed self . rng = np . random . RandomState ( seed = seed ) Default . _feedback_forward_model ( self , data ) Feedback step after forward model ran Parameters ---------- data : np.array Data Returns ------- response : str Supported responses are in ['accept', 'discard'] Source Code 1 2 3 4 @copy_ancestor_docstring def _feedback_forward_model ( self , data ): # See BaseGenerator for docstring return 'accept' Default . _feedback_proposed_param ( self , param ) Feedback step after parameter has been proposed Parameters ---------- param : np.array Parameter Returns ------- response : str Supported responses are in ['accept', 'resample'] Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @copy_ancestor_docstring def _feedback_proposed_param ( self , param ): # See BaseGenerator for docstring # if prior is uniform, reject samples outside of bounds # samples might be drawn outside bounds due to proposal if isinstance ( self . prior , dd . Uniform ): if np . any ( param < self . prior . lower ) or \\ np . any ( param > self . prior . upper ): return 'resample' elif isinstance ( self . prior , dd . IndependentJoint ): for j , p in enumerate ( self . prior . dists ): ii = self . prior . dist_index_eachdim == j if isinstance ( p , dd . Uniform ): if np . any ( param [:, ii ] < p . lower ) or \\ np . any ( param [:, ii ] > p . upper ): return 'resample' elif isinstance ( p , dd . Gamma ): if np . any ( param [:, ii ] < p . offset ): return 'resample' return 'accept' Default . _feedback_summary_stats ( self , sum_stats ) Feedback step after summary stats were computed Parameters ---------- sum_stats : np.array Summary stats Returns ------- response : str Supported responses are in ['accept', 'discard'] Source Code 1 2 3 4 @copy_ancestor_docstring def _feedback_summary_stats ( self , sum_stats ): # See BaseGenerator for docstring return 'accept' Default . draw_params ( self , n_samples , skip_feedback = False , prior_mixin = 0 , verbose = True , leave_pbar = True ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def draw_params ( self , n_samples , skip_feedback = False , prior_mixin = 0 , verbose = True , leave_pbar = True ): if not verbose : pbar = no_tqdm () else : pbar = progressbar ( total = n_samples , leave = leave_pbar ) desc = 'Draw parameters ' if type ( verbose ) == str : desc += verbose pbar . set_description ( desc ) # collect valid parameter vectors from the prior params = [] # list of parameter vectors with pbar : i = 0 while i < n_samples : # sample parameter if self . proposal is None or self . rng . random_sample () < prior_mixin : proposed_param = self . prior . gen ( n_samples = 1 ) # dim params, else : proposed_param = self . proposal . gen ( n_samples = 1 ) # check if parameter vector is valid response = self . _feedback_proposed_param ( proposed_param ) if response == 'accept' or skip_feedback : # add valid param vector to list params . append ( proposed_param . reshape ( - 1 )) i += 1 pbar . update ( 1 ) elif response == 'resample' : # continue without increment on i or updating the bar continue else : raise ValueError ( 'response not supported' ) return params Default . gen ( self , n_samples , n_reps = 1 , skip_feedback = False , prior_mixin = 0 , minibatch = 50 , keep_data = True , verbose = True , leave_pbar = True ) Draw parameters and run forward model Parameters ---------- n_samples : int Number of samples n_reps: int Number of repetitions per parameter sample skip_feedback: bool If True, feedback checks on params, data and sum stats are skipped verbose : bool or str If False, will not display progress bars. If a string is passed, it will be appended to the description of the progress bar. Returns ------- params : n_samples x n_reps x n_params Parameters stats : n_samples x n_reps x n_summary Summary statistics of data Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def gen ( self , n_samples , n_reps = 1 , skip_feedback = False , prior_mixin = 0 , minibatch = 50 , keep_data = True , verbose = True , leave_pbar = True ): assert n_reps == 1 , 'n_reps > 1 is not yet supported' params = self . draw_params ( n_samples = n_samples , skip_feedback = skip_feedback , prior_mixin = prior_mixin , verbose = verbose , leave_pbar = leave_pbar ) # Run forward model for params (in batches) if not verbose : pbar = no_tqdm () else : pbar = progressbar ( total = len ( params ), leave = leave_pbar ) desc = 'Run simulations ' if type ( verbose ) == str : desc += verbose pbar . set_description ( desc ) final_params = [] final_stats = [] # list of summary stats with pbar : for params_batch in self . iterate_minibatches ( params , minibatch ): # run forward model for all params, each n_reps times result = self . model . gen ( params_batch , n_reps = n_reps , pbar = pbar ) stats , params = self . process_batch ( params_batch , result , skip_feedback = skip_feedback ) final_params += params final_stats += stats # TODO: for n_reps > 1 duplicate params; reshape stats array # n_samples x dim theta params = np . array ( final_params ) # n_samples x dim summary stats stats = np . array ( final_stats ) if len ( final_stats ) > 0 : stats = stats . squeeze ( axis = 1 ) return params , stats Default . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Default . iterate_minibatches ( self , params , minibatch = 50 ) Source Code 1 2 3 4 5 6 7 8 9 def iterate_minibatches ( self , params , minibatch = 50 ): n_samples = len ( params ) for i in range ( 0 , n_samples - minibatch + 1 , minibatch ): yield params [ i : i + minibatch ] rem_i = n_samples - ( n_samples % minibatch ) if rem_i != n_samples : yield params [ rem_i :] Default . process_batch ( self , params_batch , result , skip_feedback = False ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def process_batch ( self , params_batch , result , skip_feedback = False ): ret_stats = [] ret_params = [] # for every datum in data, check validity params_data_valid = [] # list of params with valid data data_valid = [] # list of lists containing n_reps dicts with data for param , datum in zip ( params_batch , result ): # check validity response = self . _feedback_forward_model ( datum ) if response == 'accept' or skip_feedback : data_valid . append ( datum ) # if data is accepted, accept the param as well params_data_valid . append ( param ) elif response == 'discard' : continue else : raise ValueError ( 'response not supported' ) # for every data in data, calculate summary stats for param , datum in zip ( params_data_valid , data_valid ): # calculate summary statistics sum_stats = self . summary . calc ( datum ) # n_reps x dim stats # check validity response = self . _feedback_summary_stats ( sum_stats ) if response == 'accept' or skip_feedback : ret_stats . append ( sum_stats ) # if sum stats is accepted, accept the param as well ret_params . append ( param ) elif response == 'discard' : continue else : raise ValueError ( 'response not supported' ) return ret_stats , ret_params Default . reseed ( self , seed ) Carries out the following operations, in order: 1) Reseeds the master RNG for the generator object, using the input seed 2) Reseeds the prior from the master RNG. This may cause additional distributions to be reseeded using the prior's RNG (e.g. if the prior is a mixture) 3) Reseeds the simulator RNG, from the master RNG 4) Reseeds the proposal, if present Source Code 1 2 3 4 5 6 7 8 9 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . prior . reseed ( self . gen_newseed ()) if self . model is not None : self . model . reseed ( self . gen_newseed ()) if self . proposal is not None : self . proposal . reseed ( self . gen_newseed ()) MPGenerator class \u00b6 MPGenerator . __del__ ( self ) Source Code 1 2 def __del__ ( self ): self . stop_workers () MPGenerator . __init__ ( self , models , prior , summary , rej = None , seed = None , verbose = False ) Generator supporting multiprocessing Parameters ---------- models : List of simulator instances Forward models prior : Distribution or Mixture instance Prior over parameters summary : SummaryStats instance Summary statistics rej : Function Rejection kernel Attributes ---------- proposal : None or Distribution or Mixture instance Proposal prior over parameters. If specified, will generate samples given parameters drawn from proposal distribution rather than samples drawn from prior when `gen` is called. Source Code 1 2 3 4 5 6 7 def __init__ ( self , models , prior , summary , rej = None , seed = None , verbose = False ): super () . __init__ ( model = None , prior = prior , summary = summary , seed = seed ) self . rej = rej if rej is not None else default_MPGenerator_rej self . verbose = verbose self . models = models MPGenerator . _feedback_forward_model ( self , data ) Feedback step after forward model ran Parameters ---------- data : np.array Data Returns ------- response : str Supported responses are in ['accept', 'discard'] Source Code 1 2 3 4 @copy_ancestor_docstring def _feedback_forward_model ( self , data ): # See BaseGenerator for docstring return 'accept' MPGenerator . _feedback_proposed_param ( self , param ) Feedback step after parameter has been proposed Parameters ---------- param : np.array Parameter Returns ------- response : str Supported responses are in ['accept', 'resample'] Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @copy_ancestor_docstring def _feedback_proposed_param ( self , param ): # See BaseGenerator for docstring # if prior is uniform, reject samples outside of bounds # samples might be drawn outside bounds due to proposal if isinstance ( self . prior , dd . Uniform ): if np . any ( param < self . prior . lower ) or \\ np . any ( param > self . prior . upper ): return 'resample' elif isinstance ( self . prior , dd . IndependentJoint ): for j , p in enumerate ( self . prior . dists ): ii = self . prior . dist_index_eachdim == j if isinstance ( p , dd . Uniform ): if np . any ( param [:, ii ] < p . lower ) or \\ np . any ( param [:, ii ] > p . upper ): return 'resample' elif isinstance ( p , dd . Gamma ): if np . any ( param [:, ii ] < p . offset ): return 'resample' return 'accept' MPGenerator . _feedback_summary_stats ( self , sum_stats ) Feedback step after summary stats were computed Parameters ---------- sum_stats : np.array Summary stats Returns ------- response : str Supported responses are in ['accept', 'discard'] Source Code 1 2 3 4 5 6 def _feedback_summary_stats ( self , sum_stats ): if self . rej ( sum_stats ): return 'accept' else : return 'discard' MPGenerator . draw_params ( self , n_samples , skip_feedback = False , prior_mixin = 0 , verbose = True , leave_pbar = True ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def draw_params ( self , n_samples , skip_feedback = False , prior_mixin = 0 , verbose = True , leave_pbar = True ): if not verbose : pbar = no_tqdm () else : pbar = progressbar ( total = n_samples , leave = leave_pbar ) desc = 'Draw parameters ' if type ( verbose ) == str : desc += verbose pbar . set_description ( desc ) # collect valid parameter vectors from the prior params = [] # list of parameter vectors with pbar : i = 0 while i < n_samples : # sample parameter if self . proposal is None or self . rng . random_sample () < prior_mixin : proposed_param = self . prior . gen ( n_samples = 1 ) # dim params, else : proposed_param = self . proposal . gen ( n_samples = 1 ) # check if parameter vector is valid response = self . _feedback_proposed_param ( proposed_param ) if response == 'accept' or skip_feedback : # add valid param vector to list params . append ( proposed_param . reshape ( - 1 )) i += 1 pbar . update ( 1 ) elif response == 'resample' : # continue without increment on i or updating the bar continue else : raise ValueError ( 'response not supported' ) return params MPGenerator . filter_data ( self , stats , params , skip_feedback = False ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def filter_data ( self , stats , params , skip_feedback = False ): if skip_feedback == True : return stats , params ret_stats = [] ret_params = [] for stat , param in zip ( stats , params ): response = self . _feedback_summary_stats ( stat ) if response == 'accept' : ret_stats . append ( stat ) ret_params . append ( param ) elif response == 'discard' : continue else : raise ValueError ( 'response not supported' ) return ret_stats , ret_params MPGenerator . gen ( self , n_samples , n_reps = 1 , skip_feedback = False , prior_mixin = 0 , verbose = True , ** kwargs ) Draw parameters and run forward model Parameters ---------- n_samples : int Number of samples n_reps: int Number of repetitions per parameter sample skip_feedback: bool If True, feedback checks on params, data and sum stats are skipped verbose : bool or str If False, will not display progress bars. If a string is passed, it will be appended to the description of the progress bar. Returns ------- params : n_samples x n_reps x n_params Parameters stats : n_samples x n_reps x n_summary Summary statistics of data Source Code 1 2 3 4 5 6 7 8 9 10 def gen ( self , n_samples , n_reps = 1 , skip_feedback = False , prior_mixin = 0 , verbose = True , ** kwargs ): assert n_reps == 1 , 'n_reps > 1 is not yet supported' params = self . draw_params ( n_samples = n_samples , skip_feedback = skip_feedback , prior_mixin = prior_mixin , verbose = verbose ) return self . run_model ( params , skip_feedback = skip_feedback , verbose = verbose , ** kwargs ) MPGenerator . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) MPGenerator . iterate_minibatches ( self , params , minibatch = 50 ) Source Code 1 2 3 4 5 6 7 8 9 def iterate_minibatches ( self , params , minibatch = 50 ): n_samples = len ( params ) for i in range ( 0 , n_samples - minibatch + 1 , minibatch ): yield params [ i : i + minibatch ] rem_i = n_samples - ( n_samples % minibatch ) if rem_i != n_samples : yield params [ rem_i :] MPGenerator . log ( self , msg ) Source Code 1 2 3 def log ( self , msg ): if self . verbose : print ( \"Parent: {} \" . format ( msg )) MPGenerator . process_batch ( self , params_batch , result , skip_feedback = False ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def process_batch ( self , params_batch , result , skip_feedback = False ): ret_stats = [] ret_params = [] # for every datum in data, check validity params_data_valid = [] # list of params with valid data data_valid = [] # list of lists containing n_reps dicts with data for param , datum in zip ( params_batch , result ): # check validity response = self . _feedback_forward_model ( datum ) if response == 'accept' or skip_feedback : data_valid . append ( datum ) # if data is accepted, accept the param as well params_data_valid . append ( param ) elif response == 'discard' : continue else : raise ValueError ( 'response not supported' ) # for every data in data, calculate summary stats for param , datum in zip ( params_data_valid , data_valid ): # calculate summary statistics sum_stats = self . summary . calc ( datum ) # n_reps x dim stats # check validity response = self . _feedback_summary_stats ( sum_stats ) if response == 'accept' or skip_feedback : ret_stats . append ( sum_stats ) # if sum stats is accepted, accept the param as well ret_params . append ( param ) elif response == 'discard' : continue else : raise ValueError ( 'response not supported' ) return ret_stats , ret_params MPGenerator . reseed ( self , seed ) Carries out the following operations, in order: 1) Reseeds the master RNG for the generator object, using the input seed 2) Reseeds the prior from the master RNG. This may cause additional distributions to be reseeded using the prior's RNG (e.g. if the prior is a mixture) 3) Reseeds the simulator RNG, from the master RNG 4) Reseeds the proposal, if present Source Code 1 2 3 4 5 6 7 8 9 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . prior . reseed ( self . gen_newseed ()) for m in self . models : m . reseed ( self . gen_newseed ()) if self . proposal is not None : self . proposal . reseed ( self . gen_newseed ()) MPGenerator . run_model ( self , params , minibatch = 50 , skip_feedback = False , keep_data = True , verbose = False ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def run_model ( self , params , minibatch = 50 , skip_feedback = False , keep_data = True , verbose = False ): # Run forward model for params (in batches) if not verbose : pbar = no_tqdm () else : pbar = progressbar ( total = len ( params )) desc = 'Run simulations ' if type ( verbose ) == str : desc += verbose pbar . set_description ( desc ) self . start_workers () final_params = [] final_stats = [] # list of summary stats minibatches = self . iterate_minibatches ( params , minibatch ) done = False with pbar : while not done : active_list = [] for w , p in zip ( self . workers , self . pipes ): try : params_batch = next ( minibatches ) except StopIteration : done = True break active_list . append (( w , p )) self . log ( \"Dispatching to worker (len = {} )\" . format ( len ( params_batch ))) p . send ( params_batch ) self . log ( \"Done\" ) n_remaining = len ( active_list ) while n_remaining > 0 : self . log ( \"Listening to worker\" ) msg = self . queue . get () if type ( msg ) == int : self . log ( \"Received int\" ) pbar . update ( msg ) elif type ( msg ) == tuple : self . log ( \"Received results\" ) stats , params = self . filter_data ( * msg , skip_feedback = skip_feedback ) final_stats += stats final_params += params n_remaining -= 1 else : self . log ( \"Warning: Received unknown message of type {} \" . format ( type ( msg ))) self . stop_workers () # TODO: for n_reps > 1 duplicate params; reshape stats array # n_samples x n_reps x dim theta params = np . array ( final_params ) # n_samples x n_reps x dim summary stats stats = np . array ( final_stats ) stats = stats . squeeze ( axis = 1 ) return params , stats MPGenerator . start_workers ( self ) Source Code 1 2 3 4 5 6 7 8 9 10 11 def start_workers ( self ): pipes = [ mp . Pipe ( duplex = True ) for m in self . models ] self . queue = mp . Queue () self . workers = [ Worker ( i , self . queue , pipes [ i ][ 1 ], self . models [ i ], self . summary , seed = self . rng . randint ( low = 0 , high = 2 ** 31 ), verbose = self . verbose ) for i in range ( len ( self . models )) ] self . pipes = [ p [ 0 ] for p in pipes ] self . log ( \"Starting workers\" ) for w in self . workers : w . start () self . log ( \"Done\" ) MPGenerator . stop_workers ( self ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def stop_workers ( self ): if self . workers is None : return self . log ( \"Closing\" ) for w , p in zip ( self . workers , self . pipes ): self . log ( \"Closing pipe\" ) p . close () for w in self . workers : self . log ( \"Joining process\" ) w . join ( timeout = 1 ) w . terminate () self . queue . close () self . workers = None self . pipes = None self . queue = None","title":"Generator"},{"location":"reference/generator/#generator","text":"","title":"Generator"},{"location":"reference/generator/#Default","text":"","title":"Default"},{"location":"reference/generator/#MPGenerator","text":"","title":"MPGenerator"},{"location":"reference/inference/","text":"Inference \u00b6 Basic class \u00b6 Basic . __init__ ( self , generator , obs = None , prior_norm = False , pilot_samples = 100 , reg_lambda = 0.01 , seed = None , verbose = True , ** kwargs ) Basic inference algorithm Uses samples from the prior for density estimation likelihood-free inference. Parameters ---------- generator : generator instance Generator instance obs : array or None Observation in the format the generator returns (1 x n_summary) prior_norm : bool If set to True, will z-transform params based on mean/std of prior pilot_samples : None or int If an integer is provided, a pilot run with the given number of samples is run. The mean and std of the summary statistics of the pilot samples will be subsequently used to z-transform summary statistics. reg_lambda : float Precision parameter for weight regularizer if svi is True seed : int or None If provided, random number generator will be seeded kwargs : additional keyword arguments Additional arguments for the NeuralNet instance, including: n_components : int Number of components of the mixture density n_hiddens : list of ints Number of hidden units per layer of the neural network svi : bool Whether to use SVI version of the network or not Attributes ---------- observables : dict Dictionary containing theano variables that can be monitored while training the neural network. Source Code 1 2 3 4 5 6 7 8 def __init__ ( self , generator , obs = None , prior_norm = False , pilot_samples = 100 , reg_lambda = 0.01 , seed = None , verbose = True , ** kwargs ): super () . __init__ ( generator , prior_norm = prior_norm , pilot_samples = pilot_samples , seed = seed , verbose = verbose , ** kwargs ) self . obs = obs self . reg_lambda = reg_lambda Basic . centre_on_obs ( self ) Centres first-layer input onto observed summary statistics Ensures x' = x - xo, i.e. first-layer input x' = 0 for x = xo. Source Code 1 2 3 4 def centre_on_obs ( self ): self . stats_mean = self . obs . copy () Basic . compile_observables ( self ) Creates observables dict Source Code 1 2 3 4 5 6 def compile_observables ( self ): self . observables = {} self . observables [ 'loss.lprobs' ] = self . network . lprobs for p in self . network . aps : self . observables [ str ( p )] = p Basic . conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ) Normalizes current network output at observed summary statistics Parameters ---------- fcv : float Fraction of total that comes from uncertainty over components, i.e. Var[th] = E[Var[th|z]] + Var[E[th|z]] = (1-fcv) + fcv = 1 tmu: array Target mean. tSig: array Target covariance. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ): # avoid CDELFI.predict() attempt to analytically correct for proposal print ( 'obs' , self . obs . shape ) print ( 'mean' , self . stats_mean . shape ) print ( 'std' , self . stats_std . shape ) obz = ( self . obs - self . stats_mean ) / self . stats_std posterior = self . network . get_mog ( obz . reshape ( self . obs . shape ), deterministic = True ) mog = posterior . ztrans_inv ( self . params_mean , self . params_std ) assert np . all ( np . diff ( mog . a ) == 0. ) # assumes uniform alpha n_dim = self . kwargs [ 'n_outputs' ] triu_mask = np . triu ( np . ones ([ n_dim , n_dim ], dtype = dtype ), 1 ) diag_mask = np . eye ( n_dim , dtype = dtype ) # compute MoG mean mu, Sig = E[Var[th|z]] and C = Var[E[th|z]] mu , Sig = np . zeros_like ( mog . xs [ 0 ] . m ), np . zeros_like ( mog . xs [ 0 ] . S ) for i in range ( self . network . n_components ): Sig += mog . a [ i ] * mog . xs [ i ] . S mu += mog . a [ i ] * mog . xs [ i ] . m C = np . zeros_like ( Sig ) for i in range ( self . network . n_components ): dmu = mog . xs [ i ] . m - mu if self . network . n_components > 1 \\ else mog . xs [ i ] . m C += mog . a [ i ] * np . outer ( dmu , dmu ) # if not provided, target zero-mean unit variance (as for prior_norm=True) tmu = np . zeros_like ( mog . xs [ 0 ] . m ) if tmu is None else tmu tSig = np . eye ( mog . xs [ 0 ] . m . size ) if tSig is None else tSig # compute normalizers (we only z-score, don't whiten!) Z1inv = np . sqrt (( 1. - fcv ) / np . diag ( Sig ) * np . diag ( tSig )) . reshape ( - 1 ) Z2inv = np . sqrt ( fcv / np . diag ( C ) * np . diag ( tSig )) . reshape ( - 1 ) # first we need the center of means def idx_MoG ( x ): return x . name [: 5 ] == 'means' mu_ = np . zeros_like ( mog . xs [ 0 ] . m ) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): h = np . zeros ( w . get_value () . shape [ 0 ]) if h is None else h mu_ += h . dot ( w . get_value ()) + b . get_value () mu_ /= self . network . n_components # center and normalize means # mu = Z2inv * (Wh + b - mu_) + tmu # = Wh + (Z2inv * (b - mu_ + Wh) - Wh + tum) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) b . set_value ( Z2inv * ( Wh + b . get_value () - mu_ ) - Wh + tmu ) # normalize covariances def idx_MoG ( x ): return x . name [: 10 ] == 'precisions' # Sig^-0.5 = diag_mask * (exp(Wh+b)/exp(log(Z1)) + triu_mask * (Wh+b)*Z1 # = diag_mask * exp(Wh+ (b-log(Z1)) + triu_mask * (Wh+((b+Wh)*Z1-Wh)) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) . reshape ( n_dim , n_dim ) b_ = b . get_value () . copy () . reshape ( n_dim , n_dim ) val = diag_mask * ( b_ - np . diag ( np . log ( Z1inv ))) + triu_mask * (( b_ + Wh ) . dot ( np . diag ( 1. / Z1inv )) - Wh ) b . set_value ( val . flatten ()) Basic . gen ( self , n_samples , n_reps = 1 , prior_mixin = 0 , verbose = None ) Generate from generator and z-transform Parameters ---------- n_samples : int Number of samples to generate n_reps : int Number of repeats per parameter verbose : None or bool or str If None is passed, will default to self.verbose Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def gen ( self , n_samples , n_reps = 1 , prior_mixin = 0 , verbose = None ): assert n_reps == 1 , 'n_reps > 1 is not yet supported' verbose = self . verbose if verbose is None else verbose n_pilot = np . minimum ( n_samples , len ( self . unused_pilot_samples [ 0 ])) if n_pilot > 0 and self . generator . proposal is self . generator . prior : # reuse pilot samples params = self . unused_pilot_samples [ 0 ][: n_pilot , :] stats = self . unused_pilot_samples [ 1 ][: n_pilot , :] self . unused_pilot_samples = \\ ( self . unused_pilot_samples [ 0 ][ n_pilot :, :], self . unused_pilot_samples [ 1 ][ n_pilot :, :]) n_samples -= n_pilot if n_samples > 0 : params_rem , stats_rem = self . generator . gen ( n_samples , prior_mixin = prior_mixin , verbose = verbose ) params = np . concatenate (( params , params_rem ), axis = 0 ) stats = np . concatenate (( stats , stats_rem ), axis = 0 ) else : params , stats = self . generator . gen ( n_samples , prior_mixin = prior_mixin , verbose = verbose ) # z-transform params and stats params = ( params - self . params_mean ) / self . params_std stats = ( stats - self . stats_mean ) / self . stats_std return params , stats Basic . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) Basic . loss ( self , N , round_cl = 1 ) Loss function for training Parameters ---------- N : int Number of training samples Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def loss ( self , N , round_cl = 1 ): loss = - tt . mean ( self . network . lprobs ) if self . svi : if self . round <= round_cl : # weights close to zero-centered prior in the first round if self . reg_lambda > 0 : kl , imvs = svi_kl_zero ( self . network . mps , self . network . sps , self . reg_lambda ) else : kl , imvs = 0 , {} else : # weights close to those of previous round kl , imvs = svi_kl_init ( self . network . mps , self . network . sps ) loss = loss + 1 / N * kl return loss Basic . monitor_dict_from_names ( self , monitor = None ) Generate monitor dict from list of variable names Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def monitor_dict_from_names ( self , monitor = None ): if monitor is not None : observe = {} if isinstance ( monitor , str ): monitor = [ monitor ] for m in monitor : if m in self . observables : observe [ m ] = self . observables [ m ] else : observe = None return observe Basic . norm_init ( self ) Source Code 1 2 3 4 5 6 7 def norm_init ( self ): if self . init_norm and self . network . density == 'mog' : print ( 'standardizing network initialization' ) if self . network . n_components > 1 : self . standardize_init ( fcv = self . init_fcv ) else : self . standardize_init ( fcv = 0. ) Basic . pilot_run ( self , pilot_samples , n_stats , min_std = 0.0001 ) Pilot run in order to find parameters for z-scoring stats Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def pilot_run ( self , pilot_samples , n_stats , min_std = 1e-4 ): if pilot_samples is None or \\ ( isint ( pilot_samples ) and pilot_samples == 0 ): self . unused_pilot_samples = ([], []) self . stats_mean = np . zeros ( n_stats ) self . stats_std = np . ones ( n_stats ) return if isint ( pilot_samples ): # determine via pilot run assert pilot_samples > 0 if self . seed is not None : # reseed generator for consistent inits self . generator . reseed ( self . gen_newseed ()) verbose = '(pilot run) ' if self . verbose else False params , stats = self . generator . gen ( pilot_samples , verbose = verbose ) else : # samples were provided as an input params , stats = pilot_samples self . stats_mean = np . nanmean ( stats , axis = 0 ) self . stats_std = np . nanstd ( stats , axis = 0 ) assert not np . isnan ( self . stats_mean ) . any (), \"pilot run failed\" assert not np . isnan ( self . stats_std ) . any (), \"pilot run failed\" self . stats_std [ self . stats_std == 0.0 ] = 1.0 self . stats_std = np . maximum ( self . stats_std , min_std ) assert ( self . stats_std > 0 ) . all (), \"pilot run failed\" ok_sims = np . logical_not ( np . logical_or ( np . isnan ( stats ) . any ( axis = 1 ), np . isnan ( params ) . any ( axis = 1 ))) self . unused_pilot_samples = ( params [ ok_sims , :], stats [ ok_sims , :]) Basic . predict ( self , x , deterministic = True ) Predict posterior given x Parameters ---------- x : array Stats for which to compute the posterior deterministic : bool if True, mean weights are used for Bayesian network Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def predict ( self , x , deterministic = True ): assert isinstance ( self . network , NeuralNet ) # z-transform inputs x_zt = ( x - self . stats_mean ) / self . stats_std posterior = self . network . get_density ( x_zt , deterministic = deterministic ) # z-transform outputs if self . network . density == 'mog' : posterior = posterior . ztrans_inv ( self . params_mean , self . params_std ) elif self . network . density == 'maf' : posterior . set_scale_and_offset ( scale = self . params_std , offset = self . params_mean ) else : assert np . all ( self . params_std == 1.0 ) and \\ np . all ( self . params_mean == 0.0 ) return posterior Basic . reinit_network ( self ) Reinitializes the network instance (re-setting the weights!) Source Code 1 2 3 4 5 6 7 8 def reinit_network ( self ): self . network = NeuralNet ( ** self . kwargs ) self . svi = self . network . svi if 'svi' in dir ( self . network ) else False update self . kwargs [ 'seed' ] so that reinitializing the network gives a different result each time unless we reseed the inference method self . kwargs [ 'seed' ] = self . gen_newseed () self . norm_init () Basic . remove_hidden_biases ( self ) Resets all bias weights in hidden layers to zero. Source Code 1 2 3 4 5 6 7 def remove_hidden_biases ( self ): def idx_hiddens ( x ): return x . name [ 0 ] == 'h' for b in filter ( idx_hiddens , self . network . mps_bp ): b . set_value ( np . zeros_like ( b . get_value ())) Basic . reseed ( self , seed ) reseed inference method's RNG, then generator, then network Source Code 1 2 3 4 5 6 7 8 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . kwargs [ 'seed' ] = self . gen_newseed () # for consistent NN init self . generator . reseed ( self . gen_newseed ()) # also reseeds prior + model if isinstance ( self . network , NeuralNet ): self . network . reseed ( self . gen_newseed ()) # for reproducible samples Basic . reset ( self , seed = None ) Resets inference method to a naive state, before it has seen any real or simulated data. The following happens, in order: 1) The generator's proposal is set to None, and self.round is set to 0 2) The inference method is reseeded if a seed is provided 3) The network is reinitialized 4) Any additional resetting of state specific to each inference method Source Code 1 2 3 4 5 6 7 def reset ( self , seed = None ): self . generator . proposal = None self . round = 0 if seed is not None : self . reseed ( seed ) self . reinit_network () Basic . run ( self , n_train = 100 , n_rounds = 1 , epochs = 100 , minibatch = 50 , round_cl = 1 , stop_on_nan = False , monitor = None , ** kwargs ) Run algorithm Parameters ---------- n_train : int or list of ints Number of data points drawn per round. If a list is passed, the nth list element specifies the number of training examples in the nth round. If there are fewer list elements than rounds, the last list element is used. n_rounds : int Number of rounds epochs : int Number of epochs used for neural network training minibatch : int Size of the minibatches used for neural network training monitor : list of str Names of variables to record during training along with the value of the loss function. The observables attribute contains all possible variables that can be monitored round_cl : int Round after which to start continual learning stop_on_nan : bool If True, will halt if NaNs in the loss are encountered kwargs : additional keyword arguments Additional arguments for the Trainer instance Returns ------- logs : list of dicts Dictionaries contain information logged while training the networks trn_datasets : list of (params, stats) training datasets, z-transformed posteriors : list of distributions posterior after each round Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def run ( self , n_train = 100 , n_rounds = 1 , epochs = 100 , minibatch = 50 , round_cl = 1 , stop_on_nan = False , monitor = None , ** kwargs ): logs = [] trn_datasets = [] posteriors = [] for r in range ( n_rounds ): self . round += 1 # number of training examples for this round if type ( n_train ) == list : try : n_train_round = n_train [ self . round - 1 ] except : n_train_round = n_train [ - 1 ] else : n_train_round = n_train # draw training data (z-transformed params and stats) verbose = '(round {} ) ' . format ( self . round ) if self . verbose else False trn_data = self . gen ( n_train_round , verbose = verbose ) n_train_round = trn_data [ 0 ] . shape [ 0 ] trn_data = ( trn_data [ 0 ], trn_data [ 1 ]) trn_inputs = [ self . network . params , self . network . stats ] t = Trainer ( self . network , self . loss ( N = n_train_round , round_cl = round_cl ), trn_data = trn_data , trn_inputs = trn_inputs , seed = self . gen_newseed (), monitor = self . monitor_dict_from_names ( monitor ), ** kwargs ) logs . append ( t . train ( epochs = epochs , minibatch = minibatch , verbose = verbose , stop_on_nan = stop_on_nan )) trn_datasets . append ( trn_data ) try : if self . obs is None : posteriors . append ( None ) else : posteriors . append ( self . predict ( self . obs )) except : posteriors . append ( None ) print ( 'Posterior inference failed' ) break return logs , trn_datasets , posteriors Basic . run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ) Repeatedly run the method and collect results. Optionally, carry out several runs with the same initial generator RNG state but different neural network initializations. parameters ---------- n_repeats : int Number of times to run the algorithm n_NN_inits : int Number of times to callback: function callback function that will be called after each run. It should take 4 inputs: callback(log, train_data, posterior, self) kwargs : additional keyword arguments Additional arguments that will be passed to the run() method Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ): posteriors , outputs , repeat_index = [], [], [] for r in range ( n_repeats ): if n_NN_inits_per_repeat > 1 : generator_seed = self . gen_newseed () for i in range ( n_NN_inits_per_repeat ): self . reset () if n_NN_inits_per_repeat > 1 : self . generator . reseed ( generator_seed ) log , train_data , posterior = self . run ( ** kwargs ) if callback is not None : outputs . append ( callback ( log , train_data , posterior , self )) else : outputs . append ( None ) posteriors . append ( posterior ) repeat_index . append ( r ) return posteriors , outputs , repeat_index Basic . standardize_init ( self , fcv = 0.8 ) Standardizes the network initialization on obs Ensures output distributions for xo have mean zero and unit variance. Alters hidden layers to propagates x=xo as zero to the last layer, and alters the MoG layers to produce the desired output distribution. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def standardize_init ( self , fcv = 0.8 ): assert isinstance ( self . network , NeuralNet ) # ensure x' = x - xo self . centre_on_obs () # ensure x' = 0 stays zero up to MoG layer (setting biases to zero) self . remove_hidden_biases () # ensure MoG returns standardized output on x' = 0 self . conditional_norm ( fcv ) SNPEA class \u00b6 SNPEA . __init__ ( self , generator , obs , prior_norm = False , pilot_samples = 100 , n_components = 1 , reg_lambda = 0.01 , seed = None , verbose = True , ** kwargs ) SNPE-A Implementation of Papamakarios & Murray (NeurIPS 2016) Parameters ---------- generator : generator instance Generator instance obs : array Observation in the format the generator returns (1 x n_summary) prior_norm : bool If set to True, will z-transform params based on mean/std of prior pilot_samples : None or int If an integer is provided, a pilot run with the given number of samples is run. The mean and std of the summary statistics of the pilot samples will be subsequently used to z-transform summary statistics. n_components : int Number of components in final round (PM's algorithm 2) reg_lambda : float Precision parameter for weight regularizer if svi is True seed : int or None If provided, random number generator will be seeded verbose : bool Controls whether or not progressbars are shown kwargs : additional keyword arguments Additional arguments for the NeuralNet instance, including: n_hiddens : list of ints Number of hidden units per layer of the neural network svi : bool Whether to use SVI version of the network or not Attributes ---------- observables : dict Dictionary containing theano variables that can be monitored while training the neural network. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , generator , obs , prior_norm = False , pilot_samples = 100 , n_components = 1 , reg_lambda = 0.01 , seed = None , verbose = True , ** kwargs ): assert obs is not None , \"CDELFI requires observed data\" self . obs = obs if np . any ( np . isnan ( self . obs )): raise ValueError ( \"Observed data contains NaNs\" ) super () . __init__ ( generator , prior_norm = prior_norm , pilot_samples = pilot_samples , seed = seed , verbose = verbose , ** kwargs ) # we'll use only 1 component until the last round kwargs . update ({ 'n_components' : 1 }) self . n_components = n_components self . reg_lambda = reg_lambda SNPEA . centre_on_obs ( self ) Centres first-layer input onto observed summary statistics Ensures x' = x - xo, i.e. first-layer input x' = 0 for x = xo. Source Code 1 2 3 4 def centre_on_obs ( self ): self . stats_mean = self . obs . copy () SNPEA . compile_observables ( self ) Creates observables dict Source Code 1 2 3 4 5 6 def compile_observables ( self ): self . observables = {} self . observables [ 'loss.lprobs' ] = self . network . lprobs for p in self . network . aps : self . observables [ str ( p )] = p SNPEA . conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ) Normalizes current network output at observed summary statistics Parameters ---------- fcv : float Fraction of total that comes from uncertainty over components, i.e. Var[th] = E[Var[th|z]] + Var[E[th|z]] = (1-fcv) + fcv = 1 tmu: array Target mean. tSig: array Target covariance. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ): # avoid CDELFI.predict() attempt to analytically correct for proposal print ( 'obs' , self . obs . shape ) print ( 'mean' , self . stats_mean . shape ) print ( 'std' , self . stats_std . shape ) obz = ( self . obs - self . stats_mean ) / self . stats_std posterior = self . network . get_mog ( obz . reshape ( self . obs . shape ), deterministic = True ) mog = posterior . ztrans_inv ( self . params_mean , self . params_std ) assert np . all ( np . diff ( mog . a ) == 0. ) # assumes uniform alpha n_dim = self . kwargs [ 'n_outputs' ] triu_mask = np . triu ( np . ones ([ n_dim , n_dim ], dtype = dtype ), 1 ) diag_mask = np . eye ( n_dim , dtype = dtype ) # compute MoG mean mu, Sig = E[Var[th|z]] and C = Var[E[th|z]] mu , Sig = np . zeros_like ( mog . xs [ 0 ] . m ), np . zeros_like ( mog . xs [ 0 ] . S ) for i in range ( self . network . n_components ): Sig += mog . a [ i ] * mog . xs [ i ] . S mu += mog . a [ i ] * mog . xs [ i ] . m C = np . zeros_like ( Sig ) for i in range ( self . network . n_components ): dmu = mog . xs [ i ] . m - mu if self . network . n_components > 1 \\ else mog . xs [ i ] . m C += mog . a [ i ] * np . outer ( dmu , dmu ) # if not provided, target zero-mean unit variance (as for prior_norm=True) tmu = np . zeros_like ( mog . xs [ 0 ] . m ) if tmu is None else tmu tSig = np . eye ( mog . xs [ 0 ] . m . size ) if tSig is None else tSig # compute normalizers (we only z-score, don't whiten!) Z1inv = np . sqrt (( 1. - fcv ) / np . diag ( Sig ) * np . diag ( tSig )) . reshape ( - 1 ) Z2inv = np . sqrt ( fcv / np . diag ( C ) * np . diag ( tSig )) . reshape ( - 1 ) # first we need the center of means def idx_MoG ( x ): return x . name [: 5 ] == 'means' mu_ = np . zeros_like ( mog . xs [ 0 ] . m ) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): h = np . zeros ( w . get_value () . shape [ 0 ]) if h is None else h mu_ += h . dot ( w . get_value ()) + b . get_value () mu_ /= self . network . n_components # center and normalize means # mu = Z2inv * (Wh + b - mu_) + tmu # = Wh + (Z2inv * (b - mu_ + Wh) - Wh + tum) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) b . set_value ( Z2inv * ( Wh + b . get_value () - mu_ ) - Wh + tmu ) # normalize covariances def idx_MoG ( x ): return x . name [: 10 ] == 'precisions' # Sig^-0.5 = diag_mask * (exp(Wh+b)/exp(log(Z1)) + triu_mask * (Wh+b)*Z1 # = diag_mask * exp(Wh+ (b-log(Z1)) + triu_mask * (Wh+((b+Wh)*Z1-Wh)) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) . reshape ( n_dim , n_dim ) b_ = b . get_value () . copy () . reshape ( n_dim , n_dim ) val = diag_mask * ( b_ - np . diag ( np . log ( Z1inv ))) + triu_mask * (( b_ + Wh ) . dot ( np . diag ( 1. / Z1inv )) - Wh ) b . set_value ( val . flatten ()) SNPEA . gen ( self , n_samples , n_reps = 1 , prior_mixin = 0 , verbose = None ) Generate from generator and z-transform Parameters ---------- n_samples : int Number of samples to generate n_reps : int Number of repeats per parameter verbose : None or bool or str If None is passed, will default to self.verbose Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def gen ( self , n_samples , n_reps = 1 , prior_mixin = 0 , verbose = None ): assert n_reps == 1 , 'n_reps > 1 is not yet supported' verbose = self . verbose if verbose is None else verbose n_pilot = np . minimum ( n_samples , len ( self . unused_pilot_samples [ 0 ])) if n_pilot > 0 and self . generator . proposal is self . generator . prior : # reuse pilot samples params = self . unused_pilot_samples [ 0 ][: n_pilot , :] stats = self . unused_pilot_samples [ 1 ][: n_pilot , :] self . unused_pilot_samples = \\ ( self . unused_pilot_samples [ 0 ][ n_pilot :, :], self . unused_pilot_samples [ 1 ][ n_pilot :, :]) n_samples -= n_pilot if n_samples > 0 : params_rem , stats_rem = self . generator . gen ( n_samples , prior_mixin = prior_mixin , verbose = verbose ) params = np . concatenate (( params , params_rem ), axis = 0 ) stats = np . concatenate (( stats , stats_rem ), axis = 0 ) else : params , stats = self . generator . gen ( n_samples , prior_mixin = prior_mixin , verbose = verbose ) # z-transform params and stats params = ( params - self . params_mean ) / self . params_std stats = ( stats - self . stats_mean ) / self . stats_std return params , stats SNPEA . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) SNPEA . loss ( self , N ) Loss function for training Parameters ---------- N : int Number of training samples Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def loss ( self , N ): loss = - tt . mean ( self . network . lprobs ) if self . svi : kl , imvs = svi_kl_zero ( self . network . mps , self . network . sps , self . reg_lambda ) loss = loss + 1 / N * kl # adding nodes to dict s.t. they can be monitored self . observables [ 'loss.kl' ] = kl self . observables . update ( imvs ) return loss SNPEA . monitor_dict_from_names ( self , monitor = None ) Generate monitor dict from list of variable names Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def monitor_dict_from_names ( self , monitor = None ): if monitor is not None : observe = {} if isinstance ( monitor , str ): monitor = [ monitor ] for m in monitor : if m in self . observables : observe [ m ] = self . observables [ m ] else : observe = None return observe SNPEA . norm_init ( self ) Source Code 1 2 3 4 5 6 7 def norm_init ( self ): if self . init_norm and self . network . density == 'mog' : print ( 'standardizing network initialization' ) if self . network . n_components > 1 : self . standardize_init ( fcv = self . init_fcv ) else : self . standardize_init ( fcv = 0. ) SNPEA . pilot_run ( self , pilot_samples , n_stats , min_std = 0.0001 ) Pilot run in order to find parameters for z-scoring stats Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def pilot_run ( self , pilot_samples , n_stats , min_std = 1e-4 ): if pilot_samples is None or \\ ( isint ( pilot_samples ) and pilot_samples == 0 ): self . unused_pilot_samples = ([], []) self . stats_mean = np . zeros ( n_stats ) self . stats_std = np . ones ( n_stats ) return if isint ( pilot_samples ): # determine via pilot run assert pilot_samples > 0 if self . seed is not None : # reseed generator for consistent inits self . generator . reseed ( self . gen_newseed ()) verbose = '(pilot run) ' if self . verbose else False params , stats = self . generator . gen ( pilot_samples , verbose = verbose ) else : # samples were provided as an input params , stats = pilot_samples self . stats_mean = np . nanmean ( stats , axis = 0 ) self . stats_std = np . nanstd ( stats , axis = 0 ) assert not np . isnan ( self . stats_mean ) . any (), \"pilot run failed\" assert not np . isnan ( self . stats_std ) . any (), \"pilot run failed\" self . stats_std [ self . stats_std == 0.0 ] = 1.0 self . stats_std = np . maximum ( self . stats_std , min_std ) assert ( self . stats_std > 0 ) . all (), \"pilot run failed\" ok_sims = np . logical_not ( np . logical_or ( np . isnan ( stats ) . any ( axis = 1 ), np . isnan ( params ) . any ( axis = 1 ))) self . unused_pilot_samples = ( params [ ok_sims , :], stats [ ok_sims , :]) SNPEA . predict ( self , x , threshold = 0.05 ) Predict posterior given x Parameters ---------- x : array Stats for which to compute the posterior Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def predict ( self , x , threshold = 0.05 ): if self . generator . proposal is None : # no correction necessary return super () . predict ( x ) # via super else : # mog is posterior given proposal prior mog = super () . predict ( x ) # via super mog . prune_negligible_components ( threshold = threshold ) # compute posterior given prior by analytical division step if isinstance ( self . generator . prior , dd . Uniform ): posterior = mog / self . generator . proposal elif isinstance ( self . generator . prior , dd . Gaussian ): posterior = ( mog * self . generator . prior ) / \\ self . generator . proposal else : raise NotImplemented return posterior SNPEA . reinit_network ( self ) Reinitializes the network instance (re-setting the weights!) Source Code 1 2 3 4 5 6 7 8 def reinit_network ( self ): self . network = NeuralNet ( ** self . kwargs ) self . svi = self . network . svi if 'svi' in dir ( self . network ) else False update self . kwargs [ 'seed' ] so that reinitializing the network gives a different result each time unless we reseed the inference method self . kwargs [ 'seed' ] = self . gen_newseed () self . norm_init () SNPEA . remove_hidden_biases ( self ) Resets all bias weights in hidden layers to zero. Source Code 1 2 3 4 5 6 7 def remove_hidden_biases ( self ): def idx_hiddens ( x ): return x . name [ 0 ] == 'h' for b in filter ( idx_hiddens , self . network . mps_bp ): b . set_value ( np . zeros_like ( b . get_value ())) SNPEA . reseed ( self , seed ) reseed inference method's RNG, then generator, then network Source Code 1 2 3 4 5 6 7 8 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . kwargs [ 'seed' ] = self . gen_newseed () # for consistent NN init self . generator . reseed ( self . gen_newseed ()) # also reseeds prior + model if isinstance ( self . network , NeuralNet ): self . network . reseed ( self . gen_newseed ()) # for reproducible samples SNPEA . reset ( self , seed = None ) Resets inference method to a naive state, before it has seen any real or simulated data. The following happens, in order: 1) The generator's proposal is set to None, and self.round is set to 0 2) The inference method is reseeded if a seed is provided 3) The network is reinitialized 4) Any additional resetting of state specific to each inference method Source Code 1 2 3 4 5 6 7 def reset ( self , seed = None ): self . generator . proposal = None self . round = 0 if seed is not None : self . reseed ( seed ) self . reinit_network () SNPEA . run ( self , n_train = 100 , n_rounds = 2 , epochs = 100 , minibatch = 50 , monitor = None , ** kwargs ) Run algorithm Parameters ---------- n_train : int or list of ints Number of data points drawn per round. If a list is passed, the nth list element specifies the number of training examples in the nth round. If there are fewer list elements than rounds, the last list element is used. n_rounds : int Number of rounds epochs: int Number of epochs used for neural network training minibatch: int Size of the minibatches used for neural network training monitor : list of str Names of variables to record during training along with the value of the loss function. The observables attribute contains all possible variables that can be monitored kwargs : additional keyword arguments Additional arguments for the Trainer instance Returns ------- logs : list of dicts Dictionaries contain information logged while training the networks trn_datasets : list of (params, stats) training datasets, z-transformed posteriors : list of posteriors posterior after each round Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def run ( self , n_train = 100 , n_rounds = 2 , epochs = 100 , minibatch = 50 , monitor = None , ** kwargs ): logs = [] trn_datasets = [] posteriors = [] for r in range ( n_rounds ): # start at 1 self . round += 1 # if round > 1, set new proposal distribution before sampling if self . round > 1 : # posterior becomes new proposal prior try : posterior = self . predict ( self . obs ) except : pass self . generator . proposal = posterior . project_to_gaussian () # number of training examples for this round if type ( n_train ) == list : try : n_train_round = n_train [ self . round - 1 ] except : n_train_round = n_train [ - 1 ] else : n_train_round = n_train # draw training data (z-transformed params and stats) verbose = '(round {} ) ' . format ( r ) if self . verbose else False trn_data = self . gen ( n_train_round , verbose = verbose ) # algorithm 2 of Papamakarios and Murray if r + 1 == n_rounds and self . n_components > 1 : # get parameters of current network old_params = self . network . params_dict . copy () # create new network network_spec = self . network . spec_dict . copy () network_spec . update ({ 'n_components' : self . n_components }) self . network = NeuralNet ( ** network_spec ) new_params = self . network . params_dict In order to go from 1 component in previous rounds to self . n_components in the current round we will duplicate component 1 self . n_components times , with small random perturbations to the parameters affecting component means and precisions , and the SVI s . d . s of those parameters . Set the mixture coefficients to all be equal mp_param_names = [ s for s in new_params if 'means' in s or \\ 'precisions' in s ] # list of dict keys for param_name in mp_param_names : for each param_name , get the corresponding old parameter name / value for what was previously the only mixture component param_label = re . sub ( \"\\d\" , \"\" , param_name ) # removing layer counts source_param_name = param_label + '0' source_param_val = old_params [ source_param_name ] # copy it to the new component, add noise to break symmetry old_params [ param_name ] = ( source_param_val . copy () + \\ 1.0e-6 * self . rng . randn ( * source_param_val . shape )) . astype ( dtype ) # initialize with equal mixture coefficients for all data old_params [ 'weights.mW' ] = ( 0. * new_params [ 'weights.mW' ]) . astype ( dtype ) old_params [ 'weights.mb' ] = ( 0. * new_params [ 'weights.mb' ]) . astype ( dtype ) self . network . params_dict = old_params trn_inputs = [ self . network . params , self . network . stats ] t = Trainer ( self . network , self . loss ( N = n_train_round ), trn_data = trn_data , trn_inputs = trn_inputs , monitor = self . monitor_dict_from_names ( monitor ), seed = self . gen_newseed (), ** kwargs ) logs . append ( t . train ( epochs = epochs , minibatch = minibatch , verbose = verbose )) trn_datasets . append ( trn_data ) try : posteriors . append ( self . predict ( self . obs )) except : posteriors . append ( None ) print ( 'analytic correction for proposal seemingly failed!' ) pass return logs , trn_datasets , posteriors SNPEA . run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ) Repeatedly run the method and collect results. Optionally, carry out several runs with the same initial generator RNG state but different neural network initializations. parameters ---------- n_repeats : int Number of times to run the algorithm n_NN_inits : int Number of times to callback: function callback function that will be called after each run. It should take 4 inputs: callback(log, train_data, posterior, self) kwargs : additional keyword arguments Additional arguments that will be passed to the run() method Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ): posteriors , outputs , repeat_index = [], [], [] for r in range ( n_repeats ): if n_NN_inits_per_repeat > 1 : generator_seed = self . gen_newseed () for i in range ( n_NN_inits_per_repeat ): self . reset () if n_NN_inits_per_repeat > 1 : self . generator . reseed ( generator_seed ) log , train_data , posterior = self . run ( ** kwargs ) if callback is not None : outputs . append ( callback ( log , train_data , posterior , self )) else : outputs . append ( None ) posteriors . append ( posterior ) repeat_index . append ( r ) return posteriors , outputs , repeat_index SNPEA . standardize_init ( self , fcv = 0.8 ) Standardizes the network initialization on obs Ensures output distributions for xo have mean zero and unit variance. Alters hidden layers to propagates x=xo as zero to the last layer, and alters the MoG layers to produce the desired output distribution. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def standardize_init ( self , fcv = 0.8 ): assert isinstance ( self . network , NeuralNet ) # ensure x' = x - xo self . centre_on_obs () # ensure x' = 0 stays zero up to MoG layer (setting biases to zero) self . remove_hidden_biases () # ensure MoG returns standardized output on x' = 0 self . conditional_norm ( fcv ) SNPEB class \u00b6 SNPEB . __init__ ( self , generator , obs , prior_norm = False , pilot_samples = 100 , convert_to_T = 3 , reg_lambda = 0.01 , prior_mixin = 0 , kernel = None , seed = None , verbose = True , ** kwargs ) SNPE-B Implementation of Lueckmann, Goncalves, Bassetto, \u00d6cal, Nonnenmacher & Macke (NeurIPS 2017) Parameters ---------- generator : generator instance Generator instance obs : array Observation in the format the generator returns (1 x n_summary) prior_norm : bool If set to True, will z-transform params based on mean/std of prior pilot_samples : None or int If an integer is provided, a pilot run with the given number of samples is run. The mean and std of the summary statistics of the pilot samples will be subsequently used to z-transform summary statistics. convert_to_T : None or int Convert proposal distribution to Student's T? If a number if given, the number specifies the degrees of freedom. None for no conversion reg_lambda : float Precision parameter for weight regularizer if svi is True prior_mixin : float Percentage of the prior mixed into the proposal prior. While training, an additional prior_mixin * N samples will be drawn from the actual prior in each round. seed : int or None If provided, random number generator will be seeded verbose : bool Controls whether or not progressbars are shown kwargs : additional keyword arguments Additional arguments for the NeuralNet instance, including: n_components : int Number of components of the mixture density n_hiddens : list of ints Number of hidden units per layer of the neural network svi : bool Whether to use SVI version of the network or not Attributes ---------- observables : dict Dictionary containing theano variables that can be monitored while training the neural network. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def __init__ ( self , generator , obs , prior_norm = False , pilot_samples = 100 , convert_to_T = 3 , reg_lambda = 0.01 , prior_mixin = 0 , kernel = None , seed = None , verbose = True , ** kwargs ): assert obs is not None , \"SNPE requires observed data\" self . obs = np . asarray ( obs ) super () . __init__ ( generator , prior_norm = prior_norm , pilot_samples = pilot_samples , seed = seed , verbose = verbose , ** kwargs ) if np . any ( np . isnan ( self . obs )): raise ValueError ( \"Observed data contains NaNs\" ) self . reg_lambda = reg_lambda self . convert_to_T = convert_to_T self . prior_mixin = 0 if prior_mixin is None else prior_mixin self . kernel = kernel SNPEB . centre_on_obs ( self ) Centres first-layer input onto observed summary statistics Ensures x' = x - xo, i.e. first-layer input x' = 0 for x = xo. Source Code 1 2 3 4 def centre_on_obs ( self ): self . stats_mean = self . obs . copy () SNPEB . compile_observables ( self ) Creates observables dict Source Code 1 2 3 4 5 6 def compile_observables ( self ): self . observables = {} self . observables [ 'loss.lprobs' ] = self . network . lprobs for p in self . network . aps : self . observables [ str ( p )] = p SNPEB . conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ) Normalizes current network output at observed summary statistics Parameters ---------- fcv : float Fraction of total that comes from uncertainty over components, i.e. Var[th] = E[Var[th|z]] + Var[E[th|z]] = (1-fcv) + fcv = 1 tmu: array Target mean. tSig: array Target covariance. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ): # avoid CDELFI.predict() attempt to analytically correct for proposal print ( 'obs' , self . obs . shape ) print ( 'mean' , self . stats_mean . shape ) print ( 'std' , self . stats_std . shape ) obz = ( self . obs - self . stats_mean ) / self . stats_std posterior = self . network . get_mog ( obz . reshape ( self . obs . shape ), deterministic = True ) mog = posterior . ztrans_inv ( self . params_mean , self . params_std ) assert np . all ( np . diff ( mog . a ) == 0. ) # assumes uniform alpha n_dim = self . kwargs [ 'n_outputs' ] triu_mask = np . triu ( np . ones ([ n_dim , n_dim ], dtype = dtype ), 1 ) diag_mask = np . eye ( n_dim , dtype = dtype ) # compute MoG mean mu, Sig = E[Var[th|z]] and C = Var[E[th|z]] mu , Sig = np . zeros_like ( mog . xs [ 0 ] . m ), np . zeros_like ( mog . xs [ 0 ] . S ) for i in range ( self . network . n_components ): Sig += mog . a [ i ] * mog . xs [ i ] . S mu += mog . a [ i ] * mog . xs [ i ] . m C = np . zeros_like ( Sig ) for i in range ( self . network . n_components ): dmu = mog . xs [ i ] . m - mu if self . network . n_components > 1 \\ else mog . xs [ i ] . m C += mog . a [ i ] * np . outer ( dmu , dmu ) # if not provided, target zero-mean unit variance (as for prior_norm=True) tmu = np . zeros_like ( mog . xs [ 0 ] . m ) if tmu is None else tmu tSig = np . eye ( mog . xs [ 0 ] . m . size ) if tSig is None else tSig # compute normalizers (we only z-score, don't whiten!) Z1inv = np . sqrt (( 1. - fcv ) / np . diag ( Sig ) * np . diag ( tSig )) . reshape ( - 1 ) Z2inv = np . sqrt ( fcv / np . diag ( C ) * np . diag ( tSig )) . reshape ( - 1 ) # first we need the center of means def idx_MoG ( x ): return x . name [: 5 ] == 'means' mu_ = np . zeros_like ( mog . xs [ 0 ] . m ) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): h = np . zeros ( w . get_value () . shape [ 0 ]) if h is None else h mu_ += h . dot ( w . get_value ()) + b . get_value () mu_ /= self . network . n_components # center and normalize means # mu = Z2inv * (Wh + b - mu_) + tmu # = Wh + (Z2inv * (b - mu_ + Wh) - Wh + tum) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) b . set_value ( Z2inv * ( Wh + b . get_value () - mu_ ) - Wh + tmu ) # normalize covariances def idx_MoG ( x ): return x . name [: 10 ] == 'precisions' # Sig^-0.5 = diag_mask * (exp(Wh+b)/exp(log(Z1)) + triu_mask * (Wh+b)*Z1 # = diag_mask * exp(Wh+ (b-log(Z1)) + triu_mask * (Wh+((b+Wh)*Z1-Wh)) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) . reshape ( n_dim , n_dim ) b_ = b . get_value () . copy () . reshape ( n_dim , n_dim ) val = diag_mask * ( b_ - np . diag ( np . log ( Z1inv ))) + triu_mask * (( b_ + Wh ) . dot ( np . diag ( 1. / Z1inv )) - Wh ) b . set_value ( val . flatten ()) SNPEB . gen ( self , n_samples , n_reps = 1 , prior_mixin = 0 , verbose = None ) Generate from generator and z-transform Parameters ---------- n_samples : int Number of samples to generate n_reps : int Number of repeats per parameter verbose : None or bool or str If None is passed, will default to self.verbose Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def gen ( self , n_samples , n_reps = 1 , prior_mixin = 0 , verbose = None ): assert n_reps == 1 , 'n_reps > 1 is not yet supported' verbose = self . verbose if verbose is None else verbose n_pilot = np . minimum ( n_samples , len ( self . unused_pilot_samples [ 0 ])) if n_pilot > 0 and self . generator . proposal is self . generator . prior : # reuse pilot samples params = self . unused_pilot_samples [ 0 ][: n_pilot , :] stats = self . unused_pilot_samples [ 1 ][: n_pilot , :] self . unused_pilot_samples = \\ ( self . unused_pilot_samples [ 0 ][ n_pilot :, :], self . unused_pilot_samples [ 1 ][ n_pilot :, :]) n_samples -= n_pilot if n_samples > 0 : params_rem , stats_rem = self . generator . gen ( n_samples , prior_mixin = prior_mixin , verbose = verbose ) params = np . concatenate (( params , params_rem ), axis = 0 ) stats = np . concatenate (( stats , stats_rem ), axis = 0 ) else : params , stats = self . generator . gen ( n_samples , prior_mixin = prior_mixin , verbose = verbose ) # z-transform params and stats params = ( params - self . params_mean ) / self . params_std stats = ( stats - self . stats_mean ) / self . stats_std return params , stats SNPEB . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) SNPEB . loss ( self , N , round_cl = 1 ) Loss function for training Parameters ---------- N : int Number of training samples Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def loss ( self , N , round_cl = 1 ): loss = self . network . get_loss () # adding nodes to dict s.t. they can be monitored during training self . observables [ 'loss.lprobs' ] = self . network . lprobs self . observables [ 'loss.iws' ] = self . network . iws self . observables [ 'loss.raw_loss' ] = loss if self . svi : if self . round <= round_cl : # weights close to zero-centered prior in the first round if self . reg_lambda > 0 : kl , imvs = svi_kl_zero ( self . network . mps , self . network . sps , self . reg_lambda ) else : kl , imvs = 0 , {} else : # weights close to those of previous round kl , imvs = svi_kl_init ( self . network . mps , self . network . sps ) loss = loss + 1 / N * kl # adding nodes to dict s.t. they can be monitored self . observables [ 'loss.kl' ] = kl self . observables . update ( imvs ) return loss SNPEB . monitor_dict_from_names ( self , monitor = None ) Generate monitor dict from list of variable names Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def monitor_dict_from_names ( self , monitor = None ): if monitor is not None : observe = {} if isinstance ( monitor , str ): monitor = [ monitor ] for m in monitor : if m in self . observables : observe [ m ] = self . observables [ m ] else : observe = None return observe SNPEB . norm_init ( self ) Source Code 1 2 3 4 5 6 7 def norm_init ( self ): if self . init_norm and self . network . density == 'mog' : print ( 'standardizing network initialization' ) if self . network . n_components > 1 : self . standardize_init ( fcv = self . init_fcv ) else : self . standardize_init ( fcv = 0. ) SNPEB . pilot_run ( self , pilot_samples , n_stats , min_std = 0.0001 ) Pilot run in order to find parameters for z-scoring stats Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def pilot_run ( self , pilot_samples , n_stats , min_std = 1e-4 ): if pilot_samples is None or \\ ( isint ( pilot_samples ) and pilot_samples == 0 ): self . unused_pilot_samples = ([], []) self . stats_mean = np . zeros ( n_stats ) self . stats_std = np . ones ( n_stats ) return if isint ( pilot_samples ): # determine via pilot run assert pilot_samples > 0 if self . seed is not None : # reseed generator for consistent inits self . generator . reseed ( self . gen_newseed ()) verbose = '(pilot run) ' if self . verbose else False params , stats = self . generator . gen ( pilot_samples , verbose = verbose ) else : # samples were provided as an input params , stats = pilot_samples self . stats_mean = np . nanmean ( stats , axis = 0 ) self . stats_std = np . nanstd ( stats , axis = 0 ) assert not np . isnan ( self . stats_mean ) . any (), \"pilot run failed\" assert not np . isnan ( self . stats_std ) . any (), \"pilot run failed\" self . stats_std [ self . stats_std == 0.0 ] = 1.0 self . stats_std = np . maximum ( self . stats_std , min_std ) assert ( self . stats_std > 0 ) . all (), \"pilot run failed\" ok_sims = np . logical_not ( np . logical_or ( np . isnan ( stats ) . any ( axis = 1 ), np . isnan ( params ) . any ( axis = 1 ))) self . unused_pilot_samples = ( params [ ok_sims , :], stats [ ok_sims , :]) SNPEB . predict ( self , x , deterministic = True ) Predict posterior given x Parameters ---------- x : array Stats for which to compute the posterior deterministic : bool if True, mean weights are used for Bayesian network Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def predict ( self , x , deterministic = True ): assert isinstance ( self . network , NeuralNet ) # z-transform inputs x_zt = ( x - self . stats_mean ) / self . stats_std posterior = self . network . get_density ( x_zt , deterministic = deterministic ) # z-transform outputs if self . network . density == 'mog' : posterior = posterior . ztrans_inv ( self . params_mean , self . params_std ) elif self . network . density == 'maf' : posterior . set_scale_and_offset ( scale = self . params_std , offset = self . params_mean ) else : assert np . all ( self . params_std == 1.0 ) and \\ np . all ( self . params_mean == 0.0 ) return posterior SNPEB . reinit_network ( self ) Reinitializes the network instance (re-setting the weights!) Source Code 1 2 3 4 5 6 7 8 def reinit_network ( self ): self . network = NeuralNet ( ** self . kwargs ) self . svi = self . network . svi if 'svi' in dir ( self . network ) else False update self . kwargs [ 'seed' ] so that reinitializing the network gives a different result each time unless we reseed the inference method self . kwargs [ 'seed' ] = self . gen_newseed () self . norm_init () SNPEB . remove_hidden_biases ( self ) Resets all bias weights in hidden layers to zero. Source Code 1 2 3 4 5 6 7 def remove_hidden_biases ( self ): def idx_hiddens ( x ): return x . name [ 0 ] == 'h' for b in filter ( idx_hiddens , self . network . mps_bp ): b . set_value ( np . zeros_like ( b . get_value ())) SNPEB . reseed ( self , seed ) reseed inference method's RNG, then generator, then network Source Code 1 2 3 4 5 6 7 8 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . kwargs [ 'seed' ] = self . gen_newseed () # for consistent NN init self . generator . reseed ( self . gen_newseed ()) # also reseeds prior + model if isinstance ( self . network , NeuralNet ): self . network . reseed ( self . gen_newseed ()) # for reproducible samples SNPEB . reset ( self , seed = None ) Resets inference method to a naive state, before it has seen any real or simulated data. The following happens, in order: 1) The generator's proposal is set to None, and self.round is set to 0 2) The inference method is reseeded if a seed is provided 3) The network is reinitialized 4) Any additional resetting of state specific to each inference method Source Code 1 2 3 4 5 6 7 def reset ( self , seed = None ): self . generator . proposal = None self . round = 0 if seed is not None : self . reseed ( seed ) self . reinit_network () SNPEB . run ( self , n_train = 100 , n_rounds = 2 , epochs = 100 , minibatch = 50 , round_cl = 1 , stop_on_nan = False , proposal = None , monitor = None , ** kwargs ) Run algorithm Parameters ---------- n_train : int or list of ints Number of data points drawn per round. If a list is passed, the nth list element specifies the number of training examples in the nth round. If there are fewer list elements than rounds, the last list element is used. n_rounds : int Number of rounds epochs : int Number of epochs used for neural network training minibatch : int Size of the minibatches used for neural network training monitor : list of str Names of variables to record during training along with the value of the loss function. The observables attribute contains all possible variables that can be monitored round_cl : int Round after which to start continual learning stop_on_nan : bool If True, will halt if NaNs in the loss are encountered proposal : Distribution of None If given, will use this distribution as the starting proposal prior kwargs : additional keyword arguments Additional arguments for the Trainer instance Returns ------- logs : list of dicts Dictionaries contain information logged while training the networks trn_datasets : list of (params, stats) training datasets, z-transformed posteriors : list of distributions posterior after each round Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def run ( self , n_train = 100 , n_rounds = 2 , epochs = 100 , minibatch = 50 , round_cl = 1 , stop_on_nan = False , proposal = None , monitor = None , ** kwargs ): logs = [] trn_datasets = [] posteriors = [] for r in range ( n_rounds ): self . round += 1 if r == 0 and proposal is not None : self . generator . proposal = proposal # if round > 1, set new proposal distribution before sampling elif self . round > 1 : # posterior becomes new proposal prior proposal = self . predict ( self . obs ) # see super # convert proposal to student's T? if self . convert_to_T is not None : if type ( self . convert_to_T ) == int : dofs = self . convert_to_T else : dofs = 10 proposal = proposal . convert_to_T ( dofs = dofs ) self . generator . proposal = proposal # number of training examples for this round if type ( n_train ) == list : try : n_train_round = n_train [ self . round - 1 ] except : n_train_round = n_train [ - 1 ] else : n_train_round = n_train # draw training data (z-transformed params and stats) verbose = '(round {} ) ' . format ( self . round ) if self . verbose else False trn_data = self . gen ( n_train_round , prior_mixin = self . prior_mixin , verbose = verbose ) n_train_round = trn_data [ 0 ] . shape [ 0 ] # precompute importance weights if self . generator . proposal is not None : params = self . params_std * trn_data [ 0 ] + self . params_mean p_prior = self . generator . prior . eval ( params , log = False ) p_proposal = self . generator . proposal . eval ( params , log = False ) iws = p_prior / ( self . prior_mixin * p_prior + ( 1 - self . prior_mixin ) * p_proposal ) else : iws = np . ones (( n_train_round ,)) # normalize weights iws /= np . mean ( iws ) if self . kernel is not None : iws *= self . kernel . eval ( trn_data [ 1 ] . reshape ( n_train_round , - 1 )) trn_data = ( trn_data [ 0 ], trn_data [ 1 ], iws ) trn_inputs = [ self . network . params , self . network . stats , self . network . iws ] t = Trainer ( self . network , self . loss ( N = n_train_round , round_cl = round_cl ), trn_data = trn_data , trn_inputs = trn_inputs , seed = self . gen_newseed (), monitor = self . monitor_dict_from_names ( monitor ), ** kwargs ) logs . append ( t . train ( epochs = epochs , minibatch = minibatch , verbose = verbose , stop_on_nan = stop_on_nan )) trn_datasets . append ( trn_data ) try : posteriors . append ( self . predict ( self . obs )) except np . linalg . LinAlgError : posteriors . append ( None ) print ( \"Cannot predict posterior after round {} due to NaNs\" . format ( r )) break return logs , trn_datasets , posteriors SNPEB . run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ) Repeatedly run the method and collect results. Optionally, carry out several runs with the same initial generator RNG state but different neural network initializations. parameters ---------- n_repeats : int Number of times to run the algorithm n_NN_inits : int Number of times to callback: function callback function that will be called after each run. It should take 4 inputs: callback(log, train_data, posterior, self) kwargs : additional keyword arguments Additional arguments that will be passed to the run() method Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ): posteriors , outputs , repeat_index = [], [], [] for r in range ( n_repeats ): if n_NN_inits_per_repeat > 1 : generator_seed = self . gen_newseed () for i in range ( n_NN_inits_per_repeat ): self . reset () if n_NN_inits_per_repeat > 1 : self . generator . reseed ( generator_seed ) log , train_data , posterior = self . run ( ** kwargs ) if callback is not None : outputs . append ( callback ( log , train_data , posterior , self )) else : outputs . append ( None ) posteriors . append ( posterior ) repeat_index . append ( r ) return posteriors , outputs , repeat_index SNPEB . standardize_init ( self , fcv = 0.8 ) Standardizes the network initialization on obs Ensures output distributions for xo have mean zero and unit variance. Alters hidden layers to propagates x=xo as zero to the last layer, and alters the MoG layers to produce the desired output distribution. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def standardize_init ( self , fcv = 0.8 ): assert isinstance ( self . network , NeuralNet ) # ensure x' = x - xo self . centre_on_obs () # ensure x' = 0 stays zero up to MoG layer (setting biases to zero) self . remove_hidden_biases () # ensure MoG returns standardized output on x' = 0 self . conditional_norm ( fcv ) SNPEC class \u00b6 SNPEC . __init__ ( self , generator , obs = None , prior_norm = False , pilot_samples = 100 , reg_lambda = 0.01 , seed = None , verbose = True , add_prior_precision = True , Ptol = None , ** kwargs ) SNPE-C/APT Implementation of Greenberg, Nonnenmacher & Macke (ICML 2019) Parameters ---------- generator : generator instance Generator instance obs : array Observation in the format the generator returns (1 x n_summary) prior_norm : bool If set to True, will z-transform params based on mean/std of prior pilot_samples : None or int If an integer is provided, a pilot run with the given number of samples is run. The mean and std of the summary statistics of the pilot samples will be subsequently used to z-transform summary statistics. n_components : int Number of components in final round (PM's algorithm 2) reg_lambda : float Precision parameter for weight regularizer if svi is True seed : int or None If provided, random number generator will be seeded verbose : bool Controls whether or not progressbars are shown add_prior_precision: bool Whether to add the prior precision to each posterior component for Gauss/MoG proposals Ptol: float Quantity added to the diagonal entries of the precision matrix for each Gaussian posterior component kwargs : additional keyword arguments Additional arguments for the NeuralNet instance, including: n_hiddens : list of ints Number of hidden units per layer of the neural network svi : bool Whether to use SVI version of the network or not Attributes ---------- observables : dict Dictionary containing theano variables that can be monitored while training the neural network. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , generator , obs = None , prior_norm = False , pilot_samples = 100 , reg_lambda = 0.01 , seed = None , verbose = True , add_prior_precision = True , Ptol = None , ** kwargs ): assert obs is not None , \"APT requires observed data\" self . obs = np . asarray ( obs ) super () . __init__ ( generator , prior_norm = prior_norm , pilot_samples = pilot_samples , seed = seed , verbose = verbose , ** kwargs ) # initializes network assert 0 < self . obs . ndim <= 2 if self . obs . ndim == 1 : self . obs = self . obs . reshape ( 1 , - 1 ) assert self . obs . shape [ 0 ] == 1 if np . any ( np . isnan ( self . obs )): raise ValueError ( \"Observed data contains NaNs\" ) self . Ptol = np . finfo ( dtype ) . resolution if Ptol is None else Ptol self . add_prior_precision = add_prior_precision self . reg_lambda = reg_lambda self . exception_info = ( None , None , None ) self . trn_datasets , self . proposal_used = [], [] SNPEC . centre_on_obs ( self ) Centres first-layer input onto observed summary statistics Ensures x' = x - xo, i.e. first-layer input x' = 0 for x = xo. Source Code 1 2 3 4 def centre_on_obs ( self ): self . stats_mean = self . obs . copy () SNPEC . compile_observables ( self ) Creates observables dict Source Code 1 2 3 4 5 6 def compile_observables ( self ): self . observables = {} self . observables [ 'loss.lprobs' ] = self . network . lprobs for p in self . network . aps : self . observables [ str ( p )] = p SNPEC . conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ) Normalizes current network output at observed summary statistics Parameters ---------- fcv : float Fraction of total that comes from uncertainty over components, i.e. Var[th] = E[Var[th|z]] + Var[E[th|z]] = (1-fcv) + fcv = 1 tmu: array Target mean. tSig: array Target covariance. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def conditional_norm ( self , fcv = 0.8 , tmu = None , tSig = None , h = None ): # avoid CDELFI.predict() attempt to analytically correct for proposal print ( 'obs' , self . obs . shape ) print ( 'mean' , self . stats_mean . shape ) print ( 'std' , self . stats_std . shape ) obz = ( self . obs - self . stats_mean ) / self . stats_std posterior = self . network . get_mog ( obz . reshape ( self . obs . shape ), deterministic = True ) mog = posterior . ztrans_inv ( self . params_mean , self . params_std ) assert np . all ( np . diff ( mog . a ) == 0. ) # assumes uniform alpha n_dim = self . kwargs [ 'n_outputs' ] triu_mask = np . triu ( np . ones ([ n_dim , n_dim ], dtype = dtype ), 1 ) diag_mask = np . eye ( n_dim , dtype = dtype ) # compute MoG mean mu, Sig = E[Var[th|z]] and C = Var[E[th|z]] mu , Sig = np . zeros_like ( mog . xs [ 0 ] . m ), np . zeros_like ( mog . xs [ 0 ] . S ) for i in range ( self . network . n_components ): Sig += mog . a [ i ] * mog . xs [ i ] . S mu += mog . a [ i ] * mog . xs [ i ] . m C = np . zeros_like ( Sig ) for i in range ( self . network . n_components ): dmu = mog . xs [ i ] . m - mu if self . network . n_components > 1 \\ else mog . xs [ i ] . m C += mog . a [ i ] * np . outer ( dmu , dmu ) # if not provided, target zero-mean unit variance (as for prior_norm=True) tmu = np . zeros_like ( mog . xs [ 0 ] . m ) if tmu is None else tmu tSig = np . eye ( mog . xs [ 0 ] . m . size ) if tSig is None else tSig # compute normalizers (we only z-score, don't whiten!) Z1inv = np . sqrt (( 1. - fcv ) / np . diag ( Sig ) * np . diag ( tSig )) . reshape ( - 1 ) Z2inv = np . sqrt ( fcv / np . diag ( C ) * np . diag ( tSig )) . reshape ( - 1 ) # first we need the center of means def idx_MoG ( x ): return x . name [: 5 ] == 'means' mu_ = np . zeros_like ( mog . xs [ 0 ] . m ) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): h = np . zeros ( w . get_value () . shape [ 0 ]) if h is None else h mu_ += h . dot ( w . get_value ()) + b . get_value () mu_ /= self . network . n_components # center and normalize means # mu = Z2inv * (Wh + b - mu_) + tmu # = Wh + (Z2inv * (b - mu_ + Wh) - Wh + tum) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) b . set_value ( Z2inv * ( Wh + b . get_value () - mu_ ) - Wh + tmu ) # normalize covariances def idx_MoG ( x ): return x . name [: 10 ] == 'precisions' # Sig^-0.5 = diag_mask * (exp(Wh+b)/exp(log(Z1)) + triu_mask * (Wh+b)*Z1 # = diag_mask * exp(Wh+ (b-log(Z1)) + triu_mask * (Wh+((b+Wh)*Z1-Wh)) for w , b in zip ( filter ( idx_MoG , self . network . mps_wp ), filter ( idx_MoG , self . network . mps_bp )): Wh = h . dot ( w . get_value ()) . reshape ( n_dim , n_dim ) b_ = b . get_value () . copy () . reshape ( n_dim , n_dim ) val = diag_mask * ( b_ - np . diag ( np . log ( Z1inv ))) + triu_mask * (( b_ + Wh ) . dot ( np . diag ( 1. / Z1inv )) - Wh ) b . set_value ( val . flatten ()) SNPEC . define_loss ( self , n , round_cl = 1 , proposal = 'gaussian' , combined_loss = False ) Loss function for training Parameters ---------- n : int Number of training samples round_cl : int Round after which to start continual learning proposal : str Specifier for type of proposal used: continuous ('gaussian', 'mog') or 'atomic' proposals are implemented. combined_loss : bool Whether to include prior likelihood terms in addition to atomic Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def define_loss ( self , n , round_cl = 1 , proposal = 'gaussian' , combined_loss = False ): prior = self . generator . prior if isinstance ( prior , dd . Gaussian ) or isinstance ( prior , dd . MoG ): prior = prior . ztrans ( self . params_mean , self . params_std ) if proposal == 'prior' : # using prior as proposal loss , trn_inputs = snpe_loss_prior_as_proposal ( self . network , svi = self . svi ) elif proposal == 'gaussian' : assert self . network . density == 'mog' assert isinstance ( self . generator . proposal , dd . Gaussian ) loss , trn_inputs = apt_loss_gaussian_proposal ( self . network , prior , svi = self . svi , add_prior_precision = self . add_prior_precision ) elif proposal . lower () == 'mog' : assert self . network . density == 'mog' assert isinstance ( self . generator . proposal , dd . MoG ) loss , trn_inputs = apt_loss_MoG_proposal ( self . network , prior , svi = self . svi , add_prior_precision = self . add_prior_precision ) elif proposal == 'atomic' : loss , trn_inputs = \\ apt_loss_atomic_proposal ( self . network , svi = self . svi , combined_loss = combined_loss ) else : raise NotImplemented () # adding nodes to dict s.t. they can be monitored during training self . observables [ 'loss.lprobs' ] = self . network . lprobs self . observables [ 'loss.raw_loss' ] = loss if self . svi : if self . round <= round_cl : # weights close to zero-centered prior in the first round if self . reg_lambda > 0 : kl , imvs = svi_kl_zero ( self . network . mps , self . network . sps , self . reg_lambda ) else : kl , imvs = 0 , {} else : # weights close to those of previous round kl , imvs = svi_kl_init ( self . network . mps , self . network . sps ) loss = loss + 1 / n * kl # adding nodes to dict s.t. they can be monitored self . observables [ 'loss.kl' ] = kl self . observables . update ( imvs ) return loss , trn_inputs SNPEC . epochs_round ( self , epochs ) Source Code 1 2 3 4 5 6 7 8 9 10 11 def epochs_round ( self , epochs ): # number of training examples for this round if type ( epochs ) == list : try : epochs_round = epochs [ self . round - 1 ] except : epochs_round = epochs [ - 1 ] else : epochs_round = epochs return epochs_round SNPEC . gen ( self , n_train , project_to_gaussian = False , ** kwargs ) Generate from generator and z-transform Parameters ---------- n_samples : int Number of samples to generate n_reps : int Number of repeats per parameter verbose : None or bool or str If None is passed, will default to self.verbose project_to_gaussian: bool Whether to always return Gaussian objects (instead of MoG) n_train: int Number of training samples Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 def gen ( self , n_train , project_to_gaussian = False , ** kwargs ): if 'verbose' in kwargs . keys (): verbose = kwargs [ 'verbose' ] else : verbose = self . verbose verbose = '(round {} ) ' . format ( self . round ) if verbose else False n_train_round = self . n_train_round ( n_train ) trn_data = super () . gen ( n_train_round , verbose = verbose , ** kwargs ) n_train_round = trn_data [ 0 ] . shape [ 0 ] # may have decreased (rejection) return trn_data , n_train_round SNPEC . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) SNPEC . loss ( self ) Source Code 1 2 3 @abc . abstractmethod def loss ( self ): pass SNPEC . monitor_dict_from_names ( self , monitor = None ) Generate monitor dict from list of variable names Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def monitor_dict_from_names ( self , monitor = None ): if monitor is not None : observe = {} if isinstance ( monitor , str ): monitor = [ monitor ] for m in monitor : if m in self . observables : observe [ m ] = self . observables [ m ] else : observe = None return observe SNPEC . n_train_round ( self , n_train ) Source Code 1 2 3 4 5 6 7 8 9 10 11 def n_train_round ( self , n_train ): # number of training examples for this round if type ( n_train ) == list : try : n_train_round = n_train [ self . round - 1 ] except : n_train_round = n_train [ - 1 ] else : n_train_round = n_train return n_train_round SNPEC . norm_init ( self ) Source Code 1 2 3 4 5 6 7 def norm_init ( self ): if self . init_norm and self . network . density == 'mog' : print ( 'standardizing network initialization' ) if self . network . n_components > 1 : self . standardize_init ( fcv = self . init_fcv ) else : self . standardize_init ( fcv = 0. ) SNPEC . pilot_run ( self , pilot_samples , n_stats , min_std = 0.0001 ) Pilot run in order to find parameters for z-scoring stats Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def pilot_run ( self , pilot_samples , n_stats , min_std = 1e-4 ): if pilot_samples is None or \\ ( isint ( pilot_samples ) and pilot_samples == 0 ): self . unused_pilot_samples = ([], []) self . stats_mean = np . zeros ( n_stats ) self . stats_std = np . ones ( n_stats ) return if isint ( pilot_samples ): # determine via pilot run assert pilot_samples > 0 if self . seed is not None : # reseed generator for consistent inits self . generator . reseed ( self . gen_newseed ()) verbose = '(pilot run) ' if self . verbose else False params , stats = self . generator . gen ( pilot_samples , verbose = verbose ) else : # samples were provided as an input params , stats = pilot_samples self . stats_mean = np . nanmean ( stats , axis = 0 ) self . stats_std = np . nanstd ( stats , axis = 0 ) assert not np . isnan ( self . stats_mean ) . any (), \"pilot run failed\" assert not np . isnan ( self . stats_std ) . any (), \"pilot run failed\" self . stats_std [ self . stats_std == 0.0 ] = 1.0 self . stats_std = np . maximum ( self . stats_std , min_std ) assert ( self . stats_std > 0 ) . all (), \"pilot run failed\" ok_sims = np . logical_not ( np . logical_or ( np . isnan ( stats ) . any ( axis = 1 ), np . isnan ( params ) . any ( axis = 1 ))) self . unused_pilot_samples = ( params [ ok_sims , :], stats [ ok_sims , :]) SNPEC . predict ( self , * args , ** kwargs ) Predict posterior given x Parameters ---------- x : array Stats for which to compute the posterior deterministic : bool if True, mean weights are used for Bayesian network Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def predict ( self , * args , ** kwargs ): p = super () . predict ( * args , ** kwargs ) if self . round > 0 and self . proposal_used [ - 1 ] in [ 'gaussian' , 'mog' ]: assert self . network . density == 'mog' and isinstance ( p , dd . MoG ) P_offset = np . eye ( p . ndim ) * self . Ptol # add the prior precision to each posterior component if needed if self . add_prior_precision and isinstance ( self . generator . prior , dd . Gaussian ): P_offset += self . generator . prior . P p = dd . MoG ( a = p . a , xs = [ dd . Gaussian ( m = x . m , P = x . P + P_offset , seed = x . seed ) for x in p . xs ]) return p SNPEC . reinit_network ( self ) Reinitializes the network instance (re-setting the weights!) Source Code 1 2 3 4 5 6 7 8 def reinit_network ( self ): self . network = NeuralNet ( ** self . kwargs ) self . svi = self . network . svi if 'svi' in dir ( self . network ) else False update self . kwargs [ 'seed' ] so that reinitializing the network gives a different result each time unless we reseed the inference method self . kwargs [ 'seed' ] = self . gen_newseed () self . norm_init () SNPEC . remove_hidden_biases ( self ) Resets all bias weights in hidden layers to zero. Source Code 1 2 3 4 5 6 7 def remove_hidden_biases ( self ): def idx_hiddens ( x ): return x . name [ 0 ] == 'h' for b in filter ( idx_hiddens , self . network . mps_bp ): b . set_value ( np . zeros_like ( b . get_value ())) SNPEC . reseed ( self , seed ) reseed inference method's RNG, then generator, then network Source Code 1 2 3 4 5 6 7 8 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed self . kwargs [ 'seed' ] = self . gen_newseed () # for consistent NN init self . generator . reseed ( self . gen_newseed ()) # also reseeds prior + model if isinstance ( self . network , NeuralNet ): self . network . reseed ( self . gen_newseed ()) # for reproducible samples SNPEC . reset ( self , seed = None ) Resets inference method to a naive state, before it has seen any real or simulated data. The following happens, in order: 1) The generator's proposal is set to None, and self.round is set to 0 2) The inference method is reseeded if a seed is provided 3) The network is reinitialized 4) Any additional resetting of state specific to each inference method Source Code 1 2 3 def reset ( self , seed = None ): super () . reset ( seed = seed ) self . trn_datasets , self . proposal_used = [], [] SNPEC . run ( self , n_rounds = 1 , proposal = 'gaussian' , silent_fail = True , ** kwargs ) Run algorithm Parameters ---------- n_train : int or list of ints Number of data points drawn per round. If a list is passed, the nth list element specifies the number of training examples in the nth round. If there are fewer list elements than rounds, the last list element is used. n_rounds : int Number of rounds proposal : str Specifier for type of proposal used: continuous ('gaussian', 'mog') or 'atomic' proposals are implemented. epochs : int Number of epochs used for neural network training minibatch : int Size of the minibatches used for neural network training monitor : list of str Names of variables to record during training along with the value of the loss function. The observables attribute contains all possible variables that can be monitored round_cl : int Round after which to start continual learning stop_on_nan : bool If True, will halt if NaNs in the loss are encountered silent_fail : bool If true, will continue without throwing an error when a round fails kwargs : additional keyword arguments Additional arguments for the Trainer instance Returns ------- logs : list of dicts Dictionaries contain information logged while training the networks trn_datasets : list of (params, stats) training datasets, z-transformed posteriors : list of distributions posterior after each round Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def run ( self , n_rounds = 1 , proposal = 'gaussian' , silent_fail = True , ** kwargs ): # support 'discrete' instead of 'atomic' for backwards compatibility if proposal == 'discrete' : proposal = 'atomic' elif proposal == 'discrete_comb' : proposal = 'atomic_comb' logs = [] trn_datasets = [] posteriors = [] if 'train_on_all' in kwargs . keys () and kwargs [ 'train_on_all' ] is True : kwargs [ 'round_cl' ] = np . inf if proposal == 'gaussian' and self . network . n_components > 1 and \\ 'reuse_prior_samples' not in kwargs . keys (): # prevent numerical instability (broad unused comps) kwargs [ 'reuse_prior_samples' ] = False for r in range ( n_rounds ): self . round += 1 if silent_fail : try : log , trn_data = self . run_round ( proposal , ** kwargs ) except : print ( 'Round {0} failed' . format ( self . round )) import sys self . exception_info = sys . exc_info () break else : log , trn_data = self . run_round ( proposal , ** kwargs ) logs . append ( log ) trn_datasets . append ( trn_data ) posteriors . append ( self . predict ( self . obs )) return logs , trn_datasets , posteriors SNPEC . run_MoG ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = None , moo = None , train_on_all = False , round_cl = 1 , stop_on_nan = False , monitor = None , verbose = False , print_each_epoch = False , reuse_prior_samples = True , patience = 20 , monitor_every = None , ** kwargs ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def run_MoG ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = None , moo = None , train_on_all = False , round_cl = 1 , stop_on_nan = False , monitor = None , verbose = False , print_each_epoch = False , reuse_prior_samples = True , patience = 20 , monitor_every = None , ** kwargs ): # simulate data self . set_proposal ( project_to_gaussian = False ) assert isinstance ( self . generator . proposal , dd . MoG ) prop = self . generator . proposal . ztrans ( self . params_mean , self . params_std ) trn_data , n_train_round = self . gen ( n_train ) trn_data = ( * trn_data , * MoG_prop_APT_training_vars ( prop , n_train_round , prop . n_components )) self . trn_datasets . append ( trn_data ) if train_on_all : prev_datasets = [] for i , d in enumerate ( self . trn_datasets ): if self . proposal_used [ i ] == 'mog' : prev_datasets . append ( d ) elif self . proposal_used == 'prior' and reuse_prior_samples : prior = self . generator . prior if not isinstance ( prior , dd . Uniform ): prior = prior . ztrans ( self . params_mean , self . params_std ) d = ( * d , * MoG_prop_APT_training_vars ( prior , n_train_round )) prev_datasets . append ( d ) elif self . proposal_used [ i ] == 'gaussian' : params , stats , prop_m , prop_P = d if np . diff ( prop_m , axis = 0 ) . any () or np . diff ( prop_P , axis = 0 ) . any (): continue # reusing samples with proposals that changed within a round is not yet supported prop = dd . Gaussian ( m = prop_m [ 0 ], P = prop_P [ 0 ]) d = ( params , stats , * MoG_prop_APT_training_vars ( prop , n_train_round )) prev_datasets . append ( d ) else : # can't re-use samples from this proposal continue trn_data = combine_trn_datasets ( prev_datasets ) n_train_round = trn_data [ 0 ] . shape [ 0 ] self . loss , trn_inputs = self . define_loss ( n = n_train_round , round_cl = round_cl , proposal = 'mog' ) t = Trainer ( self . network , self . loss , trn_data = trn_data , trn_inputs = trn_inputs , seed = self . gen_newseed (), monitor = self . monitor_dict_from_names ( monitor ), ** kwargs ) log = t . train ( epochs = self . epochs_round ( epochs ), minibatch = minibatch , verbose = verbose , print_each_epoch = print_each_epoch , stop_on_nan = stop_on_nan , patience = patience , monitor_every = monitor_every ) return log , trn_data SNPEC . run_atomic ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = 10 , moo = 'resample' , train_on_all = False , reuse_prior_samples = True , combined_loss = False , round_cl = 1 , stop_on_nan = False , monitor = None , patience = 20 , monitor_every = None , verbose = False , print_each_epoch = False , ** kwargs ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def run_atomic ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = 10 , moo = 'resample' , train_on_all = False , reuse_prior_samples = True , combined_loss = False , round_cl = 1 , stop_on_nan = False , monitor = None , patience = 20 , monitor_every = None , verbose = False , print_each_epoch = False , ** kwargs ): # activetrainer doesn't de-norm params before evaluating the prior assert np . all ( self . params_mean == 0.0 ) and np . all ( self . params_std == 1.0 ), \"prior_norm + atomic not supported\" assert minibatch > 1 , \"minimum minibatch size 2 for atomic proposals\" if n_atoms is None : n_atoms = minibatch - 1 if theano . config . device . startswith ( 'cuda' ) else np . minimum ( minibatch - 1 , 9 ) assert n_atoms < minibatch , \"Minibatch too small for this many atoms\" # simulate data self . set_proposal () trn_data , n_train_round = self . gen ( n_train ) self . trn_datasets . append ( trn_data ) # don't store prior_masks if train_on_all : if reuse_prior_samples : trn_data = combine_trn_datasets ( self . trn_datasets , max_inputs = 2 ) else : trn_data = combine_trn_datasets ( [ td for td , pu in zip ( self . trn_datasets , self . proposal_used ) if pu != 'prior' ]) if combined_loss : prior_masks = \\ [ np . ones ( td [ 0 ] . shape [ 0 ], dtype ) * ( pu == 'prior' ) for td , pu in zip ( self . trn_datasets , self . proposal_used )] trn_data = ( * trn_data , np . concatenate ( prior_masks )) n_train_round = trn_data [ 0 ] . shape [ 0 ] # train network self . loss , trn_inputs = self . define_loss ( n = n_train_round , round_cl = round_cl , proposal = 'atomic' , combined_loss = combined_loss and train_on_all ) t = ActiveTrainer ( self . network , self . loss , trn_data = trn_data , trn_inputs = trn_inputs , seed = self . gen_newseed (), monitor = self . monitor_dict_from_names ( monitor ), generator = self . generator , n_atoms = n_atoms , moo = moo , obs = ( self . obs - self . stats_mean ) / self . stats_std , ** kwargs ) log = t . train ( epochs = self . epochs_round ( epochs ), minibatch = minibatch , verbose = verbose , print_each_epoch = print_each_epoch , strict_batch_size = True , patience = patience , monitor_every = monitor_every ) return log , trn_data SNPEC . run_gaussian ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = None , moo = None , train_on_all = False , round_cl = 1 , stop_on_nan = False , monitor = None , verbose = False , print_each_epoch = False , patience = 20 , monitor_every = None , reuse_prior_samples = True , ** kwargs ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def run_gaussian ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = None , moo = None , train_on_all = False , round_cl = 1 , stop_on_nan = False , monitor = None , verbose = False , print_each_epoch = False , patience = 20 , monitor_every = None , reuse_prior_samples = True , ** kwargs ): # simulate data self . set_proposal ( project_to_gaussian = True ) assert isinstance ( self . generator . proposal , dd . Gaussian ) prop = self . generator . proposal . ztrans ( self . params_mean , self . params_std ) trn_data , n_train_round = self . gen ( n_train ) prop_m = np . expand_dims ( prop . m , 0 ) . repeat ( n_train_round , axis = 0 ) prop_P = np . expand_dims ( prop . P , 0 ) . repeat ( n_train_round , axis = 0 ) trn_data = ( * trn_data , prop_m , prop_P ) self . trn_datasets . append ( trn_data ) if train_on_all : prev_datasets = [] for i , d in enumerate ( self . trn_datasets ): if self . proposal_used [ i ] == 'gaussian' : prev_datasets . append ( d ) continue elif self . proposal_used [ i ] != 'prior' or not reuse_prior_samples : continue # prior samples. the Gauss loss will reduce to the prior loss if isinstance ( self . generator . prior , dd . Gaussian ): prior = self . generator . prior . ztrans ( self . params_mean , self . params_std ) prop_m = prior . mean prop_P = prior . P elif isinstance ( self . generator . prior , dd . Uniform ): # model a uniform as an zero-precision Gaussian: prop_m = np . zeros ( self . generator . prior . ndim , dtype ) prop_P = np . zeros (( self . generator . prior . ndim , self . generator . prior . ndim ), dtype ) else : # can't reuse prior samples unless prior is uniform or Gaussian continue prop_m = np . expand_dims ( prop_m , 0 ) . repeat ( d [ 0 ] . shape [ 0 ], axis = 0 ) prop_P = np . expand_dims ( prop_P , 0 ) . repeat ( d [ 0 ] . shape [ 0 ], axis = 0 ) prev_datasets . append (( * d , prop_m , prop_P )) trn_data = combine_trn_datasets ( prev_datasets ) n_train_round = trn_data [ 0 ] . shape [ 0 ] # train network self . loss , trn_inputs = self . define_loss ( n = n_train_round , round_cl = round_cl , proposal = 'gaussian' ) t = Trainer ( self . network , self . loss , trn_data = trn_data , trn_inputs = trn_inputs , seed = self . gen_newseed (), monitor = self . monitor_dict_from_names ( monitor ), ** kwargs ) log = t . train ( epochs = self . epochs_round ( epochs ), minibatch = minibatch , verbose = verbose , print_each_epoch = print_each_epoch , stop_on_nan = stop_on_nan , patience = patience , monitor_every = monitor_every ) return log , trn_data SNPEC . run_prior ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = None , moo = None , train_on_all = False , round_cl = 1 , stop_on_nan = False , monitor = None , verbose = False , print_each_epoch = False , patience = 20 , monitor_every = None , reuse_prior_samples = True , ** kwargs ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def run_prior ( self , n_train = 100 , epochs = 100 , minibatch = 50 , n_atoms = None , moo = None , train_on_all = False , round_cl = 1 , stop_on_nan = False , monitor = None , verbose = False , print_each_epoch = False , patience = 20 , monitor_every = None , reuse_prior_samples = True , ** kwargs ): # simulate data self . generator . proposal = self . generator . prior trn_data , n_train_round = self . gen ( n_train ) self . trn_datasets . append ( trn_data ) if train_on_all and reuse_prior_samples : prior_datasets = [ d for i , d in enumerate ( self . trn_datasets ) if self . proposal_used [ i ] == 'prior' ] trn_data = combine_trn_datasets ( prior_datasets ) n_train_round = trn_data [ 0 ] . shape [ 0 ] # train network self . loss , trn_inputs = self . define_loss ( n = n_train_round , round_cl = round_cl , proposal = 'prior' ) t = Trainer ( self . network , self . loss , trn_data = trn_data , trn_inputs = trn_inputs , seed = self . gen_newseed (), monitor = self . monitor_dict_from_names ( monitor ), ** kwargs ) log = t . train ( epochs = self . epochs_round ( epochs ), minibatch = minibatch , verbose = verbose , print_each_epoch = print_each_epoch , stop_on_nan = stop_on_nan , patience = patience , monitor_every = monitor_every ) return log , trn_data SNPEC . run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ) Repeatedly run the method and collect results. Optionally, carry out several runs with the same initial generator RNG state but different neural network initializations. parameters ---------- n_repeats : int Number of times to run the algorithm n_NN_inits : int Number of times to callback: function callback function that will be called after each run. It should take 4 inputs: callback(log, train_data, posterior, self) kwargs : additional keyword arguments Additional arguments that will be passed to the run() method Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def run_repeated ( self , n_repeats = 10 , n_NN_inits_per_repeat = 1 , callback = None , ** kwargs ): posteriors , outputs , repeat_index = [], [], [] for r in range ( n_repeats ): if n_NN_inits_per_repeat > 1 : generator_seed = self . gen_newseed () for i in range ( n_NN_inits_per_repeat ): self . reset () if n_NN_inits_per_repeat > 1 : self . generator . reseed ( generator_seed ) log , train_data , posterior = self . run ( ** kwargs ) if callback is not None : outputs . append ( callback ( log , train_data , posterior , self )) else : outputs . append ( None ) posteriors . append ( posterior ) repeat_index . append ( r ) return posteriors , outputs , repeat_index SNPEC . run_round ( self , proposal = None , ** kwargs ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def run_round ( self , proposal = None , ** kwargs ): proposal = proposal . lower () if self . round > 1 else 'prior' self . proposal_used . append ( proposal ) if proposal == 'prior' or self . round == 1 : return self . run_prior ( ** kwargs ) elif proposal == 'gaussian' : return self . run_gaussian ( ** kwargs ) elif proposal == 'mog' : return self . run_MoG ( ** kwargs ) elif proposal == 'atomic' : return self . run_atomic ( combined_loss = False , ** kwargs ) elif proposal == 'atomic_comb' : return self . run_atomic ( combined_loss = True , ** kwargs ) else : raise NotImplemented () SNPEC . set_proposal ( self , project_to_gaussian = False ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def set_proposal ( self , project_to_gaussian = False ): # posterior estimate becomes new proposal prior if self . round == 0 : return None posterior = self . predict ( self . obs ) if project_to_gaussian : assert self . network . density == 'mog' , \"cannot project a MAF\" posterior = posterior . project_to_gaussian () self . generator . proposal = posterior SNPEC . standardize_init ( self , fcv = 0.8 ) Standardizes the network initialization on obs Ensures output distributions for xo have mean zero and unit variance. Alters hidden layers to propagates x=xo as zero to the last layer, and alters the MoG layers to produce the desired output distribution. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 def standardize_init ( self , fcv = 0.8 ): assert isinstance ( self . network , NeuralNet ) # ensure x' = x - xo self . centre_on_obs () # ensure x' = 0 stays zero up to MoG layer (setting biases to zero) self . remove_hidden_biases () # ensure MoG returns standardized output on x' = 0 self . conditional_norm ( fcv )","title":"Inference"},{"location":"reference/inference/#inference","text":"","title":"Inference"},{"location":"reference/inference/#Basic","text":"","title":"Basic"},{"location":"reference/inference/#SNPEA","text":"","title":"SNPEA"},{"location":"reference/inference/#SNPEB","text":"","title":"SNPEB"},{"location":"reference/inference/#SNPEC","text":"","title":"SNPEC"},{"location":"reference/neuralnet/","text":"Neural Networks \u00b6 NeuralNet class \u00b6 NeuralNet . __init__ ( self , n_inputs = None , n_outputs = None , input_shape = None , n_bypass = 0 , density = 'mog' , n_hiddens = ( 10 , 10 ), impute_missing = True , seed = None , n_filters = (), filter_sizes = 3 , pool_sizes = 2 , n_rnn = 0 , ** density_opts ) Initialize a mixture density network with custom layers Parameters ---------- n_inputs : int Total input dimensionality (data/summary stats) n_outputs : int Dimensionality of output (simulator parameters) input_shape : tuple Size to which data are reshaped before CNN or RNN n_bypass : int Number of elements at end of input which bypass CNN or RNN density : string Type of density condition on the network, can be 'mog' or 'maf' n_components : int Number of components of the mixture density n_filters : list of ints Number of filters per convolutional layer n_hiddens : list of ints Number of hidden units per fully connected layer n_rnn : None or int Number of RNN units impute_missing : bool If set to True, learns replacement value for NaNs, otherwise those inputs are set to zero seed : int or None If provided, random number generator will be seeded density_opts : dict Options for the density estimator Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , n_inputs = None , n_outputs = None , input_shape = None , n_bypass = 0 , density = 'mog' , n_hiddens = ( 10 , 10 ), impute_missing = True , seed = None , n_filters = (), filter_sizes = 3 , pool_sizes = 2 , n_rnn = 0 , ** density_opts ): if n_rnn > 0 and len ( n_filters ) > 0 : raise NotImplementedError assert isint ( n_inputs ) and isint ( n_outputs ) \\ and n_inputs > 0 and n_outputs > 0 self . density = density . lower () self . impute_missing = impute_missing self . n_hiddens = list ( n_hiddens ) self . n_outputs , self . n_inputs = n_outputs , n_inputs self . n_bypass = n_bypass self . n_rnn = n_rnn self . n_filters , self . filter_sizes , self . pool_sizes , n_cnn = \\ list ( n_filters ), filter_sizes , pool_sizes , len ( n_filters ) if type ( self . filter_sizes ) is int : self . filter_sizes = [ self . filter_sizes for _ in range ( n_cnn )] else : assert len ( self . filter_sizes ) >= n_cnn if type ( self . pool_sizes ) is int : self . pool_sizes = [ self . pool_sizes for _ in range ( n_cnn )] else : assert len ( self . pool_sizes ) >= n_cnn self . iws = tt . vector ( 'iws' , dtype = dtype ) self . seed = seed if seed is not None : self . rng = np . random . RandomState ( seed = seed ) else : self . rng = np . random . RandomState () lasagne . random . set_rng ( self . rng ) self . input_shape = ( n_inputs ,) if input_shape is None else input_shape assert np . prod ( self . input_shape ) + self . n_bypass == self . n_inputs assert 1 <= len ( self . input_shape ) <= 3 # params: output placeholder (batch, self.n_outputs) self . params = tensorN ( 2 , name = 'params' , dtype = dtype ) # stats : input placeholder, (batch, self.n_inputs) self . stats = tensorN ( 2 , name = 'stats' , dtype = dtype ) # compose layers self . layer = collections . OrderedDict () # input layer, None indicates batch size not fixed at compile time self . layer [ 'input' ] = ll . InputLayer ( ( None , self . n_inputs ), input_var = self . stats ) # learn replacement values if self . impute_missing : self . layer [ 'missing' ] = \\ dl . ImputeMissingLayer ( last ( self . layer ), n_inputs = ( self . n_inputs ,)) else : self . layer [ 'missing' ] = \\ dl . ReplaceMissingLayer ( last ( self . layer ), n_inputs = ( self . n_inputs ,)) if self . n_bypass > 0 and ( self . n_rnn > 0 or n_cnn > 0 ): last_layer = last ( self . layer ) bypass_slice = slice ( self . n_inputs - self . n_bypass , self . n_inputs ) direct_slice = slice ( 0 , self . n_inputs - self . n_bypass ) self . layer [ 'bypass' ] = ll . SliceLayer ( last_layer , bypass_slice ) self . layer [ 'direct' ] = ll . SliceLayer ( last_layer , direct_slice ) # reshape inputs prior to RNN or CNN step if self . n_rnn > 0 or n_cnn > 0 : if len ( n_filters ) > 0 and len ( self . input_shape ) == 2 : # 1 channel rs = ( - 1 , 1 , * self . input_shape ) else : if self . n_rnn > 0 : assert len ( self . input_shape ) == 2 # time, dim else : assert len ( self . input_shape ) == 3 # channel, row, col rs = ( - 1 , * self . input_shape ) # last layer is 'missing' or 'direct' self . layer [ 'reshape' ] = ll . ReshapeLayer ( last ( self . layer ), rs ) # recurrent neural net, input: (batch, sequence_length, num_inputs) if self . n_rnn > 0 : self . layer [ 'rnn' ] = ll . GRULayer ( last ( self . layer ), n_rnn , only_return_final = True ) # convolutional net, input: (batch, channels, rows, columns) if n_cnn > 0 : for l in range ( n_cnn ): # add layers if self . pool_sizes [ l ] == 1 : padding = ( self . filter_sizes [ l ] - 1 ) // 2 else : padding = 0 self . layer [ 'conv_' + str ( l + 1 )] = ll . Conv2DLayer ( name = 'c' + str ( l + 1 ), incoming = last ( self . layer ), num_filters = self . n_filters [ l ], filter_size = self . filter_sizes [ l ], stride = ( 1 , 1 ), pad = padding , untie_biases = False , W = lasagne . init . GlorotUniform (), b = lasagne . init . Constant ( 0. ), nonlinearity = lnl . rectify , flip_filters = True , convolution = tt . nnet . conv2d ) if self . pool_sizes [ l ] > 1 : self . layer [ 'pool_' + str ( l + 1 )] = ll . MaxPool2DLayer ( name = 'p' + str ( l + 1 ), incoming = last ( self . layer ), pool_size = self . pool_sizes [ l ], stride = None , ignore_border = True ) # flatten self . layer [ 'flatten' ] = ll . FlattenLayer ( incoming = last ( self . layer ), outdim = 2 ) # incorporate bypass inputs if self . n_bypass > 0 and ( self . n_rnn > 0 or n_cnn > 0 ): self . layer [ 'bypass_merge' ] = lasagne . layers . ConcatLayer ( [ self . layer [ 'bypass' ], last ( self . layer )], axis = 1 ) if self . density == 'mog' : self . init_mdn ( ** density_opts ) elif self . density == 'maf' : self . init_maf ( ** density_opts ) else : raise NotImplementedError self . compile_funs () # theano functions NeuralNet . compile_funs ( self ) Compiles theano functions Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def compile_funs ( self ): self . _f_eval_lprobs = theano . function ( inputs = [ self . params , self . stats ], outputs = self . lprobs ) self . _f_eval_dlprobs = theano . function ( inputs = [ self . params , self . stats ], outputs = self . dlprobs ) if self . density == 'mog' : self . _f_eval_comps = theano . function ( inputs = [ self . stats ], outputs = self . comps ) self . _f_eval_dcomps = theano . function ( inputs = [ self . stats ], outputs = self . dcomps ) elif self . density == 'maf' : self . _f_eval_maf_input = theano . function ( inputs = [ self . stats ], outputs = self . maf_input ) NeuralNet . eval_comps ( self , stats , deterministic = True ) Evaluate the parameters of all mixture components at given inputs Parameters ---------- stats : np.array rows are input locations deterministic : bool if True, mean weights are used for Bayesian network Returns ------- mixing coefficients, means and scale matrices Source Code 1 2 3 4 5 6 7 def eval_comps ( self , stats , deterministic = True ): assert self . density == 'mog' if deterministic : return self . _f_eval_dcomps ( stats . astype ( dtype )) else : return self . _f_eval_comps ( stats . astype ( dtype )) NeuralNet . eval_lprobs ( self , params , stats , deterministic = True ) Evaluate log probabilities for given input-output pairs. Parameters ---------- params : np.array stats : np.array deterministic : bool if True, mean weights are used for Bayesian network Returns ------- log probabilities : log p(params|stats) Source Code 1 2 3 4 5 6 def eval_lprobs ( self , params , stats , deterministic = True ): if deterministic : return self . _f_eval_dlprobs ( params . astype ( dtype ), stats . astype ( dtype )) else : return self . _f_eval_lprobs ( params . astype ( dtype ), stats . astype ( dtype )) NeuralNet . gen_newseed ( self ) Generates a new random seed Source Code 1 2 3 4 5 6 def gen_newseed ( self ): if self . seed is None : return None else : return self . rng . randint ( 0 , 2 ** 31 ) NeuralNet . get_density ( self , stats , deterministic = True ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def get_density ( self , stats , deterministic = True ): assert stats . size == self . n_inputs stats = stats . reshape ( 1 , - 1 ) . astype ( dtype ) if self . density == 'mog' : return self . get_mog ( stats , deterministic = deterministic ) elif self . density == 'maf' : assert deterministic cmaf_input = self . _f_eval_maf_input ( stats ) return MAFconditional ( model = self . cmaf , cmaf_inputs = cmaf_input . reshape ( - 1 ), makecopy = True , rng = np . random . RandomState ( seed = self . gen_newseed ())) else : raise NotImplementedError NeuralNet . get_loss ( self ) Source Code 1 2 def get_loss ( self ): return - tt . mean ( self . iws * self . lprobs ) NeuralNet . get_mog ( self , stats , deterministic = True ) Return the conditional MoG at location x Parameters ---------- stats : np.array single input location deterministic : bool if True, mean weights are used for Bayesian network Source Code 1 2 3 4 5 6 7 8 9 10 11 def get_mog ( self , stats , deterministic = True ): assert self . density == 'mog' assert stats . shape [ 0 ] == 1 , 'x.shape[0] needs to be 1' comps = self . eval_comps ( stats , deterministic ) a = comps [ 'a' ][ 0 ] ms = [ comps [ 'm' + str ( i )][ 0 ] for i in range ( self . n_components )] Us = [ comps [ 'U' + str ( i )][ 0 ] for i in range ( self . n_components )] return dd . MoG ( a = a , ms = ms , Us = Us , seed = self . gen_newseed ()) NeuralNet . get_mog_tensors ( self , svi = True , return_extras = False ) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def get_mog_tensors ( self , svi = True , return_extras = False ): assert self . density == 'mog' if svi : a , ms , Us , ldetUs = self . a , self . ms , self . Us , self . ldetUs else : a , ms , Us , ldetUs = self . da , self . dms , self . dUs , self . dldetUs if not return_extras : return a , ms , Us , ldetUs # precisions of posterior components: Ps = [ tt . batched_dot ( U . dimshuffle ( 0 , 2 , 1 ), U ) for U in Us ] # log determinants of posterior component precisions ldetPs = [ 2 * ldetU for ldetU in ldetUs ] # precision times mean for each posterior component: Pms = [ tt . sum ( P * m . dimshuffle ( 0 , 'x' , 1 ), axis = 2 ) for m , P in zip ( ms , Ps )] # calculate tensorQF(P, m): QFs = [ tt . sum ( m * Pm , axis = 1 ) for m , Pm in zip ( ms , Pms )] return a , ms , Us , ldetUs , Ps , ldetPs , Pms , QFs NeuralNet . init_maf ( self , n_mades = 5 , batch_norm = False , maf_actfun = 'tanh' , output_order = 'random' , maf_mode = 'random' , ** unused_kwargs ) :param n_mades: :param batch_norm: :param output_order: :param maf_mode: :param unused_kwargs: :return: Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def init_maf ( self , n_mades = 5 , batch_norm = False , maf_actfun = 'tanh' , output_order = 'random' , maf_mode = 'random' , ** unused_kwargs ): if batch_norm : raise NotImplementedError # why? self . n_mades , self . batch_norm , self . output_order , self . maf_mode = \\ n_mades , batch_norm , output_order , maf_mode self . maf_actfun = maf_actfun for key in unused_kwargs . keys (): print ( \"CMAF ignoring unused input {0} \" . format ( key )) # get previous output/params self . maf_input = ll . get_output ( last ( self . layer )) prev_params = ll . get_all_params ( last ( self . layer )) input_shape_cmaf = last ( self . layer ) . output_shape assert len ( input_shape_cmaf ) == 2 # (batch, input_dim) n_inputs_cmaf = input_shape_cmaf [ 1 ] rng_maf = np . random . RandomState ( seed = self . gen_newseed ()) self . cmaf = ConditionalMaskedAutoregressiveFlow ( n_inputs = n_inputs_cmaf , n_outputs = self . n_outputs , n_hiddens = self . n_hiddens , act_fun = self . maf_actfun , n_mades = self . n_mades , batch_norm = self . batch_norm , output_order = self . output_order , mode = self . maf_mode , input = self . maf_input , output = self . params , rng = rng_maf ) self . aps = prev_params + self . cmaf . parms self . lprobs = self . cmaf . L # model log-likelihood self . dlprobs = self . lprobs # svi not possible NeuralNet . init_mdn ( self , svi = False , n_components = 1 , rank = None , mdn_actfun =< function tanh at 0x1270264d0 > , homoscedastic = False , min_precisions = None , ** unused_kwargs ) :param svi: bool Whether to use SVI version or not :param n_components: int :param rank: int :param homoscedastic: bool :param unused_kwargs: dict :param mdn_actfun: lasagne nonlinearity activation function for hidden units :param min_precisions: minimum values for diagonal elements of precision matrix for all components (usually taken to be prior precisions) :return: None Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def init_mdn ( self , svi = False , n_components = 1 , rank = None , mdn_actfun = lnl . tanh , homoscedastic = False , min_precisions = None , ** unused_kwargs ): self . svi , self . n_components , self . rank , self . mdn_actfun , \\ self . homoscedastic , self . min_precisions = \\ svi , n_components , rank , mdn_actfun , homoscedastic , min_precisions for key in unused_kwargs . keys (): print ( \"MDN ignoring unused input {0} \" . format ( key )) # hidden layers for l in range ( len ( self . n_hiddens )): self . layer [ 'hidden_' + str ( l + 1 )] = dl . FullyConnectedLayer ( last ( self . layer ), n_units = self . n_hiddens [ l ], actfun = self . mdn_actfun , svi = self . svi , name = 'h' + str ( l + 1 )) last_hidden = last ( self . layer ) # mixture layers self . layer [ 'mixture_weights' ] = dl . MixtureWeightsLayer ( last_hidden , n_units = self . n_components , actfun = lnl . softmax , svi = self . svi , name = 'weights' ) self . layer [ 'mixture_means' ] = dl . MixtureMeansLayer ( last_hidden , n_components = self . n_components , n_dim = self . n_outputs , svi = self . svi , name = 'means' ) if self . homoscedastic : PrecisionsLayer = dl . MixtureHomoscedasticPrecisionsLayer else : PrecisionsLayer = dl . MixturePrecisionsLayer # why is homoscedastic an input to the layer init? self . layer [ 'mixture_precisions' ] = PrecisionsLayer ( last_hidden , n_components = self . n_components , n_dim = self . n_outputs , svi = self . svi , name = 'precisions' , rank = self . rank , homoscedastic = self . homoscedastic , min_precisions = min_precisions ) last_mog = [ self . layer [ 'mixture_weights' ], self . layer [ 'mixture_means' ], self . layer [ 'mixture_precisions' ]] # mixture parameters # a : weights, matrix with shape (batch, n_components) # ms : means, list of len n_components with (batch, n_dim, n_dim) # Us : precision factors, n_components list with (batch, n_dim, n_dim) # ldetUs : log determinants of precisions, n_comp list with (batch, ) self . a , self . ms , precision_out = ll . get_output ( last_mog , deterministic = False ) self . Us = precision_out [ 'Us' ] self . ldetUs = precision_out [ 'ldetUs' ] self . comps = { ** { 'a' : self . a }, ** { 'm' + str ( i ): self . ms [ i ] for i in range ( self . n_components )}, ** { 'U' + str ( i ): self . Us [ i ] for i in range ( self . n_components )}} # log probability of y given the mixture distribution # lprobs_comps : log probs per component, list of len n_components with (batch, ) # probs : log probs of mixture, (batch, ) self . lprobs_comps = [ - 0.5 * tt . sum ( tt . sum (( self . params - m ) . dimshuffle ( [ 0 , 'x' , 1 ]) * U , axis = 2 ) ** 2 , axis = 1 ) + ldetU for m , U , ldetU in zip ( self . ms , self . Us , self . ldetUs )] self . lprobs = ( MyLogSumExp ( tt . stack ( self . lprobs_comps , axis = 1 ) + tt . log ( self . a ), axis = 1 ) - ( 0.5 * self . n_outputs * np . log ( 2 * np . pi ))) . squeeze () # the quantities from above again, but with deterministic=True # --- in the svi case, this will disable injection of randomness; # the mean of weights is used instead self . da , self . dms , dprecision_out = ll . get_output ( last_mog , deterministic = True ) self . dUs = dprecision_out [ 'Us' ] self . dldetUs = dprecision_out [ 'ldetUs' ] self . dcomps = { ** { 'a' : self . da }, ** { 'm' + str ( i ): self . dms [ i ] for i in range ( self . n_components )}, ** { 'U' + str ( i ): self . dUs [ i ] for i in range ( self . n_components )}} self . dlprobs_comps = [ - 0.5 * tt . sum ( tt . sum (( self . params - m ) . dimshuffle ( [ 0 , 'x' , 1 ]) * U , axis = 2 ) ** 2 , axis = 1 ) + ldetU for m , U , ldetU in zip ( self . dms , self . dUs , self . dldetUs )] self . dlprobs = ( MyLogSumExp ( tt . stack ( self . dlprobs_comps , axis = 1 ) + tt . log ( self . da ), axis = 1 ) \\ - ( 0.5 * self . n_outputs * np . log ( 2 * np . pi ))) . squeeze () # parameters of network self . aps = ll . get_all_params ( last_mog ) # all parameters self . mps = ll . get_all_params ( last_mog , mp = True ) # means self . sps = ll . get_all_params ( last_mog , sp = True ) # log stds # weight and bias parameter sets as separate lists self . mps_wp = ll . get_all_params ( last_mog , mp = True , wp = True ) self . sps_wp = ll . get_all_params ( last_mog , sp = True , wp = True ) self . mps_bp = ll . get_all_params ( last_mog , mp = True , bp = True ) self . sps_bp = ll . get_all_params ( last_mog , sp = True , bp = True ) NeuralNet . reseed ( self , seed ) Reseeds the network's RNG Source Code 1 2 3 4 def reseed ( self , seed ): self . rng . seed ( seed = seed ) self . seed = seed ConditionalMaskedAutoregressiveFlow class \u00b6 Implements a Conditional Masked Autoregressive Flow. ConditionalMaskedAutoregressiveFlow . __init__ ( self , n_inputs , n_outputs , n_hiddens , act_fun , n_mades , batch_norm = True , output_order = 'sequential' , mode = 'sequential' , input = None , output = None , rng =< module 'numpy.random' from '/Users/jm/opt/anaconda3/envs/ind/lib/python3.7/site-packages/numpy/random/__init__.py' > ) Constructor. :param n_inputs: number of (conditional) inputs :param n_outputs: number of outputs :param n_hiddens: list with number of hidden units for each hidden layer :param act_fun: name of activation function :param n_mades: number of mades in the flow :param batch_norm: whether to use batch normalization between mades in the flow :param output_order: order of outputs of last made :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential' :param input: theano variable to serve as input; if None, a new variable is created :param output: theano variable to serve as output; if None, a new variable is created Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , n_inputs , n_outputs , n_hiddens , act_fun , n_mades , batch_norm = True , output_order = 'sequential' , mode = 'sequential' , input = None , output = None , rng = np . random ): # save input arguments self . n_inputs = n_inputs self . n_outputs = n_outputs self . n_hiddens = n_hiddens self . act_fun = act_fun self . n_mades = n_mades self . batch_norm = batch_norm self . mode = mode self . input = tt . matrix ( 'x' , dtype = dtype ) if input is None else input self . y = tt . matrix ( 'y' , dtype = dtype ) if output is None else output self . parms = [] self . mades = [] self . bns = [] self . u = self . y self . logdet_dudy = 0.0 for i in range ( n_mades ): # create a new made made = mades . ConditionalGaussianMade ( n_inputs , n_outputs , n_hiddens , act_fun , output_order , mode , self . input , self . u , rng ) self . mades . append ( made ) self . parms += made . parms output_order = output_order if output_order == 'random' else made . output_order [:: - 1 ] # inverse autoregressive transform self . u = made . u self . logdet_dudy += 0.5 * tt . sum ( made . logp , axis = 1 ) # batch normalization if batch_norm : bn = layers . BatchNorm ( self . u , n_outputs ) self . u = bn . y self . parms += bn . parms self . logdet_dudy += tt . sum ( bn . log_gamma ) - 0.5 * tt . sum ( tt . log ( bn . v )) self . bns . append ( bn ) self . output_order = self . mades [ 0 ] . output_order # log likelihoods self . L = - 0.5 * n_outputs * np . log ( 2 * np . pi ) - 0.5 * tt . sum ( self . u ** 2 , axis = 1 ) + self . logdet_dudy self . L . name = 'L' # train objective self . trn_loss = - tt . mean ( self . L ) self . trn_loss . name = 'trn_loss' # theano evaluation functions, will be compiled when first needed self . eval_lprob_f = None self . eval_grad_f = None self . eval_score_f = None self . eval_us_f = None ConditionalMaskedAutoregressiveFlow . calc_random_numbers ( self , xy ) Given a dataset, calculate the random numbers used internally to generate the dataset. :param xy: a pair (x, y) of numpy arrays, where x rows are inputs and y rows are outputs :return: numpy array, rows are corresponding random numbers Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def calc_random_numbers ( self , xy ): # compile theano function, if haven't already done so if self . eval_us_f is None : self . eval_us_f = theano . function ( inputs = [ self . input , self . y ], outputs = self . u ) x , y , one_datapoint = util . misc . prepare_cond_input ( xy , dtype ) u = self . eval_us_f ( x , y ) return u [ 0 ] if one_datapoint else u ConditionalMaskedAutoregressiveFlow . eval ( self , xy , log = True ) Evaluate log probabilities for given input-output pairs. :param xy: a pair (x, y) where x rows are inputs and y rows are outputs :param log: whether to return probabilities in the log domain :return: log probabilities: log p(y|x) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def eval ( self , xy , log = True ): # compile theano function, if haven't already done so if self . eval_lprob_f is None : self . eval_lprob_f = theano . function ( inputs = [ self . input , self . y ], outputs = self . L , givens = [( bn . m , bn . bm ) for bn in self . bns ] + [( bn . v , bn . bv ) for bn in self . bns ] ) x , y , one_datapoint = util . misc . prepare_cond_input ( xy , dtype ) lprob = self . eval_lprob_f ( x , y ) lprob = lprob [ 0 ] if one_datapoint else lprob return lprob if log else np . exp ( lprob ) ConditionalMaskedAutoregressiveFlow . gen ( self , x , n_samples = None , u = None , rng =< module 'numpy.random' from '/Users/jm/opt/anaconda3/envs/ind/lib/python3.7/site-packages/numpy/random/__init__.py' > ) Generate samples, by propagating random numbers through each made, after conditioning on input x. :param x: input vector :param n_samples: number of samples, 1 if None :param u: random numbers to use in generating samples; if None, new random numbers are drawn :return: samples Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def gen ( self , x , n_samples = None , u = None , rng = np . random ): if n_samples is None : return self . gen ( x , 1 , u if u is None else u [ np . newaxis , :], rng )[ 0 ] y = rng . randn ( n_samples , self . n_outputs ) . astype ( dtype ) if u is None else u if getattr ( self , 'batch_norm' , False ): for made , bn in zip ( self . mades [:: - 1 ], self . bns [:: - 1 ]): y = bn . eval_inv ( y ) y = made . gen ( x , n_samples , y , rng ) else : for made in self . mades [:: - 1 ]: y = made . gen ( x , n_samples , y , rng ) return y ConditionalMaskedAutoregressiveFlow . grad_log_p ( self , xy ) Evaluate the gradient of the log probability wrt the output, for given input-output pairs. :param xy: a pair (x, y) where x rows are inputs and y rows are outputs :return: gradient d/dy log p(y|x) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def grad_log_p ( self , xy ): # compile theano function, if haven't already done so if getattr ( self , 'eval_grad_f' , None ) is None : self . eval_grad_f = theano . function ( inputs = [ self . input , self . y ], outputs = tt . grad ( tt . sum ( self . L ), self . y ), givens = [( bn . m , bn . bm ) for bn in self . bns ] + [( bn . v , bn . bv ) for bn in self . bns ] ) x , y , one_datapoint = util . misc . prepare_cond_input ( xy , dtype ) grad = self . eval_grad_f ( x , y ) grad = grad [ 0 ] if one_datapoint else grad return grad ConditionalMaskedAutoregressiveFlow . reset_theano_functions ( self ) Resets theano functions, so that they are compiled again when needed. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 def reset_theano_functions ( self ): self . eval_lprob_f = None self . eval_grad_f = None self . eval_score_f = None self . eval_us_f = None for made in self . mades : made . reset_theano_functions () for bn in self . bns : bn . reset_theano_functions () ConditionalMaskedAutoregressiveFlow . score ( self , xy ) Evaluate the gradient of the log probability wrt the input, for given input-output pairs. :param xy: a pair (x, y) where x rows are inputs and y rows are outputs :return: gradient d/dx log p(y|x) Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def score ( self , xy ): # compile theano function, if haven't already done so if self . eval_score_f is None : self . eval_score_f = theano . function ( inputs = [ self . input , self . y ], outputs = tt . grad ( tt . sum ( self . L ), self . input ), givens = [( bn . m , bn . bm ) for bn in self . bns ] + [( bn . v , bn . bv ) for bn in self . bns ] ) x , y , one_datapoint = util . misc . prepare_cond_input ( xy , dtype ) grads = self . eval_score_f ( x , y ) grads = grads [ 0 ] if one_datapoint else grads return grads","title":"Neural Networks"},{"location":"reference/neuralnet/#neural-networks","text":"","title":"Neural Networks"},{"location":"reference/neuralnet/#NeuralNet","text":"","title":"NeuralNet"},{"location":"reference/neuralnet/#ConditionalMaskedAutoregressiveFlow","text":"Implements a Conditional Masked Autoregressive Flow.","title":"ConditionalMaskedAutoregressiveFlow"},{"location":"reference/utils/","text":"Utilities \u00b6 samples_nd ( samples , points = [], ** kwargs ) \u00b6 Plot samples and points See `opts` below for available keyword arguments. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 def samples_nd ( samples , points = [], ** kwargs ): opts = { # what to plot on triagonal and diagonal subplots 'upper' : 'hist' , # hist/scatter/None 'diag' : 'hist' , # hist/None #'lower': None, # hist/scatter/None # TODO: implement # title and legend 'title' : None , 'legend' : False , # labels 'labels' : [], # for dimensions 'labels_points' : [], # for points 'labels_samples' : [], # for samples # colors 'samples_colors' : plt . rcParams [ 'axes.prop_cycle' ] . by_key ()[ 'color' ], 'points_colors' : plt . rcParams [ 'axes.prop_cycle' ] . by_key ()[ 'color' ], # subset 'subset' : None , # axes limits 'limits' : [], # ticks 'ticks' : [], 'tickformatter' : mpl . ticker . FormatStrFormatter ( ' %g ' ), 'tick_labels' : None , # options for hist 'hist_diag' : { 'alpha' : 1. , 'bins' : 25 , 'density' : False , 'histtype' : 'step' }, 'hist_offdiag' : { #'edgecolor': 'none', #'linewidth': 0.0, 'bins' : 25 , }, # options for kde 'kde_diag' : { 'bw_method' : 'scott' , 'bins' : 100 , 'color' : 'black' }, 'kde_offdiag' : { 'bw_method' : 'scott' , 'bins' : 25 }, # options for contour 'contour_offdiag' : { 'levels' : [ 0.68 ] }, # options for scatter 'scatter_offdiag' : { 'alpha' : 0.5 , 'edgecolor' : 'none' , 'rasterized' : False , }, # options for plot 'plot_offdiag' : {}, # formatting points (scale, markers) 'points_diag' : { }, 'points_offdiag' : { 'marker' : '.' , 'markersize' : 20 , }, # matplotlib style 'style' : os . path . join ( os . path . dirname ( __file__ ), 'matplotlibrc' ), # other options 'fig_size' : ( 10 , 10 ), 'fig_bg_colors' : { 'upper' : None , 'diag' : None , 'lower' : None }, 'fig_subplots_adjust' : { 'top' : 0.9 , }, 'subplots' : { }, 'despine' : { 'offset' : 5 , }, 'title_format' : { 'fontsize' : 16 }, } # TODO: add color map support # TODO: automatically determine good bin sizes for histograms # TODO: get rid of seaborn dependency for despine # TODO: add legend (if legend is True) samples_nd . defaults = opts . copy () opts = _update ( opts , kwargs ) # Prepare samples if type ( samples ) != list : samples = [ samples ] # Prepare points if type ( points ) != list : points = [ points ] points = [ np . atleast_2d ( p ) for p in points ] # Dimensions dim = samples [ 0 ] . shape [ 1 ] num_samples = samples [ 0 ] . shape [ 0 ] # TODO: add asserts checking compatiblity of dimensions # Prepare labels if opts [ 'labels' ] == [] or opts [ 'labels' ] is None : labels_dim = [ 'dim {} ' . format ( i + 1 ) for i in range ( dim )] else : labels_dim = opts [ 'labels' ] # Prepare limits if opts [ 'limits' ] == [] or opts [ 'limits' ] is None : limits = [] for d in range ( dim ): min = + np . inf max = - np . inf for sample in samples : min_ = sample [:, d ] . min () min = min_ if min_ < min else min max_ = sample [:, d ] . max () max = max_ if max_ > max else max limits . append ([ min , max ]) else : if len ( opts [ 'limits' ]) == 1 : limits = [ opts [ 'limits' ][ 0 ] for _ in range ( dim )] else : limits = opts [ 'limits' ] # Prepare ticks if opts [ 'ticks' ] == [] or opts [ 'ticks' ] is None : ticks = None else : if len ( opts [ 'ticks' ]) == 1 : ticks = [ opts [ 'ticks' ][ 0 ] for _ in range ( dim )] else : ticks = opts [ 'ticks' ] # Prepare diag/upper/lower if type ( opts [ 'diag' ]) is not list : opts [ 'diag' ] = [ opts [ 'diag' ] for _ in range ( len ( samples ))] if type ( opts [ 'upper' ]) is not list : opts [ 'upper' ] = [ opts [ 'upper' ] for _ in range ( len ( samples ))] #if type(opts['lower']) is not list: # opts['lower'] = [opts['lower'] for _ in range(len(samples))] opts [ 'lower' ] = None # Style if opts [ 'style' ] in [ 'dark' , 'light' ]: style = os . path . join ( os . path . dirname ( __file__ ), 'matplotlib_ {} .style' . format ( opts [ 'style' ])) else : style = opts [ 'style' ]; # Apply custom style as context with mpl . rc_context ( fname = style ): # Figure out if we subset the plot subset = opts [ 'subset' ] if subset is None : rows = cols = dim subset = [ i for i in range ( dim )] else : if type ( subset ) == int : subset = [ subset ] elif type ( subset ) == list : pass else : raise NotImplementedError rows = cols = len ( subset ) fig , axes = plt . subplots ( rows , cols , figsize = opts [ 'fig_size' ], ** opts [ 'subplots' ]) axes = axes . reshape ( rows , cols ) # Style figure fig . subplots_adjust ( ** opts [ 'fig_subplots_adjust' ]) fig . suptitle ( opts [ 'title' ], ** opts [ 'title_format' ]) # Style axes row_idx = - 1 for row in range ( dim ): if row not in subset : continue else : row_idx += 1 col_idx = - 1 for col in range ( dim ): if col not in subset : continue else : col_idx += 1 if row == col : current = 'diag' elif row < col : current = 'upper' else : current = 'lower' ax = axes [ row_idx , col_idx ] plt . sca ( ax ) # Background color if current in opts [ 'fig_bg_colors' ] and \\ opts [ 'fig_bg_colors' ][ current ] is not None : ax . set_facecolor ( opts [ 'fig_bg_colors' ][ current ]) # Axes if opts [ current ] is None : ax . axis ( 'off' ) continue # Limits if limits is not None : ax . set_xlim ( ( limits [ col ][ 0 ], limits [ col ][ 1 ])) if current is not 'diag' : ax . set_ylim ( ( limits [ row ][ 0 ], limits [ row ][ 1 ])) xmin , xmax = ax . get_xlim () ymin , ymax = ax . get_ylim () # Ticks if ticks is not None : ax . set_xticks ( ( ticks [ col ][ 0 ], ticks [ col ][ 1 ])) if current is not 'diag' : ax . set_yticks ( ( ticks [ row ][ 0 ], ticks [ row ][ 1 ])) # Despine despine ( ax = ax , ** opts [ 'despine' ]) # Formatting axes if current == 'diag' : # off-diagnoals if opts [ 'lower' ] is None or col == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ 'tickformatter' ]) else : _format_axis ( ax , xhide = True , yhide = True ) else : # off-diagnoals if row == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ 'tickformatter' ]) else : _format_axis ( ax , xhide = True , yhide = True ) if opts [ 'tick_labels' ] is not None : ax . set_xticklabels ( ( str ( opts [ 'tick_labels' ][ col ][ 0 ]), str ( opts [ 'tick_labels' ][ col ][ 1 ]))) # Diagonals if current == 'diag' : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ 'diag' ][ n ] == 'hist' : h = plt . hist ( v [:, row ], color = opts [ 'samples_colors' ][ n ], ** opts [ 'hist_diag' ] ) elif opts [ 'diag' ][ n ] == 'kde' : density = gaussian_kde ( v [:, row ], bw_method = opts [ 'kde_diag' ][ 'bw_method' ]) xs = np . linspace ( xmin , xmax , opts [ 'kde_diag' ][ 'bins' ]) ys = density ( xs ) h = plt . plot ( xs , ys , color = opts [ 'samples_colors' ][ n ], ) else : pass if len ( points ) > 0 : extent = ax . get_ylim () for n , v in enumerate ( points ): h = plt . plot ( [ v [:, row ], v [:, row ]], extent , color = opts [ 'points_colors' ][ n ], ** opts [ 'points_diag' ] ) # Off-diagonals else : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ 'upper' ][ n ] == 'hist' or opts [ 'upper' ][ n ] == 'hist2d' : hist , xedges , yedges = np . histogram2d ( v [:, col ], v [:, row ], range = [ [ limits [ col ][ 0 ], limits [ col ][ 1 ]], [ limits [ row ][ 0 ], limits [ row ][ 1 ]]], ** opts [ 'hist_offdiag' ]) h = plt . imshow ( hist . T , origin = 'lower' , extent = [ xedges [ 0 ], xedges [ - 1 ], yedges [ 0 ], yedges [ - 1 ]], aspect = 'auto' ) elif opts [ 'upper' ][ n ] in [ 'kde' , 'kde2d' , 'contour' , 'contourf' ]: density = gaussian_kde ( v [:, [ col , row ]] . T , bw_method = opts [ 'kde_offdiag' ][ 'bw_method' ]) X , Y = np . meshgrid ( np . linspace ( limits [ col ][ 0 ], limits [ col ][ 1 ], opts [ 'kde_offdiag' ][ 'bins' ]), np . linspace ( limits [ row ][ 0 ], limits [ row ][ 1 ], opts [ 'kde_offdiag' ][ 'bins' ])) positions = np . vstack ([ X . ravel (), Y . ravel ()]) Z = np . reshape ( density ( positions ) . T , X . shape ) if opts [ 'upper' ][ n ] == 'kde' or opts [ 'upper' ][ n ] == 'kde2d' : h = plt . imshow ( Z , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ]], origin = 'lower' , aspect = 'auto' , ) elif opts [ 'upper' ][ n ] == 'contour' : Z = ( Z - Z . min ()) / ( Z . max () - Z . min ()) h = plt . contour ( X , Y , Z , origin = 'lower' , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ]], colors = opts [ 'samples_colors' ][ n ], ** opts [ 'contour_offdiag' ] ) else : pass elif opts [ 'upper' ][ n ] == 'scatter' : h = plt . scatter ( v [:, col ], v [:, row ], color = opts [ 'samples_colors' ][ n ], ** opts [ 'scatter_offdiag' ] ) elif opts [ 'upper' ][ n ] == 'plot' : h = plt . plot ( v [:, col ], v [:, row ], color = opts [ 'samples_colors' ][ n ], ** opts [ 'plot_offdiag' ] ) else : pass if len ( points ) > 0 : for n , v in enumerate ( points ): h = plt . plot ( v [:, col ], v [:, row ], color = opts [ 'points_colors' ][ n ], ** opts [ 'points_offdiag' ] ) if len ( subset ) < dim : for row in range ( len ( subset )): ax = axes [ row , len ( subset ) - 1 ] x0 , x1 = ax . get_xlim () y0 , y1 = ax . get_ylim () text_kwargs = { 'fontsize' : plt . rcParams [ 'font.size' ] * 2. } ax . text ( x1 + ( x1 - x0 ) / 8. , ( y0 + y1 ) / 2. , '...' , ** text_kwargs ) if row == len ( subset ) - 1 : ax . text ( x1 + ( x1 - x0 ) / 12. , y0 - ( y1 - y0 ) / 1.5 , '...' , rotation =- 45 , ** text_kwargs ) return fig , axes","title":"Utilities"},{"location":"reference/utils/#utilities","text":"","title":"Utilities"},{"location":"reference/utils/#samples_nd","text":"Plot samples and points See `opts` below for available keyword arguments. Source Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 def samples_nd ( samples , points = [], ** kwargs ): opts = { # what to plot on triagonal and diagonal subplots 'upper' : 'hist' , # hist/scatter/None 'diag' : 'hist' , # hist/None #'lower': None, # hist/scatter/None # TODO: implement # title and legend 'title' : None , 'legend' : False , # labels 'labels' : [], # for dimensions 'labels_points' : [], # for points 'labels_samples' : [], # for samples # colors 'samples_colors' : plt . rcParams [ 'axes.prop_cycle' ] . by_key ()[ 'color' ], 'points_colors' : plt . rcParams [ 'axes.prop_cycle' ] . by_key ()[ 'color' ], # subset 'subset' : None , # axes limits 'limits' : [], # ticks 'ticks' : [], 'tickformatter' : mpl . ticker . FormatStrFormatter ( ' %g ' ), 'tick_labels' : None , # options for hist 'hist_diag' : { 'alpha' : 1. , 'bins' : 25 , 'density' : False , 'histtype' : 'step' }, 'hist_offdiag' : { #'edgecolor': 'none', #'linewidth': 0.0, 'bins' : 25 , }, # options for kde 'kde_diag' : { 'bw_method' : 'scott' , 'bins' : 100 , 'color' : 'black' }, 'kde_offdiag' : { 'bw_method' : 'scott' , 'bins' : 25 }, # options for contour 'contour_offdiag' : { 'levels' : [ 0.68 ] }, # options for scatter 'scatter_offdiag' : { 'alpha' : 0.5 , 'edgecolor' : 'none' , 'rasterized' : False , }, # options for plot 'plot_offdiag' : {}, # formatting points (scale, markers) 'points_diag' : { }, 'points_offdiag' : { 'marker' : '.' , 'markersize' : 20 , }, # matplotlib style 'style' : os . path . join ( os . path . dirname ( __file__ ), 'matplotlibrc' ), # other options 'fig_size' : ( 10 , 10 ), 'fig_bg_colors' : { 'upper' : None , 'diag' : None , 'lower' : None }, 'fig_subplots_adjust' : { 'top' : 0.9 , }, 'subplots' : { }, 'despine' : { 'offset' : 5 , }, 'title_format' : { 'fontsize' : 16 }, } # TODO: add color map support # TODO: automatically determine good bin sizes for histograms # TODO: get rid of seaborn dependency for despine # TODO: add legend (if legend is True) samples_nd . defaults = opts . copy () opts = _update ( opts , kwargs ) # Prepare samples if type ( samples ) != list : samples = [ samples ] # Prepare points if type ( points ) != list : points = [ points ] points = [ np . atleast_2d ( p ) for p in points ] # Dimensions dim = samples [ 0 ] . shape [ 1 ] num_samples = samples [ 0 ] . shape [ 0 ] # TODO: add asserts checking compatiblity of dimensions # Prepare labels if opts [ 'labels' ] == [] or opts [ 'labels' ] is None : labels_dim = [ 'dim {} ' . format ( i + 1 ) for i in range ( dim )] else : labels_dim = opts [ 'labels' ] # Prepare limits if opts [ 'limits' ] == [] or opts [ 'limits' ] is None : limits = [] for d in range ( dim ): min = + np . inf max = - np . inf for sample in samples : min_ = sample [:, d ] . min () min = min_ if min_ < min else min max_ = sample [:, d ] . max () max = max_ if max_ > max else max limits . append ([ min , max ]) else : if len ( opts [ 'limits' ]) == 1 : limits = [ opts [ 'limits' ][ 0 ] for _ in range ( dim )] else : limits = opts [ 'limits' ] # Prepare ticks if opts [ 'ticks' ] == [] or opts [ 'ticks' ] is None : ticks = None else : if len ( opts [ 'ticks' ]) == 1 : ticks = [ opts [ 'ticks' ][ 0 ] for _ in range ( dim )] else : ticks = opts [ 'ticks' ] # Prepare diag/upper/lower if type ( opts [ 'diag' ]) is not list : opts [ 'diag' ] = [ opts [ 'diag' ] for _ in range ( len ( samples ))] if type ( opts [ 'upper' ]) is not list : opts [ 'upper' ] = [ opts [ 'upper' ] for _ in range ( len ( samples ))] #if type(opts['lower']) is not list: # opts['lower'] = [opts['lower'] for _ in range(len(samples))] opts [ 'lower' ] = None # Style if opts [ 'style' ] in [ 'dark' , 'light' ]: style = os . path . join ( os . path . dirname ( __file__ ), 'matplotlib_ {} .style' . format ( opts [ 'style' ])) else : style = opts [ 'style' ]; # Apply custom style as context with mpl . rc_context ( fname = style ): # Figure out if we subset the plot subset = opts [ 'subset' ] if subset is None : rows = cols = dim subset = [ i for i in range ( dim )] else : if type ( subset ) == int : subset = [ subset ] elif type ( subset ) == list : pass else : raise NotImplementedError rows = cols = len ( subset ) fig , axes = plt . subplots ( rows , cols , figsize = opts [ 'fig_size' ], ** opts [ 'subplots' ]) axes = axes . reshape ( rows , cols ) # Style figure fig . subplots_adjust ( ** opts [ 'fig_subplots_adjust' ]) fig . suptitle ( opts [ 'title' ], ** opts [ 'title_format' ]) # Style axes row_idx = - 1 for row in range ( dim ): if row not in subset : continue else : row_idx += 1 col_idx = - 1 for col in range ( dim ): if col not in subset : continue else : col_idx += 1 if row == col : current = 'diag' elif row < col : current = 'upper' else : current = 'lower' ax = axes [ row_idx , col_idx ] plt . sca ( ax ) # Background color if current in opts [ 'fig_bg_colors' ] and \\ opts [ 'fig_bg_colors' ][ current ] is not None : ax . set_facecolor ( opts [ 'fig_bg_colors' ][ current ]) # Axes if opts [ current ] is None : ax . axis ( 'off' ) continue # Limits if limits is not None : ax . set_xlim ( ( limits [ col ][ 0 ], limits [ col ][ 1 ])) if current is not 'diag' : ax . set_ylim ( ( limits [ row ][ 0 ], limits [ row ][ 1 ])) xmin , xmax = ax . get_xlim () ymin , ymax = ax . get_ylim () # Ticks if ticks is not None : ax . set_xticks ( ( ticks [ col ][ 0 ], ticks [ col ][ 1 ])) if current is not 'diag' : ax . set_yticks ( ( ticks [ row ][ 0 ], ticks [ row ][ 1 ])) # Despine despine ( ax = ax , ** opts [ 'despine' ]) # Formatting axes if current == 'diag' : # off-diagnoals if opts [ 'lower' ] is None or col == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ 'tickformatter' ]) else : _format_axis ( ax , xhide = True , yhide = True ) else : # off-diagnoals if row == dim - 1 : _format_axis ( ax , xhide = False , xlabel = labels_dim [ col ], yhide = True , tickformatter = opts [ 'tickformatter' ]) else : _format_axis ( ax , xhide = True , yhide = True ) if opts [ 'tick_labels' ] is not None : ax . set_xticklabels ( ( str ( opts [ 'tick_labels' ][ col ][ 0 ]), str ( opts [ 'tick_labels' ][ col ][ 1 ]))) # Diagonals if current == 'diag' : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ 'diag' ][ n ] == 'hist' : h = plt . hist ( v [:, row ], color = opts [ 'samples_colors' ][ n ], ** opts [ 'hist_diag' ] ) elif opts [ 'diag' ][ n ] == 'kde' : density = gaussian_kde ( v [:, row ], bw_method = opts [ 'kde_diag' ][ 'bw_method' ]) xs = np . linspace ( xmin , xmax , opts [ 'kde_diag' ][ 'bins' ]) ys = density ( xs ) h = plt . plot ( xs , ys , color = opts [ 'samples_colors' ][ n ], ) else : pass if len ( points ) > 0 : extent = ax . get_ylim () for n , v in enumerate ( points ): h = plt . plot ( [ v [:, row ], v [:, row ]], extent , color = opts [ 'points_colors' ][ n ], ** opts [ 'points_diag' ] ) # Off-diagonals else : if len ( samples ) > 0 : for n , v in enumerate ( samples ): if opts [ 'upper' ][ n ] == 'hist' or opts [ 'upper' ][ n ] == 'hist2d' : hist , xedges , yedges = np . histogram2d ( v [:, col ], v [:, row ], range = [ [ limits [ col ][ 0 ], limits [ col ][ 1 ]], [ limits [ row ][ 0 ], limits [ row ][ 1 ]]], ** opts [ 'hist_offdiag' ]) h = plt . imshow ( hist . T , origin = 'lower' , extent = [ xedges [ 0 ], xedges [ - 1 ], yedges [ 0 ], yedges [ - 1 ]], aspect = 'auto' ) elif opts [ 'upper' ][ n ] in [ 'kde' , 'kde2d' , 'contour' , 'contourf' ]: density = gaussian_kde ( v [:, [ col , row ]] . T , bw_method = opts [ 'kde_offdiag' ][ 'bw_method' ]) X , Y = np . meshgrid ( np . linspace ( limits [ col ][ 0 ], limits [ col ][ 1 ], opts [ 'kde_offdiag' ][ 'bins' ]), np . linspace ( limits [ row ][ 0 ], limits [ row ][ 1 ], opts [ 'kde_offdiag' ][ 'bins' ])) positions = np . vstack ([ X . ravel (), Y . ravel ()]) Z = np . reshape ( density ( positions ) . T , X . shape ) if opts [ 'upper' ][ n ] == 'kde' or opts [ 'upper' ][ n ] == 'kde2d' : h = plt . imshow ( Z , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ]], origin = 'lower' , aspect = 'auto' , ) elif opts [ 'upper' ][ n ] == 'contour' : Z = ( Z - Z . min ()) / ( Z . max () - Z . min ()) h = plt . contour ( X , Y , Z , origin = 'lower' , extent = [ limits [ col ][ 0 ], limits [ col ][ 1 ], limits [ row ][ 0 ], limits [ row ][ 1 ]], colors = opts [ 'samples_colors' ][ n ], ** opts [ 'contour_offdiag' ] ) else : pass elif opts [ 'upper' ][ n ] == 'scatter' : h = plt . scatter ( v [:, col ], v [:, row ], color = opts [ 'samples_colors' ][ n ], ** opts [ 'scatter_offdiag' ] ) elif opts [ 'upper' ][ n ] == 'plot' : h = plt . plot ( v [:, col ], v [:, row ], color = opts [ 'samples_colors' ][ n ], ** opts [ 'plot_offdiag' ] ) else : pass if len ( points ) > 0 : for n , v in enumerate ( points ): h = plt . plot ( v [:, col ], v [:, row ], color = opts [ 'points_colors' ][ n ], ** opts [ 'points_offdiag' ] ) if len ( subset ) < dim : for row in range ( len ( subset )): ax = axes [ row , len ( subset ) - 1 ] x0 , x1 = ax . get_xlim () y0 , y1 = ax . get_ylim () text_kwargs = { 'fontsize' : plt . rcParams [ 'font.size' ] * 2. } ax . text ( x1 + ( x1 - x0 ) / 8. , ( y0 + y1 ) / 2. , '...' , ** text_kwargs ) if row == len ( subset ) - 1 : ax . text ( x1 + ( x1 - x0 ) / 12. , y0 - ( y1 - y0 ) / 1.5 , '...' , rotation =- 45 , ** text_kwargs ) return fig , axes","title":"samples_nd"},{"location":"tutorials/quickstart/","text":"Inference on Hodgkin-Huxley model: tutorial \u00b6 In this tutorial, we use delfi to do inference on a Hodgkin-Huxley model (Hodgkin and Huxley, 1952) with two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given a current-clamp recording (synthetically generated). Different required components \u00b6 Before running inference, let us define the different required components: (1) observed data (2) model (3) prior over model parameters (4) summary statistics (5) generator class. Observed data \u00b6 Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial. Model \u00b6 We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange): where \\(V\\) is the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined by: import numpy as np def syn_current ( duration = 120 , dt = 0.01 , t_on = 10 , curr_level = 5e-4 , seed = None ): t_offset = 0. duration = duration t_off = duration - t_on t = np . arange ( 0 , duration + dt , dt ) # external current A_soma = np . pi * (( 70. * 1e-4 ) ** 2 ) # cm2 I = np . zeros_like ( t ) I [ int ( np . round ( t_on / dt )): int ( np . round ( t_off / dt ))] = curr_level / A_soma # muA/cm2 return I , t_on , t_off , dt , t , A_soma The Hodgkin-Huxley simulator is given by: def HHsimulator ( V0 , params , dt , t , I , seed = None ): \"\"\"Simulates the Hodgkin-Huxley model for a specified time duration and current Parameters ---------- V0 : float Voltage at first time step params : np.array, 1d of length dim_param Parameter vector dt : float Timestep t : array Numpy array with the time steps I : array Numpy array with the input current seed : int \"\"\" gbar_Na = params [ 0 , 0 ] # mS/cm2 gbar_Na . astype ( float ) gbar_K = params [ 0 , 1 ] # mS/cm2 gbar_K . astype ( float ) # fixed parameters g_leak = 0.1 # mS/cm2 gbar_M = 0.07 # mS/cm2 tau_max = 6e2 # ms Vt = - 60. # mV nois_fact = 0.1 # uA/cm2 E_leak = - 70. # mV C = 1. # uF/cm2 E_Na = 53 # mV E_K = - 107 # mV tstep = float ( dt ) if seed is not None : rng = np . random . RandomState ( seed = seed ) else : rng = np . random . RandomState () #################################### # kinetics def efun ( z ): if np . abs ( z ) < 1e-4 : return 1 - z / 2 else : return z / ( np . exp ( z ) - 1 ) def alpha_m ( x ): v1 = x - Vt - 13. return 0.32 * efun ( - 0.25 * v1 ) / 0.25 def beta_m ( x ): v1 = x - Vt - 40 return 0.28 * efun ( 0.2 * v1 ) / 0.2 def alpha_h ( x ): v1 = x - Vt - 17. return 0.128 * np . exp ( - v1 / 18. ) def beta_h ( x ): v1 = x - Vt - 40. return 4.0 / ( 1 + np . exp ( - 0.2 * v1 )) def alpha_n ( x ): v1 = x - Vt - 15. return 0.032 * efun ( - 0.2 * v1 ) / 0.2 def beta_n ( x ): v1 = x - Vt - 10. return 0.5 * np . exp ( - v1 / 40 ) # steady-states and time constants def tau_n ( x ): return 1 / ( alpha_n ( x ) + beta_n ( x )) def n_inf ( x ): return alpha_n ( x ) / ( alpha_n ( x ) + beta_n ( x )) def tau_m ( x ): return 1 / ( alpha_m ( x ) + beta_m ( x )) def m_inf ( x ): return alpha_m ( x ) / ( alpha_m ( x ) + beta_m ( x )) def tau_h ( x ): return 1 / ( alpha_h ( x ) + beta_h ( x )) def h_inf ( x ): return alpha_h ( x ) / ( alpha_h ( x ) + beta_h ( x )) # slow non-inactivating K+ def p_inf ( x ): v1 = x + 35. return 1.0 / ( 1. + np . exp ( - 0.1 * v1 )) def tau_p ( x ): v1 = x + 35. return tau_max / ( 3.3 * np . exp ( 0.05 * v1 ) + np . exp ( - 0.05 * v1 )) #################################### # simulation from initial point V = np . zeros_like ( t ) # voltage n = np . zeros_like ( t ) m = np . zeros_like ( t ) h = np . zeros_like ( t ) p = np . zeros_like ( t ) V [ 0 ] = float ( V0 ) n [ 0 ] = n_inf ( V [ 0 ]) m [ 0 ] = m_inf ( V [ 0 ]) h [ 0 ] = h_inf ( V [ 0 ]) p [ 0 ] = p_inf ( V [ 0 ]) for i in range ( 1 , t . shape [ 0 ]): tau_V_inv = ( ( m [ i - 1 ] ** 3 ) * gbar_Na * h [ i - 1 ] + ( n [ i - 1 ] ** 4 ) * gbar_K + g_leak + gbar_M * p [ i - 1 ] ) / C V_inf = ( ( m [ i - 1 ] ** 3 ) * gbar_Na * h [ i - 1 ] * E_Na + ( n [ i - 1 ] ** 4 ) * gbar_K * E_K + g_leak * E_leak + gbar_M * p [ i - 1 ] * E_K + I [ i - 1 ] + nois_fact * rng . randn () / ( tstep ** 0.5 ) ) / ( tau_V_inv * C ) V [ i ] = V_inf + ( V [ i - 1 ] - V_inf ) * np . exp ( - tstep * tau_V_inv ) n [ i ] = n_inf ( V [ i ]) + ( n [ i - 1 ] - n_inf ( V [ i ])) * np . exp ( - tstep / tau_n ( V [ i ])) m [ i ] = m_inf ( V [ i ]) + ( m [ i - 1 ] - m_inf ( V [ i ])) * np . exp ( - tstep / tau_m ( V [ i ])) h [ i ] = h_inf ( V [ i ]) + ( h [ i - 1 ] - h_inf ( V [ i ])) * np . exp ( - tstep / tau_h ( V [ i ])) p [ i ] = p_inf ( V [ i ]) + ( p [ i - 1 ] - p_inf ( V [ i ])) * np . exp ( - tstep / tau_p ( V [ i ])) return np . array ( V ) . reshape ( - 1 , 1 ) To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : import matplotlib as mpl import matplotlib.pyplot as plt # input current, time step, time array I , t_on , t_off , dt , t , A_soma = syn_current () # simulate Hodgkin-Huxley model for 3 different parameter sets params = np . array ([[[ 50. , 1. ]],[[ 4. , 1.5 ]],[[ 20. , 15. ]]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = HHsimulator ( V0 =- 70 , params = params [ i ,:], dt = dt , t = t , I = I , seed = None )[:, 0 ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] # plotting % matplotlib inline fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) As can be seen, the voltage traces can be quite diverse for different parameter values. How can we infer the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) underlying the particular observed voltage trace? delfi will allow us to solve this problem. In order to run the Hodgkin-Huxley simulator within delfi , we need to define a simulator class linking to the simulator: from delfi.simulator.BaseSimulator import BaseSimulator class HodgkinHuxley ( BaseSimulator ): def __init__ ( self , I , dt , V0 , seed = None ): \"\"\"Hodgkin-Huxley simulator Parameters ---------- I : array Numpy array with the input current dt : float Timestep V0 : float Voltage at first time step seed : int or None If set, randomness across runs is disabled \"\"\" dim_param = 2 super () . __init__ ( dim_param = dim_param , seed = seed ) self . I = I self . dt = dt self . t = np . arange ( 0 , len ( self . I ), 1 ) * self . dt self . HHsimulator = HHsimulator self . init = V0 def gen_single ( self , params ): \"\"\"Forward model for simulator for single parameter set Parameters ---------- params : list or np.array, 1d of length dim_param Parameter vector Returns ------- dict : dictionary with data The dictionary must contain a key data that contains the results of the forward run. Additional entries can be present. \"\"\" params = np . asarray ( params ) assert params . ndim == 1 , 'params.ndim must be 1' hh_seed = self . gen_newseed () states = self . HHsimulator ( self . init , params . reshape ( 1 , - 1 ), self . dt , self . t , self . I , seed = hh_seed ) return { 'data' : states . reshape ( - 1 ), 'time' : self . t , 'dt' : self . dt , 'I' : self . I . reshape ( - 1 )} Note that the simulator class simply wraps the simulator function we set up previously. delfi has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not need to be Python-based as long as they store simulation outputs in a format that can be read from Python. Prior over model parameters \u00b6 Now that we have the model and simulator class, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: import delfi.distribution as dd seed_p = 2 prior_min = np . array ([ . 5 , 1e-4 ]) prior_max = np . array ([ 80. , 15. ]) prior = dd . Uniform ( lower = prior_min , upper = prior_max , seed = seed_p ) Summary statistics \u00b6 We want to fit a set of summary statistics of the observed data: number of spikes, mean resting potential, standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In order to do accomplish that, we define a summary statistics class which computes those quantities: from delfi.summarystats.BaseSummaryStats import BaseSummaryStats from scipy import stats as spstats class HodgkinHuxleyStats ( BaseSummaryStats ): \"\"\"Moment based SummaryStats class for the Hodgkin-Huxley model Calculates summary statistics \"\"\" def __init__ ( self , t_on , t_off , n_mom = 4 , n_summary = 7 , seed = None ): \"\"\"See SummaryStats.py for docstring\"\"\" super ( HodgkinHuxleyStats , self ) . __init__ ( seed = seed ) self . t_on = t_on self . t_off = t_off self . n_mom = n_mom self . n_summary = np . minimum ( n_summary , n_mom + 3 ) def calc ( self , repetition_list ): \"\"\"Calculate summary statistics Parameters ---------- repetition_list : list of dictionaries, one per repetition data list, returned by `gen` method of Simulator instance Returns ------- np.array, 2d with n_reps x n_summary \"\"\" stats = [] for r in range ( len ( repetition_list )): x = repetition_list [ r ] N = x [ 'data' ] . shape [ 0 ] t = x [ 'time' ] dt = x [ 'dt' ] t_on = self . t_on t_off = self . t_off # initialise array of spike counts v = np . array ( x [ 'data' ]) # put everything to -10 that is below -10 or has negative slope ind = np . where ( v < - 10 ) v [ ind ] = - 10 ind = np . where ( np . diff ( v ) < 0 ) v [ ind ] = - 10 # remaining negative slopes are at spike peaks ind = np . where ( np . diff ( v ) < 0 ) spike_times = np . array ( t )[ ind ] spike_times_stim = spike_times [( spike_times > t_on ) & ( spike_times < t_off )] # number of spikes if spike_times_stim . shape [ 0 ] > 0 : spike_times_stim = spike_times_stim [ np . append ( 1 , np . diff ( spike_times_stim )) > 0.5 ] # resting potential and std rest_pot = np . mean ( x [ 'data' ][ t < t_on ]) rest_pot_std = np . std ( x [ 'data' ][ int ( . 9 * t_on / dt ): int ( t_on / dt )]) # moments std_pw = np . power ( np . std ( x [ 'data' ][( t > t_on ) & ( t < t_off )]), np . linspace ( 3 , self . n_mom , self . n_mom - 2 )) std_pw = np . concatenate (( np . ones ( 1 ), std_pw )) moments = spstats . moment ( x [ 'data' ][( t > t_on ) & ( t < t_off )], np . linspace ( 2 , self . n_mom , self . n_mom - 1 )) / std_pw # concatenation of summary statistics sum_stats_vec = np . concatenate (( np . array ([ spike_times_stim . shape [ 0 ]]), np . array ([ rest_pot , rest_pot_std , np . mean ( x [ 'data' ][( t > t_on ) & ( t < t_off )])]), moments )) sum_stats_vec = sum_stats_vec [ 0 : self . n_summary ] stats . append ( sum_stats_vec ) return np . asarray ( stats ) Generator class \u00b6 Now that we have the simulator class, the prior over parameters and the summary statistics class, we need to define a generator class that binds all these objects together: import delfi.generator as dg # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () # initial voltage V0 = - 70 # seeds seed_m = 1 # summary statistics hyperparameters n_mom = 4 n_summary = 7 # define model, prior, summary statistics and generator classes seed = 3 m = HodgkinHuxley ( I , dt , V0 = V0 , seed = seed_m ) s = HodgkinHuxleyStats ( t_on = t_on , t_off = t_off , n_mom = n_mom , n_summary = n_summary ) g = dg . Default ( model = m , prior = prior , summary = s ) Note that delfi allows for the use of multiprocessing tools to parallelise the simulations and increase the speed of simulations. As the simulations typically constitute the major computational burden in SNPE , we suggest the use of parallelisation. In order to accomplish that, we need to decide on the number of parallel processes (according to the computational resources available to the user - here we chose 10 parallel processes), define a list of as many models as the number of processes, and call MPGenerator class instead of Default : n_processes = 10 seeds_m = np . arange ( 1 , n_processes + 1 , 1 ) m = [] for i in range ( n_processes ): m . append ( HodgkinHuxley ( I , dt , V0 = V0 , seed = seeds_m [ i ])) g = dg . MPGenerator ( models = m , prior = prior , summary = s ) Coming back to the observed data \u00b6 As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_{Na}$' , r '$g_{K}$' ] # observed data: simulation given true parameters obs = m [ 0 ] . gen_single ( true_params ) Replotting the observed data with the respective current input: import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( obs [ 'time' ], obs [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( obs [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( obs [ 'time' ]) / 2 , max ( obs [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) Let us compute the summary statistics of the observed data: obs_stats = s . calc ([ obs ]) Inference \u00b6 Now that we have all the required components, we can run inference with SNPE . Let us start by defining the inference hyperparameters: seed_inf = 1 pilot_samples = 2000 # training schedule n_train = 2000 n_rounds = 1 # fitting setup minibatch = 100 epochs = 100 val_frac = 0.05 # network setup n_hiddens = [ 50 , 50 ] # convenience prior_norm = True # MAF parameters density = 'maf' n_mades = 5 # number of MADES Note that we chose to run SNPE where the density estimator is a masked autoregressive flow (MAF). Another option could have been a mixture of Gaussians (see http://www.mackelab.org/delfi/reference/neuralnet/ ). We can now run the inference algorithm. Note that this step can take less than one minute to up to one hour depending on the computational resources available and the number of processes defined above: import delfi.inference as infer # inference object res = infer . SNPEC ( g , obs = obs_stats , n_hiddens = n_hiddens , seed = seed_inf , pilot_samples = pilot_samples , n_mades = n_mades , prior_norm = prior_norm , density = density ) # train log , _ , posterior = res . run ( n_train = n_train , n_rounds = n_rounds , minibatch = minibatch , epochs = epochs , silent_fail = False , proposal = 'prior' , val_frac = val_frac , verbose = True ,) Note that we chose the algorithm SNPE-C , but other algorithms are available within delfi (see http://www.mackelab.org/delfi/reference/inference/ ). Analysis of the results \u00b6 After running the inference algorithm, let us inspect and analyse the results. We can start by checking whether the algorithm training has converged by inspecting the respective loss function: fig = plt . figure ( figsize = ( 15 , 5 )) plt . plot ( log [ 0 ][ 'loss' ], lw = 2 ) plt . xlabel ( 'iteration' ) plt . ylabel ( 'loss' ); It seems that the loss function has converged. Let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ): from delfi.utils.viz import samples_nd prior_min = g . prior . lower prior_max = g . prior . upper prior_lims = np . concatenate (( prior_min . reshape ( - 1 , 1 ), prior_max . reshape ( - 1 , 1 )), axis = 1 ) posterior_samples = posterior [ 0 ] . gen ( 10000 ) ################### # colors hex2rgb = lambda h : tuple ( int ( h [ i : i + 2 ], 16 ) for i in ( 0 , 2 , 4 )) # RGB colors in [0, 255] col = {} col [ 'GT' ] = hex2rgb ( '30C05D' ) col [ 'SNPE' ] = hex2rgb ( '2E7FE8' ) col [ 'SAMPLE1' ] = hex2rgb ( '8D62BC' ) col [ 'SAMPLE2' ] = hex2rgb ( 'AF99EF' ) # convert to RGB colors in [0, 1] for k , v in col . items (): col [ k ] = tuple ([ i / 255 for i in v ]) ################### # posterior fig , axes = samples_nd ( posterior_samples , limits = prior_lims , ticks = prior_lims , labels = labels_params , fig_size = ( 5 , 5 ), diag = 'kde' , upper = 'kde' , hist_diag = { 'bins' : 50 }, hist_offdiag = { 'bins' : 50 }, kde_diag = { 'bins' : 50 , 'color' : col [ 'SNPE' ]}, kde_offdiag = { 'bins' : 50 }, points = [ true_params ], points_offdiag = { 'markersize' : 5 }, points_colors = [ col [ 'GT' ]], title = '' ); As can be seen, the inferred posterior contains the ground-truth parameters (green) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for each of those samples and compare the simulations with the observed data: fig = plt . figure ( figsize = ( 7 , 5 )) y_obs = obs [ 'data' ] t = obs [ 'time' ] duration = np . max ( t ) num_samp = 2 # sample from posterior x_samp = posterior [ 0 ] . gen ( n_samples = num_samp ) # reject samples for which prior is zero ind = ( x_samp > prior_min ) & ( x_samp < prior_max ) params = x_samp [ np . prod ( ind , axis = 1 ) == 1 ] num_samp = len ( params [:, 0 ]) # simulate and plot samples V = np . zeros (( len ( t ), num_samp )) for i in range ( num_samp ): x = m [ 0 ] . gen_single ( params [ i ,:]) V [:, i ] = x [ 'data' ] plt . plot ( t , V [:, i ], color = col [ 'SAMPLE' + str ( i + 1 )], lw = 2 , label = 'sample ' + str ( num_samp - i )) # plot observation plt . plot ( t , y_obs , '--' , lw = 2 , label = 'observation' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , duration / 2 , duration ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the samples from the inferred posterior lead to simulations that closely resemble the observed data, confirming that SNPE-C did a good job at capturing the observed data. References \u00b6 A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"Tutorial"},{"location":"tutorials/quickstart/#inference-on-hodgkin-huxley-model-tutorial","text":"In this tutorial, we use delfi to do inference on a Hodgkin-Huxley model (Hodgkin and Huxley, 1952) with two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given a current-clamp recording (synthetically generated).","title":"Inference on Hodgkin-Huxley model: tutorial"},{"location":"tutorials/quickstart/#different-required-components","text":"Before running inference, let us define the different required components: (1) observed data (2) model (3) prior over model parameters (4) summary statistics (5) generator class.","title":"Different required components"},{"location":"tutorials/quickstart/#observed-data","text":"Let us assume we current-clamped a neuron and recorded the following voltage trace: In fact, this voltage trace was not measured experimentally but synthetically generated by simulating a Hodgkin-Huxley model with particular parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). We will come back to this point later in the tutorial.","title":"Observed data"},{"location":"tutorials/quickstart/#model","text":"We would like to infer the posterior over the two parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in Pospischil et al. 2008, and is defined by the following set of differential equations (parameters of interest highlighted in orange): where \\(V\\) is the membrane potential, \\(C_m\\) is the membrane capacitance, \\(g_{\\text{l}}\\) is the leak conductance, \\(E_{\\text{l}}\\) is the membrane reversal potential, \\(\\bar{g}_c\\) is the density of channels of type \\(c\\) ( \\(\\text{Na}^+\\) , \\(\\text{K}^+\\) , M), \\(E_c\\) is the reversal potential of \\(c\\) , ( \\(m\\) , \\(h\\) , \\(n\\) , \\(p\\) ) are the respective channel gating kinetic variables, and \\(\\sigma \\eta(t)\\) is the intrinsic neural noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent \\(\\text{Na}^+\\) current, a delayed-rectifier \\(\\text{K}^+\\) current, a slow voltage-dependent \\(\\text{K}^+\\) current responsible for spike-frequency adaptation, and an injected current \\(I_{\\text{inj}}\\) . Channel gating variables \\(q\\) have dynamics fully characterized by the neuron membrane potential \\(V\\) , given the respective steady-state \\(q_{\\infty}(V)\\) and time constant \\(\\tau_{q}(V)\\) (details in Pospischil et al. 2008). The input current \\(I_{\\text{inj}}\\) is defined by: import numpy as np def syn_current ( duration = 120 , dt = 0.01 , t_on = 10 , curr_level = 5e-4 , seed = None ): t_offset = 0. duration = duration t_off = duration - t_on t = np . arange ( 0 , duration + dt , dt ) # external current A_soma = np . pi * (( 70. * 1e-4 ) ** 2 ) # cm2 I = np . zeros_like ( t ) I [ int ( np . round ( t_on / dt )): int ( np . round ( t_off / dt ))] = curr_level / A_soma # muA/cm2 return I , t_on , t_off , dt , t , A_soma The Hodgkin-Huxley simulator is given by: def HHsimulator ( V0 , params , dt , t , I , seed = None ): \"\"\"Simulates the Hodgkin-Huxley model for a specified time duration and current Parameters ---------- V0 : float Voltage at first time step params : np.array, 1d of length dim_param Parameter vector dt : float Timestep t : array Numpy array with the time steps I : array Numpy array with the input current seed : int \"\"\" gbar_Na = params [ 0 , 0 ] # mS/cm2 gbar_Na . astype ( float ) gbar_K = params [ 0 , 1 ] # mS/cm2 gbar_K . astype ( float ) # fixed parameters g_leak = 0.1 # mS/cm2 gbar_M = 0.07 # mS/cm2 tau_max = 6e2 # ms Vt = - 60. # mV nois_fact = 0.1 # uA/cm2 E_leak = - 70. # mV C = 1. # uF/cm2 E_Na = 53 # mV E_K = - 107 # mV tstep = float ( dt ) if seed is not None : rng = np . random . RandomState ( seed = seed ) else : rng = np . random . RandomState () #################################### # kinetics def efun ( z ): if np . abs ( z ) < 1e-4 : return 1 - z / 2 else : return z / ( np . exp ( z ) - 1 ) def alpha_m ( x ): v1 = x - Vt - 13. return 0.32 * efun ( - 0.25 * v1 ) / 0.25 def beta_m ( x ): v1 = x - Vt - 40 return 0.28 * efun ( 0.2 * v1 ) / 0.2 def alpha_h ( x ): v1 = x - Vt - 17. return 0.128 * np . exp ( - v1 / 18. ) def beta_h ( x ): v1 = x - Vt - 40. return 4.0 / ( 1 + np . exp ( - 0.2 * v1 )) def alpha_n ( x ): v1 = x - Vt - 15. return 0.032 * efun ( - 0.2 * v1 ) / 0.2 def beta_n ( x ): v1 = x - Vt - 10. return 0.5 * np . exp ( - v1 / 40 ) # steady-states and time constants def tau_n ( x ): return 1 / ( alpha_n ( x ) + beta_n ( x )) def n_inf ( x ): return alpha_n ( x ) / ( alpha_n ( x ) + beta_n ( x )) def tau_m ( x ): return 1 / ( alpha_m ( x ) + beta_m ( x )) def m_inf ( x ): return alpha_m ( x ) / ( alpha_m ( x ) + beta_m ( x )) def tau_h ( x ): return 1 / ( alpha_h ( x ) + beta_h ( x )) def h_inf ( x ): return alpha_h ( x ) / ( alpha_h ( x ) + beta_h ( x )) # slow non-inactivating K+ def p_inf ( x ): v1 = x + 35. return 1.0 / ( 1. + np . exp ( - 0.1 * v1 )) def tau_p ( x ): v1 = x + 35. return tau_max / ( 3.3 * np . exp ( 0.05 * v1 ) + np . exp ( - 0.05 * v1 )) #################################### # simulation from initial point V = np . zeros_like ( t ) # voltage n = np . zeros_like ( t ) m = np . zeros_like ( t ) h = np . zeros_like ( t ) p = np . zeros_like ( t ) V [ 0 ] = float ( V0 ) n [ 0 ] = n_inf ( V [ 0 ]) m [ 0 ] = m_inf ( V [ 0 ]) h [ 0 ] = h_inf ( V [ 0 ]) p [ 0 ] = p_inf ( V [ 0 ]) for i in range ( 1 , t . shape [ 0 ]): tau_V_inv = ( ( m [ i - 1 ] ** 3 ) * gbar_Na * h [ i - 1 ] + ( n [ i - 1 ] ** 4 ) * gbar_K + g_leak + gbar_M * p [ i - 1 ] ) / C V_inf = ( ( m [ i - 1 ] ** 3 ) * gbar_Na * h [ i - 1 ] * E_Na + ( n [ i - 1 ] ** 4 ) * gbar_K * E_K + g_leak * E_leak + gbar_M * p [ i - 1 ] * E_K + I [ i - 1 ] + nois_fact * rng . randn () / ( tstep ** 0.5 ) ) / ( tau_V_inv * C ) V [ i ] = V_inf + ( V [ i - 1 ] - V_inf ) * np . exp ( - tstep * tau_V_inv ) n [ i ] = n_inf ( V [ i ]) + ( n [ i - 1 ] - n_inf ( V [ i ])) * np . exp ( - tstep / tau_n ( V [ i ])) m [ i ] = m_inf ( V [ i ]) + ( m [ i - 1 ] - m_inf ( V [ i ])) * np . exp ( - tstep / tau_m ( V [ i ])) h [ i ] = h_inf ( V [ i ]) + ( h [ i - 1 ] - h_inf ( V [ i ])) * np . exp ( - tstep / tau_h ( V [ i ])) p [ i ] = p_inf ( V [ i ]) + ( p [ i - 1 ] - p_inf ( V [ i ])) * np . exp ( - tstep / tau_p ( V [ i ])) return np . array ( V ) . reshape ( - 1 , 1 ) To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), given the input current \\(I_{\\text{inj}}\\) : import matplotlib as mpl import matplotlib.pyplot as plt # input current, time step, time array I , t_on , t_off , dt , t , A_soma = syn_current () # simulate Hodgkin-Huxley model for 3 different parameter sets params = np . array ([[[ 50. , 1. ]],[[ 4. , 1.5 ]],[[ 20. , 15. ]]]) num_samples = len ( params [:, 0 ]) sim_samples = np . zeros (( num_samples , len ( I ))) for i in range ( num_samples ): sim_samples [ i ,:] = HHsimulator ( V0 =- 70 , params = params [ i ,:], dt = dt , t = t , I = I , seed = None )[:, 0 ] # colors for traces col_min = 2 num_colors = num_samples + col_min cm1 = mpl . cm . Blues col1 = [ cm1 ( 1. * i / num_colors ) for i in range ( col_min , num_colors )] # plotting % matplotlib inline fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) for i in range ( num_samples ): plt . plot ( t , sim_samples [ i ,:], color = col1 [ i ], lw = 2 ) plt . ylabel ( 'voltage (mV)' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( t , I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( t ) / 2 , max ( t )]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) As can be seen, the voltage traces can be quite diverse for different parameter values. How can we infer the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ) underlying the particular observed voltage trace? delfi will allow us to solve this problem. In order to run the Hodgkin-Huxley simulator within delfi , we need to define a simulator class linking to the simulator: from delfi.simulator.BaseSimulator import BaseSimulator class HodgkinHuxley ( BaseSimulator ): def __init__ ( self , I , dt , V0 , seed = None ): \"\"\"Hodgkin-Huxley simulator Parameters ---------- I : array Numpy array with the input current dt : float Timestep V0 : float Voltage at first time step seed : int or None If set, randomness across runs is disabled \"\"\" dim_param = 2 super () . __init__ ( dim_param = dim_param , seed = seed ) self . I = I self . dt = dt self . t = np . arange ( 0 , len ( self . I ), 1 ) * self . dt self . HHsimulator = HHsimulator self . init = V0 def gen_single ( self , params ): \"\"\"Forward model for simulator for single parameter set Parameters ---------- params : list or np.array, 1d of length dim_param Parameter vector Returns ------- dict : dictionary with data The dictionary must contain a key data that contains the results of the forward run. Additional entries can be present. \"\"\" params = np . asarray ( params ) assert params . ndim == 1 , 'params.ndim must be 1' hh_seed = self . gen_newseed () states = self . HHsimulator ( self . init , params . reshape ( 1 , - 1 ), self . dt , self . t , self . I , seed = hh_seed ) return { 'data' : states . reshape ( - 1 ), 'time' : self . t , 'dt' : self . dt , 'I' : self . I . reshape ( - 1 )} Note that the simulator class simply wraps the simulator function we set up previously. delfi has the flexibility to use simulators that utilize external packages, e.g., Brian ( http://briansimulator.org/ ), nest ( https://www.nest-simulator.org/ ), or NEURON ( https://neuron.yale.edu/neuron/ ). External simulators do not need to be Python-based as long as they store simulation outputs in a format that can be read from Python.","title":"Model"},{"location":"tutorials/quickstart/#prior-over-model-parameters","text":"Now that we have the model and simulator class, we need to define a function with the prior over the model parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ), which in this case is chosen to be a Uniform distribution: import delfi.distribution as dd seed_p = 2 prior_min = np . array ([ . 5 , 1e-4 ]) prior_max = np . array ([ 80. , 15. ]) prior = dd . Uniform ( lower = prior_min , upper = prior_max , seed = seed_p )","title":"Prior over model parameters"},{"location":"tutorials/quickstart/#summary-statistics","text":"We want to fit a set of summary statistics of the observed data: number of spikes, mean resting potential, standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In order to do accomplish that, we define a summary statistics class which computes those quantities: from delfi.summarystats.BaseSummaryStats import BaseSummaryStats from scipy import stats as spstats class HodgkinHuxleyStats ( BaseSummaryStats ): \"\"\"Moment based SummaryStats class for the Hodgkin-Huxley model Calculates summary statistics \"\"\" def __init__ ( self , t_on , t_off , n_mom = 4 , n_summary = 7 , seed = None ): \"\"\"See SummaryStats.py for docstring\"\"\" super ( HodgkinHuxleyStats , self ) . __init__ ( seed = seed ) self . t_on = t_on self . t_off = t_off self . n_mom = n_mom self . n_summary = np . minimum ( n_summary , n_mom + 3 ) def calc ( self , repetition_list ): \"\"\"Calculate summary statistics Parameters ---------- repetition_list : list of dictionaries, one per repetition data list, returned by `gen` method of Simulator instance Returns ------- np.array, 2d with n_reps x n_summary \"\"\" stats = [] for r in range ( len ( repetition_list )): x = repetition_list [ r ] N = x [ 'data' ] . shape [ 0 ] t = x [ 'time' ] dt = x [ 'dt' ] t_on = self . t_on t_off = self . t_off # initialise array of spike counts v = np . array ( x [ 'data' ]) # put everything to -10 that is below -10 or has negative slope ind = np . where ( v < - 10 ) v [ ind ] = - 10 ind = np . where ( np . diff ( v ) < 0 ) v [ ind ] = - 10 # remaining negative slopes are at spike peaks ind = np . where ( np . diff ( v ) < 0 ) spike_times = np . array ( t )[ ind ] spike_times_stim = spike_times [( spike_times > t_on ) & ( spike_times < t_off )] # number of spikes if spike_times_stim . shape [ 0 ] > 0 : spike_times_stim = spike_times_stim [ np . append ( 1 , np . diff ( spike_times_stim )) > 0.5 ] # resting potential and std rest_pot = np . mean ( x [ 'data' ][ t < t_on ]) rest_pot_std = np . std ( x [ 'data' ][ int ( . 9 * t_on / dt ): int ( t_on / dt )]) # moments std_pw = np . power ( np . std ( x [ 'data' ][( t > t_on ) & ( t < t_off )]), np . linspace ( 3 , self . n_mom , self . n_mom - 2 )) std_pw = np . concatenate (( np . ones ( 1 ), std_pw )) moments = spstats . moment ( x [ 'data' ][( t > t_on ) & ( t < t_off )], np . linspace ( 2 , self . n_mom , self . n_mom - 1 )) / std_pw # concatenation of summary statistics sum_stats_vec = np . concatenate (( np . array ([ spike_times_stim . shape [ 0 ]]), np . array ([ rest_pot , rest_pot_std , np . mean ( x [ 'data' ][( t > t_on ) & ( t < t_off )])]), moments )) sum_stats_vec = sum_stats_vec [ 0 : self . n_summary ] stats . append ( sum_stats_vec ) return np . asarray ( stats )","title":"Summary statistics"},{"location":"tutorials/quickstart/#generator-class","text":"Now that we have the simulator class, the prior over parameters and the summary statistics class, we need to define a generator class that binds all these objects together: import delfi.generator as dg # input current, time step I , t_on , t_off , dt , t , A_soma = syn_current () # initial voltage V0 = - 70 # seeds seed_m = 1 # summary statistics hyperparameters n_mom = 4 n_summary = 7 # define model, prior, summary statistics and generator classes seed = 3 m = HodgkinHuxley ( I , dt , V0 = V0 , seed = seed_m ) s = HodgkinHuxleyStats ( t_on = t_on , t_off = t_off , n_mom = n_mom , n_summary = n_summary ) g = dg . Default ( model = m , prior = prior , summary = s ) Note that delfi allows for the use of multiprocessing tools to parallelise the simulations and increase the speed of simulations. As the simulations typically constitute the major computational burden in SNPE , we suggest the use of parallelisation. In order to accomplish that, we need to decide on the number of parallel processes (according to the computational resources available to the user - here we chose 10 parallel processes), define a list of as many models as the number of processes, and call MPGenerator class instead of Default : n_processes = 10 seeds_m = np . arange ( 1 , n_processes + 1 , 1 ) m = [] for i in range ( n_processes ): m . append ( HodgkinHuxley ( I , dt , V0 = V0 , seed = seeds_m [ i ])) g = dg . MPGenerator ( models = m , prior = prior , summary = s )","title":"Generator class"},{"location":"tutorials/quickstart/#coming-back-to-the-observed-data","text":"As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data: # true parameters and respective labels true_params = np . array ([ 50. , 5. ]) labels_params = [ r '$g_{Na}$' , r '$g_{K}$' ] # observed data: simulation given true parameters obs = m [ 0 ] . gen_single ( true_params ) Replotting the observed data with the respective current input: import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline fig = plt . figure ( figsize = ( 7 , 5 )) gs = mpl . gridspec . GridSpec ( 2 , 1 , height_ratios = [ 4 , 1 ]) ax = plt . subplot ( gs [ 0 ]) plt . plot ( obs [ 'time' ], obs [ 'data' ]) plt . ylabel ( 'voltage (mV)' ) plt . title ( 'observed data' ) ax . set_xticks ([]) ax . set_yticks ([ - 80 , - 20 , 40 ]) ax = plt . subplot ( gs [ 1 ]) plt . plot ( obs [ 'time' ], I * A_soma * 1e3 , 'k' , lw = 2 ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'input (nA)' ) ax . set_xticks ([ 0 , max ( obs [ 'time' ]) / 2 , max ( obs [ 'time' ])]) ax . set_yticks ([ 0 , 1.1 * np . max ( I * A_soma * 1e3 )]) ax . yaxis . set_major_formatter ( mpl . ticker . FormatStrFormatter ( ' %.2f ' )) Let us compute the summary statistics of the observed data: obs_stats = s . calc ([ obs ])","title":"Coming back to the observed data"},{"location":"tutorials/quickstart/#inference","text":"Now that we have all the required components, we can run inference with SNPE . Let us start by defining the inference hyperparameters: seed_inf = 1 pilot_samples = 2000 # training schedule n_train = 2000 n_rounds = 1 # fitting setup minibatch = 100 epochs = 100 val_frac = 0.05 # network setup n_hiddens = [ 50 , 50 ] # convenience prior_norm = True # MAF parameters density = 'maf' n_mades = 5 # number of MADES Note that we chose to run SNPE where the density estimator is a masked autoregressive flow (MAF). Another option could have been a mixture of Gaussians (see http://www.mackelab.org/delfi/reference/neuralnet/ ). We can now run the inference algorithm. Note that this step can take less than one minute to up to one hour depending on the computational resources available and the number of processes defined above: import delfi.inference as infer # inference object res = infer . SNPEC ( g , obs = obs_stats , n_hiddens = n_hiddens , seed = seed_inf , pilot_samples = pilot_samples , n_mades = n_mades , prior_norm = prior_norm , density = density ) # train log , _ , posterior = res . run ( n_train = n_train , n_rounds = n_rounds , minibatch = minibatch , epochs = epochs , silent_fail = False , proposal = 'prior' , val_frac = val_frac , verbose = True ,) Note that we chose the algorithm SNPE-C , but other algorithms are available within delfi (see http://www.mackelab.org/delfi/reference/inference/ ).","title":"Inference"},{"location":"tutorials/quickstart/#analysis-of-the-results","text":"After running the inference algorithm, let us inspect and analyse the results. We can start by checking whether the algorithm training has converged by inspecting the respective loss function: fig = plt . figure ( figsize = ( 15 , 5 )) plt . plot ( log [ 0 ][ 'loss' ], lw = 2 ) plt . xlabel ( 'iteration' ) plt . ylabel ( 'loss' ); It seems that the loss function has converged. Let us inspect the inferred posterior distribution over the parameters ( \\(\\bar g_{Na}\\) , \\(\\bar g_K\\) ): from delfi.utils.viz import samples_nd prior_min = g . prior . lower prior_max = g . prior . upper prior_lims = np . concatenate (( prior_min . reshape ( - 1 , 1 ), prior_max . reshape ( - 1 , 1 )), axis = 1 ) posterior_samples = posterior [ 0 ] . gen ( 10000 ) ################### # colors hex2rgb = lambda h : tuple ( int ( h [ i : i + 2 ], 16 ) for i in ( 0 , 2 , 4 )) # RGB colors in [0, 255] col = {} col [ 'GT' ] = hex2rgb ( '30C05D' ) col [ 'SNPE' ] = hex2rgb ( '2E7FE8' ) col [ 'SAMPLE1' ] = hex2rgb ( '8D62BC' ) col [ 'SAMPLE2' ] = hex2rgb ( 'AF99EF' ) # convert to RGB colors in [0, 1] for k , v in col . items (): col [ k ] = tuple ([ i / 255 for i in v ]) ################### # posterior fig , axes = samples_nd ( posterior_samples , limits = prior_lims , ticks = prior_lims , labels = labels_params , fig_size = ( 5 , 5 ), diag = 'kde' , upper = 'kde' , hist_diag = { 'bins' : 50 }, hist_offdiag = { 'bins' : 50 }, kde_diag = { 'bins' : 50 , 'color' : col [ 'SNPE' ]}, kde_offdiag = { 'bins' : 50 }, points = [ true_params ], points_offdiag = { 'markersize' : 5 }, points_colors = [ col [ 'GT' ]], title = '' ); As can be seen, the inferred posterior contains the ground-truth parameters (green) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for each of those samples and compare the simulations with the observed data: fig = plt . figure ( figsize = ( 7 , 5 )) y_obs = obs [ 'data' ] t = obs [ 'time' ] duration = np . max ( t ) num_samp = 2 # sample from posterior x_samp = posterior [ 0 ] . gen ( n_samples = num_samp ) # reject samples for which prior is zero ind = ( x_samp > prior_min ) & ( x_samp < prior_max ) params = x_samp [ np . prod ( ind , axis = 1 ) == 1 ] num_samp = len ( params [:, 0 ]) # simulate and plot samples V = np . zeros (( len ( t ), num_samp )) for i in range ( num_samp ): x = m [ 0 ] . gen_single ( params [ i ,:]) V [:, i ] = x [ 'data' ] plt . plot ( t , V [:, i ], color = col [ 'SAMPLE' + str ( i + 1 )], lw = 2 , label = 'sample ' + str ( num_samp - i )) # plot observation plt . plot ( t , y_obs , '--' , lw = 2 , label = 'observation' ) plt . xlabel ( 'time (ms)' ) plt . ylabel ( 'voltage (mV)' ) ax = plt . gca () handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], bbox_to_anchor = ( 1.3 , 1 ), loc = 'upper right' ) ax . set_xticks ([ 0 , duration / 2 , duration ]) ax . set_yticks ([ - 80 , - 20 , 40 ]); As can be seen, the samples from the inferred posterior lead to simulations that closely resemble the observed data, confirming that SNPE-C did a good job at capturing the observed data.","title":"Analysis of the results"},{"location":"tutorials/quickstart/#references","text":"A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500\u2013544, 1952. M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Fr\u00e9gnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008.","title":"References"},{"location":"tutorials/quickstart_old/","text":"Getting started \u00b6 In this tutorial, we will do parameter inference on a simple statistical model. In total, this tutorial has an expected run time of 5 minutes. First off, let\u2019s see whether delfi is installed properly: import delfi print ( delfi . __version__ ) 0.8.0 No error was raised, and we can continue. Simulator \u00b6 For the purpose of this example, we will consider a very simple statistical model, with a single parameter \\(\\theta\\) . Depending on \\(\\theta\\) , data is generated according to: \\(x|\\mathbf{\\theta} \\sim 0.5 \\mathcal{N}(x|\\mu=\\theta, \\sigma^2=1) + 0.5 \\mathcal{N}(x|\\mu=\\theta, \\sigma^2=0.1)\\) This model is implemented in delfi.simulator , we will import the model and create an instance: from delfi.simulator import GaussMixture n_params = 1 m = GaussMixture ( dim = n_params ) Prior \u00b6 Next, we will define a prior distribution over \\(\\theta\\) as \\(\\mathcal{U}(-10, 10)\\) : import delfi.distribution as dd import numpy as np p = dd . Uniform ( lower = [ - 10 ], upper = [ 10 ]) Summary statistics \u00b6 We will not reduce the dimensionality of our data, and instead just apply the identity: from delfi.summarystats import Identity s = Identity () Generator \u00b6 Having defined the simulator, prior, and summary statistics, we instantiate a generator object: from delfi.generator import Default g = Default ( model = m , prior = p , summary = s ) Having specified the generator, we can draw parameters and data using the gen() method: params , stats = g . gen ( 500 ) import matplotlib.pyplot as plt % matplotlib inline plt . plot ( stats , params , '.' ) plt . ylabel ( r '$\\theta$' ) plt . xlabel ( r 'x' ); Inference \u00b6 Say, we observed data \\(x_\\text{o}\\) and are interested in the posterior distribution \\(p(\\theta|x=x_\\text{o})\\) . The likelihood-free inference (LFI) algorithms implemented in delfi allow doing so, without using the likelihood function: In most real world application, the likelihood function is not available. We aim to do inference by just generating examples, i.e., simulating the model. We choose the toy model such that the posterior is easily tractable analytically. This will allow us to judge the result against the ground truth. We will carry out inference for \\(x_0 = 0\\) . x0 = np . array ([[ 0. ]]) Basic density estimation based LFI \u00b6 A simple algorithm to use density estimation for LFI looks as follows: \\(q_\\phi\\) will be a mixture density network (MDN): The MDN is a neural network that maps from \\(x\\) to a Gaussian mixture distribution. For our example, we will set \\(K=2\\) , such that the resulting mixtures will have two components. To apply the above algorithm to the toy problem, create an instance of the inference algorithm specifying details: %% capture from delfi.inference import Basic inf_basic = Basic ( obs = x0 , generator = g , n_components = 2 , n_hiddens = [ 10 ]) We specified that we want K=2 components, and the neural network got a single hidden layer with 10 units. Additional entries in the list passed for n_hiddens would create additional layers. log , train_data , _ = inf_basic . run ( n_train = 4000 ) We drew N=n_train training examples and ran the algorithm. We can plot the loss function, across training rounds of the neural network: plt . figure () for i in range ( len ( log )): plt . subplot ( 1 , len ( log ), i + 1 ) plt . plot ( log [ i ][ 'loss' ]) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Loss' ) plt . title ( 'Round {0}' . format ( i + 1 )) Say we observed \\(x_o = 0.\\) and want to predict the posterior \\(\\hat{p}(\\theta | x=x_{\\text{o}})\\) : posterior = inf_basic . predict ( x0 ) The parameters of the predicted posterior are: for k in range ( 2 ): print ( r 'component {}: mixture weight = {:.2f}; mean = {:.2f}; variance = {:.2f}' . format ( k + 1 , posterior . a [ k ], posterior . xs [ k ] . m [ 0 ], posterior . xs [ k ] . S [ 0 ][ 0 ])) component 1: mixture weight = 0.45; mean = -0.00; variance = 0.08 component 2: mixture weight = 0.55; mean = -0.05; variance = 0.35 The correct posterior for this problem is known \u2013 since the prior is uniform it is simply: \\[p(\\theta|x=x_{\\text{o}}) = 0.5 \\mathcal{N}(\\theta|\\mu=0, \\sigma^2=1) + 0.5 \\mathcal{N}(\\theta|\\mu=0, \\sigma^2=0.1)\\] true_posterior = dd . MoG ( a = [ 0.5 , 0.5 ], ms = [[ 0. ], [ 0. ]], Ss = [[[ 1.0 ]], [[ 0.1 ]]]) plt . plot ( posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-b' ) plt . plot ( true_posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-r' ) plt . legend ([ 'predicted posterior' , 'true posterior' ], frameon = False ) plt . axis ( 'off' ); Sequential Neural Posterior Estimation \u00b6 For the problem we picked here, the basic inference algorithm does a good job. More complicated problems, however, may require more sophisticated algorithms. Instead of sampling all parameters from the prior, we can use our simulator more efficiently by drawin only a first sent of parameters from the prior and then switch to using a different distribution to sample from (a proposal distribution). Intuitively, the proposal distribution is chosen such that, when simulated, data is closer to \\(x_o\\) . In order to get the correct posterior, we will need to account for the fact that we drew these samples from a different distribution. There are three inference methods impelemented in delfi, each with a different approaches to this problem. APT or SNPE-C is the latest version of the SNPE algorithm and performs best on most problems. Automatic posterior transformation \u00b6 Automatic posterior transformation (APT, sometimes referred to as SNPE-C) improves on SNPE-A and SNPE-B by adjusting for the difference between the proposal and prior during learing. This means that APT doesn\u2019t rely on post-hoc corrections or importance weights, which leads to more accurate inference than the previous two methods. This technique is described in \u201cAutomatic Posterior Transformation for Likelihood-Free Inference,\u201d by Greenberg, Nonnenmacher and Macke, ICML 2019. from delfi.inference import APT inf_APT = APT ( generator = g , n_components = 2 , n_hiddens = [ 10 ], obs = x0 ) logs , tds , posteriors = inf_APT . run ( n_train = [ 3000 , 1000 ], n_rounds = 2 , train_on_all = True ) posterior = posteriors [ - 1 ] for k in range ( 2 ): print ( r 'component {}: mixture weight = {:.2f}; mean = {:.2f}; variance = {:.2f}' . format ( k + 1 , posterior . a [ k ], posterior . xs [ k ] . m [ 0 ], posterior . xs [ k ] . S [ 0 ][ 0 ])) true_posterior = dd . MoG ( a = [ 0.5 , 0.5 ], ms = [[ 0. ], [ 0. ]], Ss = [[[ 1.0 ]], [[ 0.1 ]]]) plt . plot ( posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-b' ) plt . plot ( true_posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-r' ) plt . legend ([ 'predicted posterior' , 'true posterior' ], frameon = False ) plt . axis ( 'off' ); component 1: mixture weight = 0.49; mean = 0.11; variance = 0.94 component 2: mixture weight = 0.51; mean = -0.00; variance = 0.12 Adapting to other problems \u00b6 In order to use delfi with a specific problem, you\u2019d need to implement a simulator class and possibly summary statistics. Simulators should inherit from a simulator base class. The base class for simulators specifies that each child needs to inherit a method called gen_single() \u2013 the function that forward simulates a single \\(\\theta\\) to data. It should return a dictionary that contains the result under a key called data . This is also detailled in the docstring of the base class . For an actual implementation, you can see the code for the mixture model we used above: https://github.com/mackelab/delfi/blob/master/delfi/simulator/GaussMixture.py . For this example, we used the identity as a summary statistics. If you were to use summary statistics, consider the implementation of mean summary statistics for a basic example: https://github.com/mackelab/delfi/blob/master/delfi/summarystats/Mean.py , and again, there is a base class specifying the interface .","title":"Getting started"},{"location":"tutorials/quickstart_old/#getting-started","text":"In this tutorial, we will do parameter inference on a simple statistical model. In total, this tutorial has an expected run time of 5 minutes. First off, let\u2019s see whether delfi is installed properly: import delfi print ( delfi . __version__ ) 0.8.0 No error was raised, and we can continue.","title":"Getting started"},{"location":"tutorials/quickstart_old/#simulator","text":"For the purpose of this example, we will consider a very simple statistical model, with a single parameter \\(\\theta\\) . Depending on \\(\\theta\\) , data is generated according to: \\(x|\\mathbf{\\theta} \\sim 0.5 \\mathcal{N}(x|\\mu=\\theta, \\sigma^2=1) + 0.5 \\mathcal{N}(x|\\mu=\\theta, \\sigma^2=0.1)\\) This model is implemented in delfi.simulator , we will import the model and create an instance: from delfi.simulator import GaussMixture n_params = 1 m = GaussMixture ( dim = n_params )","title":"Simulator"},{"location":"tutorials/quickstart_old/#prior","text":"Next, we will define a prior distribution over \\(\\theta\\) as \\(\\mathcal{U}(-10, 10)\\) : import delfi.distribution as dd import numpy as np p = dd . Uniform ( lower = [ - 10 ], upper = [ 10 ])","title":"Prior"},{"location":"tutorials/quickstart_old/#summary-statistics","text":"We will not reduce the dimensionality of our data, and instead just apply the identity: from delfi.summarystats import Identity s = Identity ()","title":"Summary statistics"},{"location":"tutorials/quickstart_old/#generator","text":"Having defined the simulator, prior, and summary statistics, we instantiate a generator object: from delfi.generator import Default g = Default ( model = m , prior = p , summary = s ) Having specified the generator, we can draw parameters and data using the gen() method: params , stats = g . gen ( 500 ) import matplotlib.pyplot as plt % matplotlib inline plt . plot ( stats , params , '.' ) plt . ylabel ( r '$\\theta$' ) plt . xlabel ( r 'x' );","title":"Generator"},{"location":"tutorials/quickstart_old/#inference","text":"Say, we observed data \\(x_\\text{o}\\) and are interested in the posterior distribution \\(p(\\theta|x=x_\\text{o})\\) . The likelihood-free inference (LFI) algorithms implemented in delfi allow doing so, without using the likelihood function: In most real world application, the likelihood function is not available. We aim to do inference by just generating examples, i.e., simulating the model. We choose the toy model such that the posterior is easily tractable analytically. This will allow us to judge the result against the ground truth. We will carry out inference for \\(x_0 = 0\\) . x0 = np . array ([[ 0. ]])","title":"Inference"},{"location":"tutorials/quickstart_old/#basic-density-estimation-based-lfi","text":"A simple algorithm to use density estimation for LFI looks as follows: \\(q_\\phi\\) will be a mixture density network (MDN): The MDN is a neural network that maps from \\(x\\) to a Gaussian mixture distribution. For our example, we will set \\(K=2\\) , such that the resulting mixtures will have two components. To apply the above algorithm to the toy problem, create an instance of the inference algorithm specifying details: %% capture from delfi.inference import Basic inf_basic = Basic ( obs = x0 , generator = g , n_components = 2 , n_hiddens = [ 10 ]) We specified that we want K=2 components, and the neural network got a single hidden layer with 10 units. Additional entries in the list passed for n_hiddens would create additional layers. log , train_data , _ = inf_basic . run ( n_train = 4000 ) We drew N=n_train training examples and ran the algorithm. We can plot the loss function, across training rounds of the neural network: plt . figure () for i in range ( len ( log )): plt . subplot ( 1 , len ( log ), i + 1 ) plt . plot ( log [ i ][ 'loss' ]) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Loss' ) plt . title ( 'Round {0}' . format ( i + 1 )) Say we observed \\(x_o = 0.\\) and want to predict the posterior \\(\\hat{p}(\\theta | x=x_{\\text{o}})\\) : posterior = inf_basic . predict ( x0 ) The parameters of the predicted posterior are: for k in range ( 2 ): print ( r 'component {}: mixture weight = {:.2f}; mean = {:.2f}; variance = {:.2f}' . format ( k + 1 , posterior . a [ k ], posterior . xs [ k ] . m [ 0 ], posterior . xs [ k ] . S [ 0 ][ 0 ])) component 1: mixture weight = 0.45; mean = -0.00; variance = 0.08 component 2: mixture weight = 0.55; mean = -0.05; variance = 0.35 The correct posterior for this problem is known \u2013 since the prior is uniform it is simply: \\[p(\\theta|x=x_{\\text{o}}) = 0.5 \\mathcal{N}(\\theta|\\mu=0, \\sigma^2=1) + 0.5 \\mathcal{N}(\\theta|\\mu=0, \\sigma^2=0.1)\\] true_posterior = dd . MoG ( a = [ 0.5 , 0.5 ], ms = [[ 0. ], [ 0. ]], Ss = [[[ 1.0 ]], [[ 0.1 ]]]) plt . plot ( posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-b' ) plt . plot ( true_posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-r' ) plt . legend ([ 'predicted posterior' , 'true posterior' ], frameon = False ) plt . axis ( 'off' );","title":"Basic density estimation based LFI"},{"location":"tutorials/quickstart_old/#sequential-neural-posterior-estimation","text":"For the problem we picked here, the basic inference algorithm does a good job. More complicated problems, however, may require more sophisticated algorithms. Instead of sampling all parameters from the prior, we can use our simulator more efficiently by drawin only a first sent of parameters from the prior and then switch to using a different distribution to sample from (a proposal distribution). Intuitively, the proposal distribution is chosen such that, when simulated, data is closer to \\(x_o\\) . In order to get the correct posterior, we will need to account for the fact that we drew these samples from a different distribution. There are three inference methods impelemented in delfi, each with a different approaches to this problem. APT or SNPE-C is the latest version of the SNPE algorithm and performs best on most problems.","title":"Sequential Neural Posterior Estimation"},{"location":"tutorials/quickstart_old/#automatic-posterior-transformation","text":"Automatic posterior transformation (APT, sometimes referred to as SNPE-C) improves on SNPE-A and SNPE-B by adjusting for the difference between the proposal and prior during learing. This means that APT doesn\u2019t rely on post-hoc corrections or importance weights, which leads to more accurate inference than the previous two methods. This technique is described in \u201cAutomatic Posterior Transformation for Likelihood-Free Inference,\u201d by Greenberg, Nonnenmacher and Macke, ICML 2019. from delfi.inference import APT inf_APT = APT ( generator = g , n_components = 2 , n_hiddens = [ 10 ], obs = x0 ) logs , tds , posteriors = inf_APT . run ( n_train = [ 3000 , 1000 ], n_rounds = 2 , train_on_all = True ) posterior = posteriors [ - 1 ] for k in range ( 2 ): print ( r 'component {}: mixture weight = {:.2f}; mean = {:.2f}; variance = {:.2f}' . format ( k + 1 , posterior . a [ k ], posterior . xs [ k ] . m [ 0 ], posterior . xs [ k ] . S [ 0 ][ 0 ])) true_posterior = dd . MoG ( a = [ 0.5 , 0.5 ], ms = [[ 0. ], [ 0. ]], Ss = [[[ 1.0 ]], [[ 0.1 ]]]) plt . plot ( posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-b' ) plt . plot ( true_posterior . eval ( np . arange ( - 5.0 , 5.0 , 0.01 ) . reshape ( - 1 , 1 ), log = False ), '-r' ) plt . legend ([ 'predicted posterior' , 'true posterior' ], frameon = False ) plt . axis ( 'off' ); component 1: mixture weight = 0.49; mean = 0.11; variance = 0.94 component 2: mixture weight = 0.51; mean = -0.00; variance = 0.12","title":"Automatic posterior transformation"},{"location":"tutorials/quickstart_old/#adapting-to-other-problems","text":"In order to use delfi with a specific problem, you\u2019d need to implement a simulator class and possibly summary statistics. Simulators should inherit from a simulator base class. The base class for simulators specifies that each child needs to inherit a method called gen_single() \u2013 the function that forward simulates a single \\(\\theta\\) to data. It should return a dictionary that contains the result under a key called data . This is also detailled in the docstring of the base class . For an actual implementation, you can see the code for the mixture model we used above: https://github.com/mackelab/delfi/blob/master/delfi/simulator/GaussMixture.py . For this example, we used the identity as a summary statistics. If you were to use summary statistics, consider the implementation of mean summary statistics for a basic example: https://github.com/mackelab/delfi/blob/master/delfi/summarystats/Mean.py , and again, there is a base class specifying the interface .","title":"Adapting to other problems"}]}